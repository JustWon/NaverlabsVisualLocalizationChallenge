{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_posDistThr = 5\n",
    "G_posDistSqThr = 25\n",
    "G_nonTrivPosDistSqThr = 20\n",
    "G_cache_name = '_feat_cache_naverlabs_2.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "import random, shutil, json\n",
    "from math import log10, ceil\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "import faiss\n",
    "\n",
    "import netvlad\n",
    "\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "from os import makedirs, remove, chdir, environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--fromscratch'], dest='fromscratch', nargs=0, const=True, default=False, type=None, choices=None, help='Train from scratch rather than using pretrained models', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='pytorch-NetVlad')\n",
    "parser.add_argument('--mode', type=str, default='train', help='Mode', choices=['train', 'test', 'cluster'])\n",
    "parser.add_argument('--batchSize', type=int, default=4, help='Number of triplets (query, pos, negs). Each triplet consists of 12 images.')\n",
    "parser.add_argument('--cacheBatchSize', type=int, default=24, help='Batch size for caching and testing')\n",
    "parser.add_argument('--cacheRefreshRate', type=int, default=1000, help='How often to refresh cache, in number of queries. 0 for off')\n",
    "parser.add_argument('--nEpochs', type=int, default=30, help='number of epochs to train for')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--nGPU', type=int, default=1, help='number of GPU to use.')\n",
    "parser.add_argument('--optim', type=str, default='SGD', help='optimizer to use', choices=['SGD', 'ADAM'])\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate.')\n",
    "parser.add_argument('--lrStep', type=float, default=5, help='Decay LR ever N steps.')\n",
    "parser.add_argument('--lrGamma', type=float, default=0.5, help='Multiply LR by Gamma for decaying.')\n",
    "parser.add_argument('--weightDecay', type=float, default=0.001, help='Weight decay for SGD.')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGD.')\n",
    "parser.add_argument('--nocuda', action='store_true', help='Dont use cuda')\n",
    "parser.add_argument('--threads', type=int, default=8, help='Number of threads for each data loader to use')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed to use.')\n",
    "parser.add_argument('--dataPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/data/', help='Path for centroid data.')\n",
    "parser.add_argument('--runsPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', help='Path to save runs to.')\n",
    "parser.add_argument('--savePath', type=str, default='checkpoints', help='Path to save checkpoints to in logdir. Default=checkpoints/')\n",
    "parser.add_argument('--cachePath', type=str, default='/tmp', help='Path to save cache to.')\n",
    "parser.add_argument('--resume', type=str, default='', help='Path to load checkpoint from, for resuming training or testing.')\n",
    "parser.add_argument('--ckpt', type=str, default='latest', help='Resume from latest or best checkpoint.', choices=['latest', 'best'])\n",
    "parser.add_argument('--evalEvery', type=int, default=1, help='Do a validation set run, and save, every N epochs.')\n",
    "parser.add_argument('--patience', type=int, default=10, help='Patience for early stopping. 0 is off.')\n",
    "parser.add_argument('--dataset', type=str, default='pittsburgh', help='Dataset to use', choices=['pittsburgh','naverlabs'])\n",
    "parser.add_argument('--arch', type=str, default='vgg16', help='basenetwork to use', choices=['vgg16', 'alexnet'])\n",
    "parser.add_argument('--vladv2', action='store_true', help='Use VLAD v2')\n",
    "parser.add_argument('--pooling', type=str, default='netvlad', help='type of pooling to use', choices=['netvlad', 'max', 'avg'])\n",
    "parser.add_argument('--num_clusters', type=int, default=64, help='Number of NetVlad clusters. Default=64')\n",
    "parser.add_argument('--margin', type=float, default=0.1, help='Margin for triplet loss. Default=0.1')\n",
    "parser.add_argument('--split', type=str, default='val', help='Data split to use for testing. Default is val', choices=['test', 'test250k', 'train', 'val'])\n",
    "parser.add_argument('--fromscratch', action='store_true', help='Train from scratch rather than using pretrained models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "from os.path import join, exists\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import h5py\n",
    "\n",
    "root_dir = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/'\n",
    "queries_dir = root_dir\n",
    "\n",
    "def input_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def get_whole_training_set(onlyDB=False):\n",
    "    return WholeDatasetFromStruct(input_transform=input_transform(), onlyDB=onlyDB, mode='train')\n",
    "\n",
    "def get_training_query_set(margin=0.1):\n",
    "    return QueryDatasetFromStruct(input_transform=input_transform(), margin=margin, mode='train')\n",
    "\n",
    "def get_whole_val_set():\n",
    "    return WholeDatasetFromStruct(input_transform=input_transform(), mode='val')\n",
    "\n",
    "dbStruct = namedtuple('dbStruct', ['whichSet', 'dataset', \n",
    "    'dbImage', 'utmDb', 'qImage', 'utmQ', 'numDb', 'numQ',\n",
    "    'posDistThr', 'posDistSqThr', 'nonTrivPosDistSqThr'])\n",
    "\n",
    "class WholeDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, input_transform=None, onlyDB=False, mode='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "        self.dbStruct = my_parse_dbStruct(mode)\n",
    "        self.images = [join(root_dir, dbIm) for dbIm in self.dbStruct.dbImage]\n",
    "        if not onlyDB:\n",
    "            self.images += [join(queries_dir, qIm) for qIm in self.dbStruct.qImage]\n",
    "\n",
    "        self.whichSet = self.dbStruct.whichSet\n",
    "        self.dataset = self.dbStruct.dataset\n",
    "\n",
    "        self.positives = None\n",
    "        self.distances = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index])\n",
    "        img = img.resize((640, 480))\n",
    "\n",
    "        if self.input_transform:\n",
    "            img = self.input_transform(img)\n",
    "\n",
    "        return img, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def getPositives(self):\n",
    "        # positives for evaluation are those within trivial threshold range\n",
    "        #fit NN to find them, search by radius\n",
    "        if  self.positives is None:\n",
    "            knn = NearestNeighbors(n_jobs=-1)\n",
    "            knn.fit(self.dbStruct.utmDb)\n",
    "\n",
    "            self.distances, self.positives = knn.radius_neighbors(self.dbStruct.utmQ,\n",
    "                    radius=self.dbStruct.posDistThr)\n",
    "\n",
    "        return self.positives\n",
    "\n",
    "class QueryDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, nNegSample=1000, nNeg=10, margin=0.1, input_transform=None, mode='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.margin = margin\n",
    "\n",
    "        self.dbStruct = my_parse_dbStruct(mode)\n",
    "        self.whichSet = self.dbStruct.whichSet\n",
    "        self.dataset = self.dbStruct.dataset\n",
    "        self.nNegSample = nNegSample # number of negatives to randomly sample\n",
    "        self.nNeg = nNeg # number of negatives used for training\n",
    "\n",
    "        # potential positives are those within nontrivial threshold range\n",
    "        #fit NN to find them, search by radius\n",
    "        knn = NearestNeighbors(n_jobs=-1)\n",
    "        knn.fit(self.dbStruct.utmDb)\n",
    "\n",
    "        # TODO use sqeuclidean as metric?\n",
    "        self.nontrivial_positives = list(knn.radius_neighbors(self.dbStruct.utmQ,\n",
    "                radius=self.dbStruct.nonTrivPosDistSqThr**0.5, \n",
    "                return_distance=False))\n",
    "        # radius returns unsorted, sort once now so we dont have to later\n",
    "        for i,posi in enumerate(self.nontrivial_positives):\n",
    "            self.nontrivial_positives[i] = np.sort(posi)\n",
    "        # its possible some queries don't have any non trivial potential positives\n",
    "        # lets filter those out\n",
    "        self.queries = np.where(np.array([len(x) for x in self.nontrivial_positives])>0)[0]\n",
    "\n",
    "        # potential negatives are those outside of posDistThr range\n",
    "        potential_positives = knn.radius_neighbors(self.dbStruct.utmQ,\n",
    "                radius=self.dbStruct.posDistThr, \n",
    "                return_distance=False)\n",
    "\n",
    "        self.potential_negatives = []\n",
    "        for pos in potential_positives:\n",
    "            self.potential_negatives.append(np.setdiff1d(np.arange(self.dbStruct.numDb),\n",
    "                pos, assume_unique=True))\n",
    "\n",
    "        self.cache = None # filepath of HDF5 containing feature vectors for images\n",
    "\n",
    "        self.negCache = [np.empty((0,)) for _ in range(self.dbStruct.numQ)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.queries[index] # re-map index to match dataset\n",
    "        with h5py.File(self.cache, mode='r') as h5: \n",
    "            h5feat = h5.get(\"features\")\n",
    "\n",
    "            qOffset = self.dbStruct.numDb \n",
    "            qFeat = h5feat[index+qOffset]\n",
    "\n",
    "            posFeat = h5feat[self.nontrivial_positives[index].tolist()]\n",
    "            knn = NearestNeighbors(n_jobs=-1) # TODO replace with faiss?\n",
    "            knn.fit(posFeat)\n",
    "            dPos, posNN = knn.kneighbors(qFeat.reshape(1,-1), 1)\n",
    "            dPos = dPos.item()\n",
    "            posIndex = self.nontrivial_positives[index][posNN[0]].item()\n",
    "\n",
    "            negSample = np.random.choice(self.potential_negatives[index], self.nNegSample)\n",
    "            negSample = np.unique(np.concatenate([self.negCache[index], negSample]))\n",
    "\n",
    "            negFeat = h5feat[negSample.tolist()]\n",
    "            knn.fit(negFeat)\n",
    "\n",
    "            dNeg, negNN = knn.kneighbors(qFeat.reshape(1,-1), \n",
    "                    self.nNeg*10) # to quote netvlad paper code: 10x is hacky but fine\n",
    "            dNeg = dNeg.reshape(-1)\n",
    "            negNN = negNN.reshape(-1)\n",
    "\n",
    "            # try to find negatives that are within margin, if there aren't any return none\n",
    "            violatingNeg = dNeg < dPos + self.margin**0.5\n",
    "     \n",
    "            if np.sum(violatingNeg) < 1:\n",
    "                #if none are violating then skip this query\n",
    "                return None\n",
    "\n",
    "            negNN = negNN[violatingNeg][:self.nNeg]\n",
    "            negIndices = negSample[negNN].astype(np.int32)\n",
    "            self.negCache[index] = negIndices\n",
    "\n",
    "        query = Image.open(join(queries_dir, self.dbStruct.qImage[index]))\n",
    "        query = query.resize((640, 480))\n",
    "        positive = Image.open(join(root_dir, self.dbStruct.dbImage[posIndex]))\n",
    "        positive = positive.resize((640, 480))\n",
    "\n",
    "        if self.input_transform:\n",
    "            query = self.input_transform(query)\n",
    "            positive = self.input_transform(positive)\n",
    "\n",
    "        negatives = []\n",
    "        for negIndex in negIndices:\n",
    "            negative = Image.open(join(root_dir, self.dbStruct.dbImage[negIndex]))\n",
    "            negative = negative.resize((640, 480))\n",
    "            if self.input_transform:\n",
    "                negative = self.input_transform(negative)\n",
    "            negatives.append(negative)\n",
    "\n",
    "        negatives = torch.stack(negatives, 0)\n",
    "\n",
    "        return query, positive, negatives, [index, posIndex]+negIndices.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_parse_dbStruct(_whichSet='train'):\n",
    "\n",
    "    whichSet = _whichSet\n",
    "    dataset = 'naverlabs'\n",
    "    image_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/images'\n",
    "    image_files_list = []\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970285_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970286_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970288_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970289_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970290_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970291_*.jpg'))))\n",
    "    image_files = np.hstack(image_files_list)\n",
    "\n",
    "    total_image_path = [os.path.join('images', image_file.split('/')[-1]) for image_file in image_files]\n",
    "\n",
    "    gt_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/groundtruth.hdf5'\n",
    "    total_pose = []\n",
    "    with h5py.File(gt_path, \"r\") as f:\n",
    "        total_pose.append(np.array(f['22970285_pose']))\n",
    "        total_pose.append(np.array(f['22970286_pose']))\n",
    "        total_pose.append(np.array(f['22970288_pose']))\n",
    "        total_pose.append(np.array(f['22970289_pose']))\n",
    "        total_pose.append(np.array(f['22970290_pose']))\n",
    "        total_pose.append(np.array(f['22970291_pose']))   \n",
    "        total_pose = np.vstack(utmDb)[:,:2]\n",
    "\n",
    "    dbImage = []\n",
    "    utmDb = []\n",
    "    qImage = []\n",
    "    utmQ = []\n",
    "    for idx, (img_path, pose) in enumerate(zip(total_image_path,total_pose)):\n",
    "        if (idx % 10 == 0):\n",
    "            qImage.append(img_path)\n",
    "            utmQ.append(pose)\n",
    "        else:\n",
    "            dbImage.append(img_path)\n",
    "            utmDb.append(pose)\n",
    "\n",
    "    utmQ = np.array(utmQ)\n",
    "    utmDb = np.array(utmDb)\n",
    "\n",
    "    return dbStruct(whichSet, dataset, dbImage, utmDb, \n",
    "                    qImage, utmQ, len(dbImage), len(qImage), \n",
    "                   G_posDistThr, G_posDistSqThr, G_nonTrivPosDistSqThr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (query, positive, negatives).\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuple (query, positive, negatives). \n",
    "            - query: torch tensor of shape (3, h, w).\n",
    "            - positive: torch tensor of shape (3, h, w).\n",
    "            - negative: torch tensor of shape (n, 3, h, w).\n",
    "    Returns:\n",
    "        query: torch tensor of shape (batch_size, 3, h, w).\n",
    "        positive: torch tensor of shape (batch_size, 3, h, w).\n",
    "        negatives: torch tensor of shape (batch_size, n, 3, h, w).\n",
    "    \"\"\"\n",
    "\n",
    "    batch = list(filter (lambda x:x is not None, batch))\n",
    "    if len(batch) == 0: return None, None, None, None, None\n",
    "\n",
    "    query, positive, negatives, indices = zip(*batch)\n",
    "\n",
    "    query = data.dataloader.default_collate(query)\n",
    "    positive = data.dataloader.default_collate(positive)\n",
    "    negCounts = data.dataloader.default_collate([x.shape[0] for x in negatives])\n",
    "    negatives = torch.cat(negatives, 0)\n",
    "    import itertools\n",
    "    indices = list(itertools.chain(*indices))\n",
    "\n",
    "    return query, positive, negatives, negCounts, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    epoch_loss = 0\n",
    "    startIter = 1 # keep track of batch iter across subsets for logging\n",
    "\n",
    "    if opt.cacheRefreshRate > 0:\n",
    "        subsetN = ceil(len(train_set) / opt.cacheRefreshRate)\n",
    "        #TODO randomise the arange before splitting?\n",
    "        subsetIdx = np.array_split(np.arange(len(train_set)), subsetN)\n",
    "    else:\n",
    "        subsetN = 1\n",
    "        subsetIdx = [np.arange(len(train_set))]\n",
    "\n",
    "    nBatches = (len(train_set) + opt.batchSize - 1) // opt.batchSize\n",
    "\n",
    "    for subIter in range(subsetN):\n",
    "        print('====> Building Cache')\n",
    "        model.eval()\n",
    "        train_set.cache = join(opt.cachePath, train_set.whichSet + G_cache_name)\n",
    "        with h5py.File(train_set.cache, mode='w') as h5: \n",
    "            pool_size = encoder_dim\n",
    "            if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters\n",
    "            h5feat = h5.create_dataset(\"features\", \n",
    "                    [len(whole_train_set), pool_size], \n",
    "                    dtype=np.float32)\n",
    "            with torch.no_grad():\n",
    "                for iteration, (input, indices) in enumerate(whole_training_data_loader, 1):\n",
    "                    input = input.to(device)\n",
    "                    image_encoding = model.encoder(input)\n",
    "                    vlad_encoding = model.pool(image_encoding) \n",
    "                    h5feat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()\n",
    "                    del input, image_encoding, vlad_encoding\n",
    "\n",
    "        sub_train_set = Subset(dataset=train_set, indices=subsetIdx[subIter])\n",
    "\n",
    "        training_data_loader = DataLoader(dataset=sub_train_set, num_workers=opt.threads, \n",
    "                    batch_size=opt.batchSize, shuffle=True, \n",
    "                    collate_fn=collate_fn, pin_memory=cuda)\n",
    "\n",
    "        print('Allocated:', torch.cuda.memory_allocated())\n",
    "        print('Cached:', torch.cuda.memory_cached())\n",
    "\n",
    "        model.train()\n",
    "        for iteration, (query, positives, negatives, negCounts, indices) in enumerate(training_data_loader, startIter):\n",
    "            # some reshaping to put query, pos, negs in a single (N, 3, H, W) tensor\n",
    "            # where N = batchSize * (nQuery + nPos + nNeg)\n",
    "            if query is None: continue # in case we get an empty batch\n",
    "\n",
    "            B, C, H, W = query.shape\n",
    "            nNeg = torch.sum(negCounts)\n",
    "            input = torch.cat([query, positives, negatives])\n",
    "\n",
    "            input = input.to(device)\n",
    "            image_encoding = model.encoder(input)\n",
    "            vlad_encoding = model.pool(image_encoding) \n",
    "\n",
    "            vladQ, vladP, vladN = torch.split(vlad_encoding, [B, B, nNeg])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # calculate loss for each Query, Positive, Negative triplet\n",
    "            # due to potential difference in number of negatives have to \n",
    "            # do it per query, per negative\n",
    "            loss = 0\n",
    "            for i, negCount in enumerate(negCounts):\n",
    "                for n in range(negCount):\n",
    "                    negIx = (torch.sum(negCounts[:i]) + n).item()\n",
    "                    loss += criterion(vladQ[i:i+1], vladP[i:i+1], vladN[negIx:negIx+1])\n",
    "\n",
    "            loss /= nNeg.float().to(device) # normalise by actual number of negatives\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del input, image_encoding, vlad_encoding, vladQ, vladP, vladN\n",
    "            del query, positives, negatives\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            if iteration % 50 == 0 or nBatches <= 10:\n",
    "                print(\"==> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, \n",
    "                    nBatches, batch_loss), flush=True)\n",
    "                writer.add_scalar('Train/Loss', batch_loss, \n",
    "                        ((epoch-1) * nBatches) + iteration)\n",
    "                writer.add_scalar('Train/nNeg', nNeg, \n",
    "                        ((epoch-1) * nBatches) + iteration)\n",
    "                print('Allocated:', torch.cuda.memory_allocated())\n",
    "                print('Cached:', torch.cuda.memory_cached())\n",
    "\n",
    "        startIter += len(training_data_loader)\n",
    "        del training_data_loader, loss\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        remove(train_set.cache) # delete HDF5 cache\n",
    "\n",
    "    avg_loss = epoch_loss / nBatches\n",
    "\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, avg_loss), \n",
    "            flush=True)\n",
    "    writer.add_scalar('Train/AvgLoss', avg_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(eval_set, epoch=0, write_tboard=False):\n",
    "    # TODO what if features dont fit in memory? \n",
    "    test_data_loader = DataLoader(dataset=eval_set, \n",
    "                num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False, \n",
    "                pin_memory=cuda)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('====> Extracting Features')\n",
    "        pool_size = encoder_dim\n",
    "        if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters\n",
    "        dbFeat = np.empty((len(eval_set), pool_size))\n",
    "\n",
    "        for iteration, (input, indices) in enumerate(test_data_loader, 1):\n",
    "            input = input.to(device)\n",
    "            image_encoding = model.encoder(input)\n",
    "            vlad_encoding = model.pool(image_encoding) \n",
    "\n",
    "            dbFeat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()\n",
    "            if iteration % 50 == 0 or len(test_data_loader) <= 10:\n",
    "                print(\"==> Batch ({}/{})\".format(iteration, \n",
    "                    len(test_data_loader)), flush=True)\n",
    "\n",
    "            del input, image_encoding, vlad_encoding\n",
    "    del test_data_loader\n",
    "\n",
    "    # extracted for both db and query, now split in own sets\n",
    "    qFeat = dbFeat[eval_set.dbStruct.numDb:].astype('float32')\n",
    "    dbFeat = dbFeat[:eval_set.dbStruct.numDb].astype('float32')\n",
    "    \n",
    "    print('====> Building faiss index')\n",
    "    faiss_index = faiss.IndexFlatL2(pool_size)\n",
    "    faiss_index.add(dbFeat)\n",
    "\n",
    "    print('====> Calculating recall @ N')\n",
    "    n_values = [1,5,10,20]\n",
    "\n",
    "    _, predictions = faiss_index.search(qFeat, max(n_values)) \n",
    "\n",
    "    # for each query get those within threshold distance\n",
    "    gt = eval_set.getPositives() \n",
    "\n",
    "    correct_at_n = np.zeros(len(n_values))\n",
    "    #TODO can we do this on the matrix in one go?\n",
    "    for qIx, pred in enumerate(predictions):\n",
    "        for i,n in enumerate(n_values):\n",
    "            # if in top N then also in top NN, where NN > N\n",
    "            if np.any(np.in1d(pred[:n], gt[qIx])):\n",
    "                correct_at_n[i:] += 1\n",
    "                break\n",
    "    recall_at_n = correct_at_n / eval_set.dbStruct.numQ\n",
    "\n",
    "    recalls = {} #make dict for output\n",
    "    for i,n in enumerate(n_values):\n",
    "        recalls[n] = recall_at_n[i]\n",
    "        print(\"====> Recall@{}: {:.4f}\".format(n, recall_at_n[i]))\n",
    "        if write_tboard: writer.add_scalar('Val/Recall@' + str(n), recall_at_n[i], epoch)\n",
    "\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    model_out_path = join(opt.savePath, filename)\n",
    "    torch.save(state, model_out_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(model_out_path, join(opt.savePath, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='vgg16', batchSize=4, cacheBatchSize=24, cachePath='/tmp', cacheRefreshRate=1000, ckpt='latest', dataPath='/home/ubuntu/Desktop/pytorch-NetVlad/data/', dataset='naverlabs', evalEvery=1, fromscratch=False, lr=0.0001, lrGamma=0.5, lrStep=5, margin=0.1, mode='train', momentum=0.9, nEpochs=30, nGPU=1, nocuda=False, num_clusters=64, optim='SGD', patience=10, pooling='netvlad', resume='', runsPath='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', savePath='checkpoints', seed=123, split='val', start_epoch=0, threads=8, vladv2=False, weightDecay=0.001)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args(args='--dataset=naverlabs --mode=train --arch=vgg16 --pooling=netvlad --num_clusters=64'.split(' '))\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = not opt.nocuda\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Training query set: 6228\n",
      "===> Evaluating on val set, query count: 6228\n"
     ]
    }
   ],
   "source": [
    "whole_train_set = get_whole_training_set()\n",
    "whole_training_data_loader = DataLoader(dataset=whole_train_set, \n",
    "        num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False, \n",
    "        pin_memory=cuda)\n",
    "\n",
    "train_set = get_training_query_set(opt.margin)\n",
    "\n",
    "print('====> Training query set:', len(train_set))\n",
    "whole_test_set = get_whole_val_set()\n",
    "print('===> Evaluating on val set, query count:', whole_test_set.dbStruct.numQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = not opt.fromscratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = 512\n",
    "encoder = models.vgg16(pretrained=pretrained)\n",
    "# capture only feature part and remove last relu and maxpool\n",
    "layers = list(encoder.features.children())[:-2]\n",
    "\n",
    "if pretrained:\n",
    "    # if using pretrained then only train conv5_1, conv5_2, and conv5_3\n",
    "    for l in layers[:-5]: \n",
    "        for p in l.parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(*layers)\n",
    "model = nn.Module() \n",
    "model.add_module('encoder', encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_vlad = netvlad.NetVLAD(num_clusters=opt.num_clusters, dim=encoder_dim, vladv2=opt.vladv2)\n",
    "if not opt.resume: \n",
    "    if opt.mode.lower() == 'train':\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + train_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "    else:\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + whole_test_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "\n",
    "    if not exists(initcache):\n",
    "        raise FileNotFoundError('Could not find clusters, please run with --mode=cluster before proceeding')\n",
    "\n",
    "    with h5py.File(initcache, mode='r') as h5: \n",
    "        clsts = h5.get(\"centroids\")[...]\n",
    "        traindescs = h5.get(\"descriptors\")[...]\n",
    "        net_vlad.init_params(clsts, traindescs) \n",
    "        del clsts, traindescs\n",
    "\n",
    "model.add_module('pool', net_vlad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "isParallel = False\n",
    "if opt.nGPU > 1 and torch.cuda.device_count() > 1:\n",
    "    model.encoder = nn.DataParallel(model.encoder)\n",
    "    if opt.mode.lower() != 'cluster':\n",
    "        model.pool = nn.DataParallel(model.pool)\n",
    "    isParallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.optim.upper() == 'ADAM':\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, \n",
    "        model.parameters()), lr=opt.lr)#, betas=(0,0.9))\n",
    "elif opt.optim.upper() == 'SGD':\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, \n",
    "        model.parameters()), lr=opt.lr,\n",
    "        momentum=opt.momentum,\n",
    "        weight_decay=opt.weightDecay)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lrStep, gamma=opt.lrGamma)\n",
    "else:\n",
    "    raise ValueError('Unknown optimizer: ' + opt.optim)\n",
    "\n",
    "# original paper/code doesn't sqrt() the distances, we do, so sqrt() the margin, I think :D\n",
    "criterion = nn.TripletMarginLoss(margin=opt.margin**0.5, p=2, reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Training model\n",
      "===> Saving state to: /home/ubuntu/Desktop/pytorch-NetVlad/runs/May20_23-12-21_vgg16_netvlad\n",
      "====> Building Cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 60039168\n",
      "Cached: 13847494656\n",
      "==> Epoch[1](50/1557): Loss: 0.1486\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "==> Epoch[1](100/1557): Loss: 0.2437\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "==> Epoch[1](150/1557): Loss: 0.1689\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "==> Epoch[1](200/1557): Loss: 0.1937\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 17532190720\n",
      "==> Epoch[1](250/1557): Loss: 0.1225\n",
      "Allocated: 117199360\n",
      "Cached: 12350128128\n",
      "==> Epoch[1](300/1557): Loss: 0.0987\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "==> Epoch[1](350/1557): Loss: 0.2753\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "==> Epoch[1](400/1557): Loss: 0.1561\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 17532190720\n",
      "==> Epoch[1](450/1557): Loss: 0.1825\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "==> Epoch[1](500/1557): Loss: 0.1465\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "==> Epoch[1](550/1557): Loss: 0.1484\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "==> Epoch[1](600/1557): Loss: 0.1635\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "==> Epoch[1](650/1557): Loss: 0.1637\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 17532190720\n",
      "==> Epoch[1](700/1557): Loss: 0.2392\n",
      "Allocated: 117199360\n",
      "Cached: 12352225280\n"
     ]
    }
   ],
   "source": [
    "print('===> Training model')\n",
    "writer = SummaryWriter(log_dir=join(opt.runsPath, datetime.now().strftime('%b%d_%H-%M-%S')+'_'+opt.arch+'_'+opt.pooling))\n",
    "\n",
    "# write checkpoints in logdir\n",
    "logdir = writer.file_writer.get_logdir()\n",
    "opt.savePath = join(logdir, opt.savePath)\n",
    "if not opt.resume:\n",
    "    makedirs(opt.savePath)\n",
    "\n",
    "with open(join(opt.savePath, 'flags.json'), 'w') as f:\n",
    "    f.write(json.dumps(\n",
    "        {k:v for k,v in vars(opt).items()}\n",
    "        ))\n",
    "print('===> Saving state to:', logdir)\n",
    "\n",
    "not_improved = 0\n",
    "best_score = 0\n",
    "for epoch in range(opt.start_epoch+1, opt.nEpochs + 1):\n",
    "    if opt.optim.upper() == 'SGD':\n",
    "        scheduler.step(epoch)\n",
    "    train(epoch)\n",
    "    if (epoch % opt.evalEvery) == 0:\n",
    "        recalls = test(whole_test_set, epoch, write_tboard=True)\n",
    "        is_best = recalls[5] > best_score \n",
    "        if is_best:\n",
    "            not_improved = 0\n",
    "            best_score = recalls[5]\n",
    "        else: \n",
    "            not_improved += 1\n",
    "\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'recalls': recalls,\n",
    "                'best_score': best_score,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'parallel' : isParallel,\n",
    "        }, is_best)\n",
    "\n",
    "        if opt.patience > 0 and not_improved > (opt.patience / opt.evalEvery):\n",
    "            print('Performance did not improve for', opt.patience, 'epochs. Stopping.')\n",
    "            break\n",
    "\n",
    "print(\"=> Best Recall@5: {:.4f}\".format(best_score), flush=True)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
