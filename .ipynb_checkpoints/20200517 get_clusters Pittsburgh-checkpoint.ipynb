{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "import random, shutil, json\n",
    "from math import log10, ceil\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--fromscratch'], dest='fromscratch', nargs=0, const=True, default=False, type=None, choices=None, help='Train from scratch rather than using pretrained models', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='pytorch-NetVlad')\n",
    "parser.add_argument('--mode', type=str, default='train', help='Mode', choices=['train', 'test', 'cluster'])\n",
    "parser.add_argument('--batchSize', type=int, default=4, help='Number of triplets (query, pos, negs). Each triplet consists of 12 images.')\n",
    "parser.add_argument('--cacheBatchSize', type=int, default=1, help='Batch size for caching and testing')\n",
    "parser.add_argument('--cacheRefreshRate', type=int, default=1000, help='How often to refresh cache, in number of queries. 0 for off')\n",
    "parser.add_argument('--nEpochs', type=int, default=30, help='number of epochs to train for')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--nGPU', type=int, default=1, help='number of GPU to use.')\n",
    "parser.add_argument('--optim', type=str, default='SGD', help='optimizer to use', choices=['SGD', 'ADAM'])\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate.')\n",
    "parser.add_argument('--lrStep', type=float, default=5, help='Decay LR ever N steps.')\n",
    "parser.add_argument('--lrGamma', type=float, default=0.5, help='Multiply LR by Gamma for decaying.')\n",
    "parser.add_argument('--weightDecay', type=float, default=0.001, help='Weight decay for SGD.')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGD.')\n",
    "parser.add_argument('--nocuda', action='store_true', help='Dont use cuda')\n",
    "parser.add_argument('--threads', type=int, default=8, help='Number of threads for each data loader to use')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed to use.')\n",
    "parser.add_argument('--dataPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/data/', help='Path for centroid data.')\n",
    "parser.add_argument('--runsPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', help='Path to save runs to.')\n",
    "parser.add_argument('--savePath', type=str, default='checkpoints', help='Path to save checkpoints to in logdir. Default=checkpoints/')\n",
    "parser.add_argument('--cachePath', type=str, default='/tmp', help='Path to save cache to.')\n",
    "parser.add_argument('--resume', type=str, default='', help='Path to load checkpoint from, for resuming training or testing.')\n",
    "parser.add_argument('--ckpt', type=str, default='latest', help='Resume from latest or best checkpoint.', choices=['latest', 'best'])\n",
    "parser.add_argument('--evalEvery', type=int, default=1, help='Do a validation set run, and save, every N epochs.')\n",
    "parser.add_argument('--patience', type=int, default=10, help='Patience for early stopping. 0 is off.')\n",
    "parser.add_argument('--dataset', type=str, default='pittsburgh', help='Dataset to use', choices=['pittsburgh'])\n",
    "parser.add_argument('--arch', type=str, default='vgg16', help='basenetwork to use', choices=['vgg16', 'alexnet'])\n",
    "parser.add_argument('--vladv2', action='store_true', help='Use VLAD v2')\n",
    "parser.add_argument('--pooling', type=str, default='netvlad', help='type of pooling to use', choices=['netvlad', 'max', 'avg'])\n",
    "parser.add_argument('--num_clusters', type=int, default=64, help='Number of NetVlad clusters. Default=64')\n",
    "parser.add_argument('--margin', type=float, default=0.1, help='Margin for triplet loss. Default=0.1')\n",
    "parser.add_argument('--split', type=str, default='val', help='Data split to use for testing. Default is val', choices=['test', 'test250k', 'train', 'val'])\n",
    "parser.add_argument('--fromscratch', action='store_true', help='Train from scratch rather than using pretrained models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "from os.path import join, exists\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import h5py\n",
    "\n",
    "root_dir = '/home/ubuntu/Desktop/visual-localization-challenge-2020'\n",
    "\n",
    "def input_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def get_whole_training_set(onlyDB=False):\n",
    "    return WholeDatasetFromStruct(input_transform=input_transform())\n",
    "\n",
    "dbStruct = namedtuple('dbStruct', ['whichSet', 'dataset', \n",
    "    'dbImage', 'utmDb', 'qImage', 'utmQ', 'numDb', 'numQ',\n",
    "    'posDistThr', 'posDistSqThr', 'nonTrivPosDistSqThr'])\n",
    "\n",
    "class WholeDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, input_transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        \n",
    "        self.dataset = 'train'\n",
    "        self.whichSet = 'vlc'\n",
    "        \n",
    "        dbImage_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/images'\n",
    "        self.dbImage = sorted(glob(os.path.join(dbImage_path, '*.jpg')))[:1000]\n",
    "        self.qImage = sorted(glob(os.path.join(dbImage_path, '*.jpg')))[:1000]\n",
    "        \n",
    "        self.images = self.dbImage\n",
    "        \n",
    "        filename = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/groundtruth.hdf5'\n",
    "        with h5py.File(filename, \"r\") as f:\n",
    "            self.dbCameraPose = np.array(f['22970285_pose'])[:1000,:2]\n",
    "            \n",
    "        self.nonTrivPosDistSqThr = 625\n",
    "        self.posDistThr = 25\n",
    "        \n",
    "        self.numDb = len(self.dbCameraPose)\n",
    "        self.numQ = len(self.dbCameraPose)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index])\n",
    "\n",
    "        if self.input_transform:\n",
    "            img = self.input_transform(img)\n",
    "\n",
    "        return img, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "#     def getPositives(self):\n",
    "#         # positives for evaluation are those within trivial threshold range\n",
    "#         #fit NN to find them, search by radius\n",
    "#         if  self.positives is None:\n",
    "#             knn = NearestNeighbors(n_jobs=-1)\n",
    "#             knn.fit(self.dbStruct.utmDb)\n",
    "\n",
    "#             self.distances, self.positives = knn.radius_neighbors(self.dbStruct.utmQ,\n",
    "#                     radius=self.dbStruct.posDistThr)\n",
    "\n",
    "#         return self.positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='vgg16', batchSize=4, cacheBatchSize=1, cachePath='/tmp', cacheRefreshRate=1000, ckpt='latest', dataPath='/home/ubuntu/Desktop/pytorch-NetVlad/data/', dataset='pittsburgh', evalEvery=1, fromscratch=False, lr=0.0001, lrGamma=0.5, lrStep=5, margin=0.1, mode='train', momentum=0.9, nEpochs=30, nGPU=1, nocuda=False, num_clusters=64, optim='SGD', patience=10, pooling='netvlad', resume='', runsPath='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', savePath='checkpoints', seed=123, split='val', start_epoch=0, threads=8, vladv2=False, weightDecay=0.001)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args('')\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = not opt.nocuda\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_train_set = get_whole_training_set(onlyDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = not opt.fromscratch\n",
    "\n",
    "encoder_dim = 512\n",
    "encoder = models.vgg16(pretrained=pretrained)\n",
    "# capture only feature part and remove last relu and maxpool\n",
    "layers = list(encoder.features.children())[:-2]\n",
    "\n",
    "if pretrained:\n",
    "    # if using pretrained then only train conv5_1, conv5_2, and conv5_3\n",
    "    for l in layers[:-5]: \n",
    "        for p in l.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "if opt.mode.lower() == 'cluster' and not opt.vladv2:\n",
    "        layers.append(L2Norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(*layers)\n",
    "model = nn.Module() \n",
    "model.add_module('encoder', encoder)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_set = whole_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nDescriptors = 50000\n",
    "nPerImage = 100\n",
    "nIm = ceil(nDescriptors/nPerImage)\n",
    "\n",
    "sampler = SubsetRandomSampler(np.random.choice(len(cluster_set), nIm, replace=False))\n",
    "data_loader = DataLoader(dataset=cluster_set, \n",
    "            num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False, \n",
    "            pin_memory=cuda,\n",
    "            sampler=sampler)\n",
    "\n",
    "if not exists(join(opt.dataPath, 'centroids')):\n",
    "    os.makedirs(join(opt.dataPath, 'centroids'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Extracting Descriptors\n",
      "==> Batch (50/500)\n",
      "==> Batch (100/500)\n",
      "==> Batch (150/500)\n",
      "==> Batch (200/500)\n",
      "==> Batch (250/500)\n",
      "==> Batch (300/500)\n",
      "==> Batch (350/500)\n",
      "==> Batch (400/500)\n",
      "==> Batch (450/500)\n",
      "==> Batch (500/500)\n",
      "====> Clustering..\n",
      "====> Storing centroids (64, 512)\n",
      "====> Done!\n"
     ]
    }
   ],
   "source": [
    "initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + cluster_set.dataset + '_' + str(opt.num_clusters) + '_desc_cen.hdf5')\n",
    "with h5py.File(initcache, mode='w') as h5: \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        print('====> Extracting Descriptors')\n",
    "        dbFeat = h5.create_dataset(\"descriptors\", \n",
    "                    [nDescriptors, encoder_dim], \n",
    "                    dtype=np.float32)\n",
    "\n",
    "        for iteration, (input, indices) in enumerate(data_loader, 1):\n",
    "            input = input.to(device)\n",
    "            image_descriptors = model.encoder(input).view(input.size(0), encoder_dim, -1).permute(0, 2, 1)\n",
    "            \n",
    "            batchix = (iteration-1)*opt.cacheBatchSize*nPerImage\n",
    "            for ix in range(image_descriptors.size(0)):\n",
    "                # sample different location for each image in batch\n",
    "                sample = np.random.choice(image_descriptors.size(1), nPerImage, replace=False)\n",
    "                startix = batchix + ix*nPerImage\n",
    "                dbFeat[startix:startix+nPerImage, :] = image_descriptors[ix, sample, :].detach().cpu().numpy()\n",
    "\n",
    "            if iteration % 50 == 0 or len(data_loader) <= 10:\n",
    "                print(\"==> Batch ({}/{})\".format(iteration, \n",
    "                    ceil(nIm/opt.cacheBatchSize)), flush=True)\n",
    "            \n",
    "            del input, image_descriptors\n",
    "\n",
    "    print('====> Clustering..')\n",
    "    niter = 100\n",
    "    kmeans = faiss.Kmeans(encoder_dim, opt.num_clusters, niter=niter, verbose=False)\n",
    "    kmeans.train(dbFeat[...])\n",
    "\n",
    "    print('====> Storing centroids', kmeans.centroids.shape)\n",
    "    h5.create_dataset('centroids', data=kmeans.centroids)\n",
    "    print('====> Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
