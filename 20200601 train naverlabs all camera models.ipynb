{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_posDistThr = 5\n",
    "G_posDistSqThr = 25\n",
    "G_nonTrivPosDistSqThr = 20\n",
    "G_cache_name = '_feat_cache_naverlabs.hdf5'\n",
    "query_val_partition_idx = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "import random, shutil, json\n",
    "from math import log10, ceil\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "import faiss\n",
    "\n",
    "import netvlad\n",
    "\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "from os import makedirs, remove, chdir, environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--fromscratch'], dest='fromscratch', nargs=0, const=True, default=False, type=None, choices=None, help='Train from scratch rather than using pretrained models', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='pytorch-NetVlad')\n",
    "parser.add_argument('--mode', type=str, default='train', help='Mode', choices=['train', 'test', 'cluster'])\n",
    "parser.add_argument('--batchSize', type=int, default=4, help='Number of triplets (query, pos, negs). Each triplet consists of 12 images.')\n",
    "parser.add_argument('--cacheBatchSize', type=int, default=24, help='Batch size for caching and testing')\n",
    "parser.add_argument('--cacheRefreshRate', type=int, default=1000, help='How often to refresh cache, in number of queries. 0 for off')\n",
    "parser.add_argument('--nEpochs', type=int, default=30, help='number of epochs to train for')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--nGPU', type=int, default=1, help='number of GPU to use.')\n",
    "parser.add_argument('--optim', type=str, default='SGD', help='optimizer to use', choices=['SGD', 'ADAM'])\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate.')\n",
    "parser.add_argument('--lrStep', type=float, default=5, help='Decay LR ever N steps.')\n",
    "parser.add_argument('--lrGamma', type=float, default=0.5, help='Multiply LR by Gamma for decaying.')\n",
    "parser.add_argument('--weightDecay', type=float, default=0.001, help='Weight decay for SGD.')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGD.')\n",
    "parser.add_argument('--nocuda', action='store_true', help='Dont use cuda')\n",
    "parser.add_argument('--threads', type=int, default=8, help='Number of threads for each data loader to use')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed to use.')\n",
    "parser.add_argument('--dataPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/data/', help='Path for centroid data.')\n",
    "parser.add_argument('--runsPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', help='Path to save runs to.')\n",
    "parser.add_argument('--savePath', type=str, default='checkpoints', help='Path to save checkpoints to in logdir. Default=checkpoints/')\n",
    "parser.add_argument('--cachePath', type=str, default='/tmp', help='Path to save cache to.')\n",
    "parser.add_argument('--resume', type=str, default='', help='Path to load checkpoint from, for resuming training or testing.')\n",
    "parser.add_argument('--ckpt', type=str, default='latest', help='Resume from latest or best checkpoint.', choices=['latest', 'best'])\n",
    "parser.add_argument('--evalEvery', type=int, default=1, help='Do a validation set run, and save, every N epochs.')\n",
    "parser.add_argument('--patience', type=int, default=10, help='Patience for early stopping. 0 is off.')\n",
    "parser.add_argument('--dataset', type=str, default='pittsburgh', help='Dataset to use', choices=['pittsburgh','naverlabs'])\n",
    "parser.add_argument('--arch', type=str, default='vgg16', help='basenetwork to use', choices=['vgg16', 'alexnet'])\n",
    "parser.add_argument('--vladv2', action='store_true', help='Use VLAD v2')\n",
    "parser.add_argument('--pooling', type=str, default='netvlad', help='type of pooling to use', choices=['netvlad', 'max', 'avg'])\n",
    "parser.add_argument('--num_clusters', type=int, default=64, help='Number of NetVlad clusters. Default=64')\n",
    "parser.add_argument('--margin', type=float, default=0.1, help='Margin for triplet loss. Default=0.1')\n",
    "parser.add_argument('--split', type=str, default='val', help='Data split to use for testing. Default is val', choices=['test', 'test250k', 'train', 'val'])\n",
    "parser.add_argument('--fromscratch', action='store_true', help='Train from scratch rather than using pretrained models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "from os.path import join, exists\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import h5py\n",
    "\n",
    "root_dir = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/'\n",
    "queries_dir = root_dir\n",
    "\n",
    "def input_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def get_whole_training_set(onlyDB=False):\n",
    "    return WholeDatasetFromStruct(input_transform=input_transform(), onlyDB=onlyDB, mode='train')\n",
    "\n",
    "def get_training_query_set(margin=0.1):\n",
    "    return QueryDatasetFromStruct(input_transform=input_transform(), margin=margin, mode='train')\n",
    "\n",
    "def get_whole_val_set():\n",
    "    return WholeDatasetFromStruct(input_transform=input_transform(), mode='val')\n",
    "\n",
    "dbStruct = namedtuple('dbStruct', ['whichSet', 'dataset', \n",
    "                                   'db_image', 'db_utms', 'db_num', 'db_full_pose',\n",
    "                                   'q_image', 'q_utms', 'q_num', 'q_full_pose',\n",
    "                                   'posDistThr', 'posDistSqThr', 'nonTrivPosDistSqThr'])\n",
    "\n",
    "class WholeDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, input_transform=None, onlyDB=False, mode='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "        self.dbStruct = my_parse_dbStruct(mode)\n",
    "        self.images = [join(root_dir, dbIm) for dbIm in self.dbStruct.db_image]\n",
    "        if not onlyDB:\n",
    "            self.images += [join(queries_dir, qIm) for qIm in self.dbStruct.q_image]\n",
    "\n",
    "        self.whichSet = self.dbStruct.whichSet\n",
    "        self.dataset = self.dbStruct.dataset\n",
    "\n",
    "        self.positives = None\n",
    "        self.distances = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index])\n",
    "        img = img.resize((640, 480))\n",
    "\n",
    "        if self.input_transform:\n",
    "            img = self.input_transform(img)\n",
    "\n",
    "        return img, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def getPositives(self):\n",
    "        # positives for evaluation are those within trivial threshold range\n",
    "        #fit NN to find them, search by radius\n",
    "        if  self.positives is None:\n",
    "            knn = NearestNeighbors(n_jobs=-1)\n",
    "            knn.fit(self.dbStruct.db_utms)\n",
    "\n",
    "            self.distances, self.positives = knn.radius_neighbors(self.dbStruct.q_utms, radius=self.dbStruct.posDistThr)\n",
    "\n",
    "        return self.positives\n",
    "\n",
    "class QueryDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, nNegSample=1000, nNeg=10, margin=0.1, input_transform=None, mode='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.margin = margin\n",
    "\n",
    "        self.dbStruct = my_parse_dbStruct(mode)\n",
    "        self.whichSet = self.dbStruct.whichSet\n",
    "        self.dataset = self.dbStruct.dataset\n",
    "        self.nNegSample = nNegSample # number of negatives to randomly sample\n",
    "        self.nNeg = nNeg # number of negatives used for training\n",
    "\n",
    "        # potential positives are those within nontrivial threshold range\n",
    "        #fit NN to find them, search by radius\n",
    "        knn = NearestNeighbors(n_jobs=-1)\n",
    "        knn.fit(self.dbStruct.db_utms)\n",
    "\n",
    "        # TODO use sqeuclidean as metric?\n",
    "        self.nontrivial_positives = list(knn.radius_neighbors(self.dbStruct.q_utms,\n",
    "                radius=self.dbStruct.nonTrivPosDistSqThr**0.5, \n",
    "                return_distance=False))\n",
    "        # radius returns unsorted, sort once now so we dont have to later\n",
    "        for i,posi in enumerate(self.nontrivial_positives):\n",
    "            self.nontrivial_positives[i] = np.sort(posi)\n",
    "        # its possible some queries don't have any non trivial potential positives\n",
    "        # lets filter those out\n",
    "        self.queries = np.where(np.array([len(x) for x in self.nontrivial_positives])>0)[0]\n",
    "\n",
    "        # potential negatives are those outside of posDistThr range\n",
    "        potential_positives = knn.radius_neighbors(self.dbStruct.q_utms,\n",
    "                radius=self.dbStruct.posDistThr, \n",
    "                return_distance=False)\n",
    "\n",
    "        self.potential_negatives = []\n",
    "        for pos in potential_positives:\n",
    "            self.potential_negatives.append(np.setdiff1d(np.arange(self.dbStruct.db_num),\n",
    "                pos, assume_unique=True))\n",
    "\n",
    "        self.cache = None # filepath of HDF5 containing feature vectors for images\n",
    "\n",
    "        self.negCache = [np.empty((0,)) for _ in range(self.dbStruct.q_num)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.queries[index] # re-map index to match dataset\n",
    "        with h5py.File(self.cache, mode='r') as h5: \n",
    "            h5feat = h5.get(\"features\")\n",
    "\n",
    "            qOffset = self.dbStruct.db_num \n",
    "            qFeat = h5feat[index+qOffset]\n",
    "\n",
    "            posFeat = h5feat[self.nontrivial_positives[index].tolist()]\n",
    "            knn = NearestNeighbors(n_jobs=-1) # TODO replace with faiss?\n",
    "            knn.fit(posFeat)\n",
    "            dPos, posNN = knn.kneighbors(qFeat.reshape(1,-1), 1)\n",
    "            dPos = dPos.item()\n",
    "            posIndex = self.nontrivial_positives[index][posNN[0]].item()\n",
    "\n",
    "            negSample = np.random.choice(self.potential_negatives[index], self.nNegSample)\n",
    "            negSample = np.unique(np.concatenate([self.negCache[index], negSample]))\n",
    "\n",
    "            negFeat = h5feat[negSample.tolist()]\n",
    "            knn.fit(negFeat)\n",
    "\n",
    "            dNeg, negNN = knn.kneighbors(qFeat.reshape(1,-1), \n",
    "                    self.nNeg*10) # to quote netvlad paper code: 10x is hacky but fine\n",
    "            dNeg = dNeg.reshape(-1)\n",
    "            negNN = negNN.reshape(-1)\n",
    "\n",
    "            # try to find negatives that are within margin, if there aren't any return none\n",
    "            violatingNeg = dNeg < dPos + self.margin**0.5\n",
    "     \n",
    "            if np.sum(violatingNeg) < 1:\n",
    "                #if none are violating then skip this query\n",
    "                return None\n",
    "\n",
    "            negNN = negNN[violatingNeg][:self.nNeg]\n",
    "            negIndices = negSample[negNN].astype(np.int32)\n",
    "            self.negCache[index] = negIndices\n",
    "\n",
    "        query = Image.open(join(queries_dir, self.dbStruct.q_image[index]))\n",
    "        query = query.resize((640, 480))\n",
    "        positive = Image.open(join(root_dir, self.dbStruct.db_image[posIndex]))\n",
    "        positive = positive.resize((640, 480))\n",
    "\n",
    "        if self.input_transform:\n",
    "            query = self.input_transform(query)\n",
    "            positive = self.input_transform(positive)\n",
    "\n",
    "        negatives = []\n",
    "        for negIndex in negIndices:\n",
    "            negative = Image.open(join(root_dir, self.dbStruct.db_image[negIndex]))\n",
    "            negative = negative.resize((640, 480))\n",
    "            if self.input_transform:\n",
    "                negative = self.input_transform(negative)\n",
    "            negatives.append(negative)\n",
    "\n",
    "        negatives = torch.stack(negatives, 0)\n",
    "\n",
    "        return query, positive, negatives, [index, posIndex]+negIndices.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_parse_dbStruct(_whichSet='train'):\n",
    "\n",
    "    whichSet = _whichSet\n",
    "    dataset = 'naverlabs'\n",
    "    image_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/images'\n",
    "    image_files_list = []\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970285_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970286_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970288_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970289_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970290_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970291_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324954_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324955_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324968_*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324969_*.jpg'))))\n",
    "    image_files = np.hstack(image_files_list)\n",
    "    \n",
    "    full_images = [os.path.join('images', image_file.split('/')[-1]) for image_file in image_files]\n",
    "    \n",
    "    gt_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/groundtruth.hdf5'\n",
    "\n",
    "    full_pose = []\n",
    "    with h5py.File(gt_path, \"r\") as f:\n",
    "        full_pose.append(np.array(f['22970285_pose']))\n",
    "        full_pose.append(np.array(f['22970286_pose']))\n",
    "        full_pose.append(np.array(f['22970288_pose']))\n",
    "        full_pose.append(np.array(f['22970289_pose']))\n",
    "        full_pose.append(np.array(f['22970290_pose']))\n",
    "        full_pose.append(np.array(f['22970291_pose']))\n",
    "        full_pose.append(np.array(f['AC01324954_pose']))\n",
    "        full_pose.append(np.array(f['AC01324955_pose']))\n",
    "        full_pose.append(np.array(f['AC01324968_pose']))\n",
    "        full_pose.append(np.array(f['AC01324969_pose']))\n",
    "\n",
    "        full_pose = np.vstack(full_pose)\n",
    "    \n",
    "    full_utms = full_pose[:,:2]\n",
    "    \n",
    "    db_image = full_images[:query_val_partition_idx]\n",
    "    db_utms = full_utms[:query_val_partition_idx]\n",
    "    db_num = len(db_image)\n",
    "    db_full_pose = full_pose[:query_val_partition_idx]\n",
    "    \n",
    "    q_image = full_images[query_val_partition_idx:]\n",
    "    q_utms = full_utms[query_val_partition_idx:]\n",
    "    q_num = len(q_image)\n",
    "    q_full_pose = full_pose[query_val_partition_idx:]\n",
    "    \n",
    "\n",
    "    return dbStruct(whichSet, dataset, \n",
    "                    db_image, db_utms, db_num, db_full_pose,\n",
    "                    q_image, q_utms, q_num, q_full_pose, \n",
    "                    5, 25, 20.0)\n",
    "\n",
    "# dbStruct = namedtuple('dbStruct', ['whichSet', 'dataset', \n",
    "#                                    'db_image', 'db_utms', 'db_num', 'db_full_pose',\n",
    "#                                    'q_image', 'q_utms', 'q_num', 'q_full_pose',\n",
    "#                                    'posDistThr', 'posDistSqThr', 'nonTrivPosDistSqThr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (query, positive, negatives).\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuple (query, positive, negatives). \n",
    "            - query: torch tensor of shape (3, h, w).\n",
    "            - positive: torch tensor of shape (3, h, w).\n",
    "            - negative: torch tensor of shape (n, 3, h, w).\n",
    "    Returns:\n",
    "        query: torch tensor of shape (batch_size, 3, h, w).\n",
    "        positive: torch tensor of shape (batch_size, 3, h, w).\n",
    "        negatives: torch tensor of shape (batch_size, n, 3, h, w).\n",
    "    \"\"\"\n",
    "\n",
    "    batch = list(filter (lambda x:x is not None, batch))\n",
    "    if len(batch) == 0: return None, None, None, None, None\n",
    "\n",
    "    query, positive, negatives, indices = zip(*batch)\n",
    "\n",
    "    query = data.dataloader.default_collate(query)\n",
    "    positive = data.dataloader.default_collate(positive)\n",
    "    negCounts = data.dataloader.default_collate([x.shape[0] for x in negatives])\n",
    "    negatives = torch.cat(negatives, 0)\n",
    "    import itertools\n",
    "    indices = list(itertools.chain(*indices))\n",
    "\n",
    "    return query, positive, negatives, negCounts, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    epoch_loss = 0\n",
    "    startIter = 1 # keep track of batch iter across subsets for logging\n",
    "\n",
    "    if opt.cacheRefreshRate > 0:\n",
    "        subsetN = ceil(len(train_set) / opt.cacheRefreshRate)\n",
    "        #TODO randomise the arange before splitting?\n",
    "        subsetIdx = np.array_split(np.arange(len(train_set)), subsetN)\n",
    "    else:\n",
    "        subsetN = 1\n",
    "        subsetIdx = [np.arange(len(train_set))]\n",
    "\n",
    "    nBatches = (len(train_set) + opt.batchSize - 1) // opt.batchSize\n",
    "\n",
    "    for subIter in range(subsetN):\n",
    "        print('====> Building Cache')\n",
    "        model.eval()\n",
    "        train_set.cache = join(opt.cachePath, train_set.whichSet + G_cache_name)\n",
    "        with h5py.File(train_set.cache, mode='w') as h5: \n",
    "            pool_size = encoder_dim\n",
    "            if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters\n",
    "            h5feat = h5.create_dataset(\"features\", \n",
    "                    [len(whole_train_set), pool_size], \n",
    "                    dtype=np.float32)\n",
    "            with torch.no_grad():\n",
    "                for iteration, (input, indices) in enumerate(whole_training_data_loader, 1):\n",
    "                    input = input.to(device)\n",
    "                    image_encoding = model.encoder(input)\n",
    "                    vlad_encoding = model.pool(image_encoding) \n",
    "                    h5feat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()\n",
    "                    del input, image_encoding, vlad_encoding\n",
    "\n",
    "        sub_train_set = Subset(dataset=train_set, indices=subsetIdx[subIter])\n",
    "\n",
    "        training_data_loader = DataLoader(dataset=sub_train_set, num_workers=opt.threads, \n",
    "                    batch_size=opt.batchSize, shuffle=True, \n",
    "                    collate_fn=collate_fn, pin_memory=cuda)\n",
    "\n",
    "        print('Allocated:', torch.cuda.memory_allocated())\n",
    "        print('Cached:', torch.cuda.memory_cached())\n",
    "\n",
    "        model.train()\n",
    "        for iteration, (query, positives, negatives, negCounts, indices) in enumerate(training_data_loader, startIter):\n",
    "            # some reshaping to put query, pos, negs in a single (N, 3, H, W) tensor\n",
    "            # where N = batchSize * (nQuery + nPos + nNeg)\n",
    "            if query is None: continue # in case we get an empty batch\n",
    "\n",
    "            B, C, H, W = query.shape\n",
    "            nNeg = torch.sum(negCounts)\n",
    "            input = torch.cat([query, positives, negatives])\n",
    "\n",
    "            input = input.to(device)\n",
    "            image_encoding = model.encoder(input)\n",
    "            vlad_encoding = model.pool(image_encoding) \n",
    "\n",
    "            vladQ, vladP, vladN = torch.split(vlad_encoding, [B, B, nNeg])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # calculate loss for each Query, Positive, Negative triplet\n",
    "            # due to potential difference in number of negatives have to \n",
    "            # do it per query, per negative\n",
    "            loss = 0\n",
    "            for i, negCount in enumerate(negCounts):\n",
    "                for n in range(negCount):\n",
    "                    negIx = (torch.sum(negCounts[:i]) + n).item()\n",
    "                    loss += criterion(vladQ[i:i+1], vladP[i:i+1], vladN[negIx:negIx+1])\n",
    "\n",
    "            loss /= nNeg.float().to(device) # normalise by actual number of negatives\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del input, image_encoding, vlad_encoding, vladQ, vladP, vladN\n",
    "            del query, positives, negatives\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            if iteration % 50 == 0 or nBatches <= 10:\n",
    "                print(\"==> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, \n",
    "                    nBatches, batch_loss), flush=True)\n",
    "                writer.add_scalar('Train/Loss', batch_loss, \n",
    "                        ((epoch-1) * nBatches) + iteration)\n",
    "                writer.add_scalar('Train/nNeg', nNeg, \n",
    "                        ((epoch-1) * nBatches) + iteration)\n",
    "                print('Allocated:', torch.cuda.memory_allocated())\n",
    "                print('Cached:', torch.cuda.memory_cached())\n",
    "\n",
    "        startIter += len(training_data_loader)\n",
    "        del training_data_loader, loss\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        remove(train_set.cache) # delete HDF5 cache\n",
    "\n",
    "    avg_loss = epoch_loss / nBatches\n",
    "\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, avg_loss), \n",
    "            flush=True)\n",
    "    writer.add_scalar('Train/AvgLoss', avg_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(eval_set, epoch=0, write_tboard=False):\n",
    "    # TODO what if features dont fit in memory? \n",
    "    test_data_loader = DataLoader(dataset=eval_set, \n",
    "                num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False, \n",
    "                pin_memory=cuda)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('====> Extracting Features')\n",
    "        pool_size = encoder_dim\n",
    "        if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters\n",
    "        dbFeat = np.empty((len(eval_set), pool_size))\n",
    "\n",
    "        for iteration, (input, indices) in enumerate(test_data_loader, 1):\n",
    "            input = input.to(device)\n",
    "            image_encoding = model.encoder(input)\n",
    "            vlad_encoding = model.pool(image_encoding) \n",
    "\n",
    "            dbFeat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()\n",
    "            if iteration % 50 == 0 or len(test_data_loader) <= 10:\n",
    "                print(\"==> Batch ({}/{})\".format(iteration, \n",
    "                    len(test_data_loader)), flush=True)\n",
    "\n",
    "            del input, image_encoding, vlad_encoding\n",
    "    del test_data_loader\n",
    "\n",
    "    # extracted for both db and query, now split in own sets\n",
    "    qFeat = dbFeat[eval_set.dbStruct.db_num:].astype('float32')\n",
    "    dbFeat = dbFeat[:eval_set.dbStruct.db_num].astype('float32')\n",
    "    \n",
    "    print('====> Building faiss index')\n",
    "    faiss_index = faiss.IndexFlatL2(pool_size)\n",
    "    faiss_index.add(dbFeat)\n",
    "\n",
    "    print('====> Calculating recall @ N')\n",
    "    n_values = [1,5,10,20]\n",
    "\n",
    "    _, predictions = faiss_index.search(qFeat, max(n_values)) \n",
    "\n",
    "    # for each query get those within threshold distance\n",
    "    gt = eval_set.getPositives() \n",
    "\n",
    "    correct_at_n = np.zeros(len(n_values))\n",
    "    #TODO can we do this on the matrix in one go?\n",
    "    for qIx, pred in enumerate(predictions):\n",
    "        for i,n in enumerate(n_values):\n",
    "            # if in top N then also in top NN, where NN > N\n",
    "            if np.any(np.in1d(pred[:n], gt[qIx])):\n",
    "                correct_at_n[i:] += 1\n",
    "                break\n",
    "    recall_at_n = correct_at_n / eval_set.dbStruct.q_num\n",
    "\n",
    "    recalls = {} #make dict for output\n",
    "    for i,n in enumerate(n_values):\n",
    "        recalls[n] = recall_at_n[i]\n",
    "        print(\"====> Recall@{}: {:.4f}\".format(n, recall_at_n[i]))\n",
    "        if write_tboard: writer.add_scalar('Val/Recall@' + str(n), recall_at_n[i], epoch)\n",
    "\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    model_out_path = join(opt.savePath, filename)\n",
    "    torch.save(state, model_out_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(model_out_path, join(opt.savePath, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='vgg16', batchSize=4, cacheBatchSize=24, cachePath='/tmp', cacheRefreshRate=1000, ckpt='latest', dataPath='/home/ubuntu/Desktop/pytorch-NetVlad/data/', dataset='naverlabs', evalEvery=1, fromscratch=False, lr=0.0001, lrGamma=0.5, lrStep=5, margin=0.1, mode='train', momentum=0.9, nEpochs=30, nGPU=1, nocuda=False, num_clusters=64, optim='SGD', patience=10, pooling='netvlad', resume='', runsPath='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', savePath='checkpoints', seed=123, split='val', start_epoch=0, threads=8, vladv2=False, weightDecay=0.001)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args(args='--dataset=naverlabs --mode=train --arch=vgg16 --pooling=netvlad --num_clusters=64'.split(' '))\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = not opt.nocuda\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Training query set: 6606\n",
      "===> Evaluating on val set, query count: 6606\n"
     ]
    }
   ],
   "source": [
    "whole_train_set = get_whole_training_set()\n",
    "whole_training_data_loader = DataLoader(dataset=whole_train_set, \n",
    "        num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False, \n",
    "        pin_memory=cuda)\n",
    "\n",
    "train_set = get_training_query_set(opt.margin)\n",
    "\n",
    "print('====> Training query set:', len(train_set))\n",
    "whole_test_set = get_whole_val_set()\n",
    "print('===> Evaluating on val set, query count:', whole_test_set.dbStruct.q_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = not opt.fromscratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = 512\n",
    "encoder = models.vgg16(pretrained=pretrained)\n",
    "# capture only feature part and remove last relu and maxpool\n",
    "layers = list(encoder.features.children())[:-2]\n",
    "\n",
    "if pretrained:\n",
    "    # if using pretrained then only train conv5_1, conv5_2, and conv5_3\n",
    "    for l in layers[:-5]: \n",
    "        for p in l.parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(*layers)\n",
    "model = nn.Module() \n",
    "model.add_module('encoder', encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_vlad = netvlad.NetVLAD(num_clusters=opt.num_clusters, dim=encoder_dim, vladv2=opt.vladv2)\n",
    "if not opt.resume: \n",
    "    if opt.mode.lower() == 'train':\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + train_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "    else:\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + whole_test_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "\n",
    "    if not exists(initcache):\n",
    "        raise FileNotFoundError('Could not find clusters, please run with --mode=cluster before proceeding')\n",
    "\n",
    "    with h5py.File(initcache, mode='r') as h5: \n",
    "        clsts = h5.get(\"centroids\")[...]\n",
    "        traindescs = h5.get(\"descriptors\")[...]\n",
    "        net_vlad.init_params(clsts, traindescs) \n",
    "        del clsts, traindescs\n",
    "\n",
    "model.add_module('pool', net_vlad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "isParallel = False\n",
    "if opt.nGPU > 1 and torch.cuda.device_count() > 1:\n",
    "    model.encoder = nn.DataParallel(model.encoder)\n",
    "    if opt.mode.lower() != 'cluster':\n",
    "        model.pool = nn.DataParallel(model.pool)\n",
    "    isParallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.optim.upper() == 'ADAM':\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, \n",
    "        model.parameters()), lr=opt.lr)#, betas=(0,0.9))\n",
    "elif opt.optim.upper() == 'SGD':\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, \n",
    "        model.parameters()), lr=opt.lr,\n",
    "        momentum=opt.momentum,\n",
    "        weight_decay=opt.weightDecay)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lrStep, gamma=opt.lrGamma)\n",
    "else:\n",
    "    raise ValueError('Unknown optimizer: ' + opt.optim)\n",
    "\n",
    "# original paper/code doesn't sqrt() the distances, we do, so sqrt() the margin, I think :D\n",
    "criterion = nn.TripletMarginLoss(margin=opt.margin**0.5, p=2, reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Training model\n",
      "===> Saving state to: /home/ubuntu/Desktop/pytorch-NetVlad/runs/Jun01_22-29-30_vgg16_netvlad\n",
      "====> Building Cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 60039168\n",
      "Cached: 13847494656\n",
      "==> Epoch[1](50/1652): Loss: 0.1413\n",
      "Allocated: 117199360\n",
      "Cached: 28730982400\n",
      "==> Epoch[1](100/1652): Loss: 0.1543\n",
      "Allocated: 117199360\n",
      "Cached: 28730982400\n",
      "==> Epoch[1](150/1652): Loss: 0.0921\n",
      "Allocated: 117199360\n",
      "Cached: 28730982400\n",
      "==> Epoch[1](200/1652): Loss: 0.1813\n",
      "Allocated: 117199360\n",
      "Cached: 28730982400\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](250/1652): Loss: 0.2079\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](300/1652): Loss: 0.2241\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](350/1652): Loss: 0.1750\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](400/1652): Loss: 0.1755\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](450/1652): Loss: 0.2113\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](500/1652): Loss: 0.2666\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](550/1652): Loss: 0.2238\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](600/1652): Loss: 0.2618\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](650/1652): Loss: 0.2318\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](700/1652): Loss: 0.1738\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](750/1652): Loss: 0.2579\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](800/1652): Loss: 0.2154\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](850/1652): Loss: 0.2323\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](900/1652): Loss: 0.2419\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](950/1652): Loss: 0.2178\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1000/1652): Loss: 0.1953\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1050/1652): Loss: 0.2491\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1100/1652): Loss: 0.2229\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1150/1652): Loss: 0.2036\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](1200/1652): Loss: 0.2499\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1250/1652): Loss: 0.2155\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1300/1652): Loss: 0.2230\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1350/1652): Loss: 0.1930\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1400/1652): Loss: 0.2239\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](1450/1652): Loss: 0.2538\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1500/1652): Loss: 0.2030\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1550/1652): Loss: 0.2076\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1600/1652): Loss: 0.2220\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1650/1652): Loss: 0.2434\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 1 Complete: Avg. Loss: 0.2139\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.8648\n",
      "====> Recall@5: 0.9561\n",
      "====> Recall@10: 0.9767\n",
      "====> Recall@20: 0.9888\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](50/1652): Loss: 0.1136\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](100/1652): Loss: 0.1558\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](150/1652): Loss: 0.1582\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](200/1652): Loss: 0.1978\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](250/1652): Loss: 0.1876\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](300/1652): Loss: 0.2283\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](350/1652): Loss: 0.2403\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](400/1652): Loss: 0.2306\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](450/1652): Loss: 0.2115\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](500/1652): Loss: 0.2425\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](550/1652): Loss: 0.2073\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](600/1652): Loss: 0.2520\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](650/1652): Loss: 0.2191\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](700/1652): Loss: 0.2523\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](750/1652): Loss: 0.2444\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](800/1652): Loss: 0.1963\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](850/1652): Loss: 0.2305\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](900/1652): Loss: 0.2202\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](950/1652): Loss: 0.2335\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1000/1652): Loss: 0.2268\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1050/1652): Loss: 0.2414\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1100/1652): Loss: 0.2374\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1150/1652): Loss: 0.1422\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](1200/1652): Loss: 0.2615\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1250/1652): Loss: 0.1731\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1300/1652): Loss: 0.1954\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1350/1652): Loss: 0.2393\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1400/1652): Loss: 0.2107\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](1450/1652): Loss: 0.2255\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1500/1652): Loss: 0.2365\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1550/1652): Loss: 0.2176\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1600/1652): Loss: 0.2306\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1650/1652): Loss: 0.2328\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 2 Complete: Avg. Loss: 0.2077\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.8800\n",
      "====> Recall@5: 0.9661\n",
      "====> Recall@10: 0.9814\n",
      "====> Recall@20: 0.9936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](50/1652): Loss: 0.1018\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](100/1652): Loss: 0.0902\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](150/1652): Loss: 0.1715\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](200/1652): Loss: 0.1827\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](250/1652): Loss: 0.1413\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](300/1652): Loss: 0.1778\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](350/1652): Loss: 0.1814\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](400/1652): Loss: 0.2283\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](450/1652): Loss: 0.2499\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](500/1652): Loss: 0.2552\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](550/1652): Loss: 0.2288\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](600/1652): Loss: 0.2080\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](650/1652): Loss: 0.1586\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](700/1652): Loss: 0.2230\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](750/1652): Loss: 0.2269\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](800/1652): Loss: 0.1861\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](850/1652): Loss: 0.2402\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](900/1652): Loss: 0.2466\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](950/1652): Loss: 0.2142\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1000/1652): Loss: 0.1893\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1050/1652): Loss: 0.2267\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1100/1652): Loss: 0.1982\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1150/1652): Loss: 0.1981\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](1200/1652): Loss: 0.1693\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1250/1652): Loss: 0.2247\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1300/1652): Loss: 0.2216\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1350/1652): Loss: 0.2043\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1400/1652): Loss: 0.2227\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](1450/1652): Loss: 0.1818\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1500/1652): Loss: 0.1875\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1550/1652): Loss: 0.2239\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1600/1652): Loss: 0.2345\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1650/1652): Loss: 0.2443\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 3 Complete: Avg. Loss: 0.2034\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.8887\n",
      "====> Recall@5: 0.9691\n",
      "====> Recall@10: 0.9850\n",
      "====> Recall@20: 0.9955\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](50/1652): Loss: 0.1752\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](100/1652): Loss: 0.2062\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](150/1652): Loss: 0.1694\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](200/1652): Loss: 0.0606\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](250/1652): Loss: 0.1834\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](300/1652): Loss: 0.2032\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](350/1652): Loss: 0.1893\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](400/1652): Loss: 0.1932\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](450/1652): Loss: 0.1715\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](500/1652): Loss: 0.2095\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](550/1652): Loss: 0.2174\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](600/1652): Loss: 0.2150\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](650/1652): Loss: 0.2072\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](700/1652): Loss: 0.1916\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](750/1652): Loss: 0.2546\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](800/1652): Loss: 0.2371\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](850/1652): Loss: 0.1962\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](900/1652): Loss: 0.2211\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](950/1652): Loss: 0.1930\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1000/1652): Loss: 0.2479\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1050/1652): Loss: 0.2462\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1100/1652): Loss: 0.1677\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1150/1652): Loss: 0.2298\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](1200/1652): Loss: 0.1965\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1250/1652): Loss: 0.2213\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1300/1652): Loss: 0.1955\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1350/1652): Loss: 0.2190\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1400/1652): Loss: 0.2086\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](1450/1652): Loss: 0.1698\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1500/1652): Loss: 0.2243\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1550/1652): Loss: 0.1931\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1600/1652): Loss: 0.2484\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1650/1652): Loss: 0.1723\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 4 Complete: Avg. Loss: 0.1995\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.8913\n",
      "====> Recall@5: 0.9720\n",
      "====> Recall@10: 0.9874\n",
      "====> Recall@20: 0.9958\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](50/1652): Loss: 0.1054\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](100/1652): Loss: 0.1519\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](150/1652): Loss: 0.1081\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](200/1652): Loss: 0.1327\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](250/1652): Loss: 0.1861\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](300/1652): Loss: 0.1636\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](350/1652): Loss: 0.1506\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](400/1652): Loss: 0.1691\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](450/1652): Loss: 0.1938\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](500/1652): Loss: 0.2164\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](550/1652): Loss: 0.2361\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](600/1652): Loss: 0.2130\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](650/1652): Loss: 0.1982\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](700/1652): Loss: 0.2230\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](750/1652): Loss: 0.2205\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](800/1652): Loss: 0.2506\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](850/1652): Loss: 0.2097\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](900/1652): Loss: 0.1841\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](950/1652): Loss: 0.1707\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1000/1652): Loss: 0.2143\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1050/1652): Loss: 0.2157\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1100/1652): Loss: 0.2181\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1150/1652): Loss: 0.2138\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](1200/1652): Loss: 0.1823\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1250/1652): Loss: 0.1990\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1300/1652): Loss: 0.2489\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1350/1652): Loss: 0.1550\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1400/1652): Loss: 0.2144\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](1450/1652): Loss: 0.2004\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1500/1652): Loss: 0.1950\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1550/1652): Loss: 0.2128\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1600/1652): Loss: 0.1979\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1650/1652): Loss: 0.2214\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 5 Complete: Avg. Loss: 0.1971\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.8951\n",
      "====> Recall@5: 0.9731\n",
      "====> Recall@10: 0.9880\n",
      "====> Recall@20: 0.9959\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](50/1652): Loss: 0.1691\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](100/1652): Loss: 0.1071\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](150/1652): Loss: 0.1174\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](200/1652): Loss: 0.1703\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](250/1652): Loss: 0.1326\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](300/1652): Loss: 0.1765\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](350/1652): Loss: 0.1542\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](400/1652): Loss: 0.1973\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](450/1652): Loss: 0.2163\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](500/1652): Loss: 0.2134\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](550/1652): Loss: 0.2066\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](600/1652): Loss: 0.2106\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](650/1652): Loss: 0.2036\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](700/1652): Loss: 0.2381\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](750/1652): Loss: 0.2326\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](800/1652): Loss: 0.2372\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](850/1652): Loss: 0.1981\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](900/1652): Loss: 0.2184\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](950/1652): Loss: 0.1800\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1000/1652): Loss: 0.2088\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1050/1652): Loss: 0.1983\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1100/1652): Loss: 0.2091\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1150/1652): Loss: 0.2181\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](1200/1652): Loss: 0.2016\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1250/1652): Loss: 0.2065\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1300/1652): Loss: 0.2501\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1350/1652): Loss: 0.2540\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1400/1652): Loss: 0.2063\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](1450/1652): Loss: 0.2152\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1500/1652): Loss: 0.2035\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1550/1652): Loss: 0.1688\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1600/1652): Loss: 0.2213\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1650/1652): Loss: 0.2177\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 6 Complete: Avg. Loss: 0.1958\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.8966\n",
      "====> Recall@5: 0.9741\n",
      "====> Recall@10: 0.9889\n",
      "====> Recall@20: 0.9962\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](50/1652): Loss: 0.1570\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](100/1652): Loss: 0.0913\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](150/1652): Loss: 0.1979\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](200/1652): Loss: 0.1709\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](250/1652): Loss: 0.2169\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](300/1652): Loss: 0.1384\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](350/1652): Loss: 0.1267\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](400/1652): Loss: 0.1171\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](450/1652): Loss: 0.1631\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](500/1652): Loss: 0.2049\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](550/1652): Loss: 0.1973\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](600/1652): Loss: 0.2260\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](650/1652): Loss: 0.1956\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](700/1652): Loss: 0.1739\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](750/1652): Loss: 0.2438\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](800/1652): Loss: 0.2108\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](850/1652): Loss: 0.2491\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](900/1652): Loss: 0.1634\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](950/1652): Loss: 0.1594\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1000/1652): Loss: 0.1896\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1050/1652): Loss: 0.1999\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1100/1652): Loss: 0.2482\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1150/1652): Loss: 0.2117\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](1200/1652): Loss: 0.1674\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1250/1652): Loss: 0.2026\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1300/1652): Loss: 0.2113\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1350/1652): Loss: 0.1764\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1400/1652): Loss: 0.1742\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](1450/1652): Loss: 0.2249\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1500/1652): Loss: 0.1880\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1550/1652): Loss: 0.2473\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1600/1652): Loss: 0.1850\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1650/1652): Loss: 0.2115\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 7 Complete: Avg. Loss: 0.1940\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.8992\n",
      "====> Recall@5: 0.9749\n",
      "====> Recall@10: 0.9897\n",
      "====> Recall@20: 0.9961\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](50/1652): Loss: 0.1493\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](100/1652): Loss: 0.1238\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](150/1652): Loss: 0.1264\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](200/1652): Loss: 0.1818\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](250/1652): Loss: 0.2155\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](300/1652): Loss: 0.1834\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](350/1652): Loss: 0.1676\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](400/1652): Loss: 0.2318\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](450/1652): Loss: 0.1922\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](500/1652): Loss: 0.1743\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](550/1652): Loss: 0.2157\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](600/1652): Loss: 0.2081\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](650/1652): Loss: 0.2085\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](700/1652): Loss: 0.1920\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](750/1652): Loss: 0.2087\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](800/1652): Loss: 0.1928\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](850/1652): Loss: 0.1886\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](900/1652): Loss: 0.2341\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](950/1652): Loss: 0.1917\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1000/1652): Loss: 0.2477\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1050/1652): Loss: 0.1889\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1100/1652): Loss: 0.2230\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1150/1652): Loss: 0.2406\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](1200/1652): Loss: 0.1903\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1250/1652): Loss: 0.1261\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1300/1652): Loss: 0.2015\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1350/1652): Loss: 0.1671\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1400/1652): Loss: 0.1997\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](1450/1652): Loss: 0.2176\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1500/1652): Loss: 0.1974\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1550/1652): Loss: 0.1974\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1600/1652): Loss: 0.2010\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1650/1652): Loss: 0.2006\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 8 Complete: Avg. Loss: 0.1928\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.8998\n",
      "====> Recall@5: 0.9755\n",
      "====> Recall@10: 0.9899\n",
      "====> Recall@20: 0.9962\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](50/1652): Loss: 0.1515\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](100/1652): Loss: 0.1427\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](150/1652): Loss: 0.2040\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](200/1652): Loss: 0.0815\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](250/1652): Loss: 0.2095\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](300/1652): Loss: 0.1522\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](350/1652): Loss: 0.2192\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](400/1652): Loss: 0.1306\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](450/1652): Loss: 0.1711\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](500/1652): Loss: 0.2257\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](550/1652): Loss: 0.1596\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](600/1652): Loss: 0.1941\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](650/1652): Loss: 0.1924\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](700/1652): Loss: 0.1924\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](750/1652): Loss: 0.2280\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](800/1652): Loss: 0.2050\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](850/1652): Loss: 0.2081\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](900/1652): Loss: 0.2235\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](950/1652): Loss: 0.2292\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1000/1652): Loss: 0.1852\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1050/1652): Loss: 0.1748\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1100/1652): Loss: 0.2216\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1150/1652): Loss: 0.2405\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](1200/1652): Loss: 0.1511\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1250/1652): Loss: 0.1811\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1300/1652): Loss: 0.1632\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1350/1652): Loss: 0.2042\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1400/1652): Loss: 0.1913\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](1450/1652): Loss: 0.1911\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1500/1652): Loss: 0.1721\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1550/1652): Loss: 0.2194\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1600/1652): Loss: 0.1909\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1650/1652): Loss: 0.2001\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 9 Complete: Avg. Loss: 0.1916\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.9012\n",
      "====> Recall@5: 0.9765\n",
      "====> Recall@10: 0.9899\n",
      "====> Recall@20: 0.9965\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](50/1652): Loss: 0.1123\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](100/1652): Loss: 0.1250\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](150/1652): Loss: 0.0987\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](200/1652): Loss: 0.1613\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](250/1652): Loss: 0.1980\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](300/1652): Loss: 0.1835\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](350/1652): Loss: 0.1780\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](400/1652): Loss: 0.1785\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](450/1652): Loss: 0.1799\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](500/1652): Loss: 0.1678\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](550/1652): Loss: 0.1980\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](600/1652): Loss: 0.1999\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](650/1652): Loss: 0.1498\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](700/1652): Loss: 0.1447\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](750/1652): Loss: 0.2266\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](800/1652): Loss: 0.2210\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](850/1652): Loss: 0.2185\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](900/1652): Loss: 0.2107\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](950/1652): Loss: 0.1895\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1000/1652): Loss: 0.2123\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1050/1652): Loss: 0.1712\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1100/1652): Loss: 0.1942\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1150/1652): Loss: 0.2075\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](1200/1652): Loss: 0.1726\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1250/1652): Loss: 0.1551\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1300/1652): Loss: 0.1675\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1350/1652): Loss: 0.2182\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1400/1652): Loss: 0.1827\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](1450/1652): Loss: 0.2022\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1500/1652): Loss: 0.2149\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1550/1652): Loss: 0.2362\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1600/1652): Loss: 0.1798\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1650/1652): Loss: 0.2124\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 10 Complete: Avg. Loss: 0.1908\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.9013\n",
      "====> Recall@5: 0.9770\n",
      "====> Recall@10: 0.9899\n",
      "====> Recall@20: 0.9967\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](50/1652): Loss: 0.1286\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](100/1652): Loss: 0.1819\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](150/1652): Loss: 0.0686\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](200/1652): Loss: 0.0619\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](250/1652): Loss: 0.1611\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](300/1652): Loss: 0.1789\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](350/1652): Loss: 0.1736\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](400/1652): Loss: 0.1604\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](450/1652): Loss: 0.2015\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](500/1652): Loss: 0.1990\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](550/1652): Loss: 0.1927\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](600/1652): Loss: 0.2001\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](650/1652): Loss: 0.2339\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](700/1652): Loss: 0.2174\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](750/1652): Loss: 0.2089\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](800/1652): Loss: 0.2156\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](850/1652): Loss: 0.2186\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](900/1652): Loss: 0.1974\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](950/1652): Loss: 0.2152\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1000/1652): Loss: 0.2083\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1050/1652): Loss: 0.1845\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1100/1652): Loss: 0.1706\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1150/1652): Loss: 0.2079\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](1200/1652): Loss: 0.1575\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1250/1652): Loss: 0.2345\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1300/1652): Loss: 0.1719\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1350/1652): Loss: 0.1712\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1400/1652): Loss: 0.1680\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](1450/1652): Loss: 0.1874\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1500/1652): Loss: 0.2282\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1550/1652): Loss: 0.1666\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1600/1652): Loss: 0.2145\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1650/1652): Loss: 0.1611\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 11 Complete: Avg. Loss: 0.1899\n",
      "====> Extracting Features\n",
      "==> Batch (50/901)\n",
      "==> Batch (100/901)\n",
      "==> Batch (150/901)\n",
      "==> Batch (200/901)\n",
      "==> Batch (250/901)\n",
      "==> Batch (300/901)\n",
      "==> Batch (350/901)\n",
      "==> Batch (400/901)\n",
      "==> Batch (450/901)\n",
      "==> Batch (500/901)\n",
      "==> Batch (550/901)\n",
      "==> Batch (600/901)\n",
      "==> Batch (650/901)\n",
      "==> Batch (700/901)\n",
      "==> Batch (750/901)\n",
      "==> Batch (800/901)\n",
      "==> Batch (850/901)\n",
      "==> Batch (900/901)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.9022\n",
      "====> Recall@5: 0.9777\n",
      "====> Recall@10: 0.9900\n",
      "====> Recall@20: 0.9967\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[12](50/1652): Loss: 0.1514\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](100/1652): Loss: 0.1339\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](150/1652): Loss: 0.1368\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](200/1652): Loss: 0.1820\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[12](250/1652): Loss: 0.1230\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](300/1652): Loss: 0.1528\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](350/1652): Loss: 0.1843\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](400/1652): Loss: 0.2123\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](450/1652): Loss: 0.1675\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    }
   ],
   "source": [
    "print('===> Training model')\n",
    "writer = SummaryWriter(log_dir=join(opt.runsPath, datetime.now().strftime('%b%d_%H-%M-%S')+'_'+opt.arch+'_'+opt.pooling))\n",
    "\n",
    "# write checkpoints in logdir\n",
    "logdir = writer.file_writer.get_logdir()\n",
    "opt.savePath = join(logdir, opt.savePath)\n",
    "if not opt.resume:\n",
    "    makedirs(opt.savePath)\n",
    "\n",
    "with open(join(opt.savePath, 'flags.json'), 'w') as f:\n",
    "    f.write(json.dumps(\n",
    "        {k:v for k,v in vars(opt).items()}\n",
    "        ))\n",
    "print('===> Saving state to:', logdir)\n",
    "\n",
    "not_improved = 0\n",
    "best_score = 0\n",
    "for epoch in range(opt.start_epoch+1, opt.nEpochs + 1):\n",
    "    if opt.optim.upper() == 'SGD':\n",
    "        scheduler.step(epoch)\n",
    "    train(epoch)\n",
    "    if (epoch % opt.evalEvery) == 0:\n",
    "        recalls = test(whole_test_set, epoch, write_tboard=True)\n",
    "        is_best = recalls[5] > best_score \n",
    "        if is_best:\n",
    "            not_improved = 0\n",
    "            best_score = recalls[5]\n",
    "        else: \n",
    "            not_improved += 1\n",
    "\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'recalls': recalls,\n",
    "                'best_score': best_score,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'parallel' : isParallel,\n",
    "        }, is_best)\n",
    "\n",
    "        if opt.patience > 0 and not_improved > (opt.patience / opt.evalEvery):\n",
    "            print('Performance did not improve for', opt.patience, 'epochs. Stopping.')\n",
    "            break\n",
    "\n",
    "print(\"=> Best Recall@5: {:.4f}\".format(best_score), flush=True)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
