{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "import random, shutil, json\n",
    "from math import log10, ceil\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "import faiss\n",
    "\n",
    "import netvlad\n",
    "\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "from os import makedirs, remove, chdir, environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--fromscratch'], dest='fromscratch', nargs=0, const=True, default=False, type=None, choices=None, help='Train from scratch rather than using pretrained models', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='pytorch-NetVlad')\n",
    "parser.add_argument('--mode', type=str, default='train', help='Mode', choices=['train', 'test', 'cluster'])\n",
    "parser.add_argument('--batchSize', type=int, default=4, help='Number of triplets (query, pos, negs). Each triplet consists of 12 images.')\n",
    "parser.add_argument('--cacheBatchSize', type=int, default=24, help='Batch size for caching and testing')\n",
    "parser.add_argument('--cacheRefreshRate', type=int, default=1000, help='How often to refresh cache, in number of queries. 0 for off')\n",
    "parser.add_argument('--nEpochs', type=int, default=30, help='number of epochs to train for')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--nGPU', type=int, default=1, help='number of GPU to use.')\n",
    "parser.add_argument('--optim', type=str, default='SGD', help='optimizer to use', choices=['SGD', 'ADAM'])\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate.')\n",
    "parser.add_argument('--lrStep', type=float, default=5, help='Decay LR ever N steps.')\n",
    "parser.add_argument('--lrGamma', type=float, default=0.5, help='Multiply LR by Gamma for decaying.')\n",
    "parser.add_argument('--weightDecay', type=float, default=0.001, help='Weight decay for SGD.')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGD.')\n",
    "parser.add_argument('--nocuda', action='store_true', help='Dont use cuda')\n",
    "parser.add_argument('--threads', type=int, default=8, help='Number of threads for each data loader to use')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed to use.')\n",
    "parser.add_argument('--dataPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/data/', help='Path for centroid data.')\n",
    "parser.add_argument('--runsPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', help='Path to save runs to.')\n",
    "parser.add_argument('--savePath', type=str, default='checkpoints', help='Path to save checkpoints to in logdir. Default=checkpoints/')\n",
    "parser.add_argument('--cachePath', type=str, default='/tmp', help='Path to save cache to.')\n",
    "parser.add_argument('--resume', type=str, default='', help='Path to load checkpoint from, for resuming training or testing.')\n",
    "parser.add_argument('--ckpt', type=str, default='latest', help='Resume from latest or best checkpoint.', choices=['latest', 'best'])\n",
    "parser.add_argument('--evalEvery', type=int, default=1, help='Do a validation set run, and save, every N epochs.')\n",
    "parser.add_argument('--patience', type=int, default=10, help='Patience for early stopping. 0 is off.')\n",
    "parser.add_argument('--dataset', type=str, default='pittsburgh', help='Dataset to use', choices=['pittsburgh'])\n",
    "parser.add_argument('--arch', type=str, default='vgg16', help='basenetwork to use', choices=['vgg16', 'alexnet'])\n",
    "parser.add_argument('--vladv2', action='store_true', help='Use VLAD v2')\n",
    "parser.add_argument('--pooling', type=str, default='netvlad', help='type of pooling to use', choices=['netvlad', 'max', 'avg'])\n",
    "parser.add_argument('--num_clusters', type=int, default=64, help='Number of NetVlad clusters. Default=64')\n",
    "parser.add_argument('--margin', type=float, default=0.1, help='Margin for triplet loss. Default=0.1')\n",
    "parser.add_argument('--split', type=str, default='val', help='Data split to use for testing. Default is val', choices=['test', 'test250k', 'train', 'val'])\n",
    "parser.add_argument('--fromscratch', action='store_true', help='Train from scratch rather than using pretrained models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "from os.path import join, exists\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import h5py\n",
    "\n",
    "root_dir = '/home/ubuntu/Desktop/Pittsburgh'\n",
    "\n",
    "struct_dir = join(root_dir, 'datasets/')\n",
    "queries_dir = join(root_dir, 'queries_real')\n",
    "\n",
    "def input_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def get_whole_training_set(onlyDB=False):\n",
    "    structFile = join(struct_dir, 'pitts30k_train.mat')\n",
    "    return WholeDatasetFromStruct(structFile,\n",
    "                             input_transform=input_transform(),\n",
    "                             onlyDB=onlyDB)\n",
    "\n",
    "def get_training_query_set(margin=0.1):\n",
    "    structFile = join(struct_dir, 'pitts30k_train.mat')\n",
    "    return QueryDatasetFromStruct(structFile,\n",
    "                             input_transform=input_transform(), margin=margin)\n",
    "\n",
    "def get_whole_val_set():\n",
    "    structFile = join(struct_dir, 'pitts30k_val.mat')\n",
    "    return WholeDatasetFromStruct(structFile,\n",
    "                             input_transform=input_transform())\n",
    "\n",
    "dbStruct = namedtuple('dbStruct', ['whichSet', 'dataset', \n",
    "    'dbImage', 'utmDb', 'qImage', 'utmQ', 'numDb', 'numQ',\n",
    "    'posDistThr', 'posDistSqThr', 'nonTrivPosDistSqThr'])\n",
    "\n",
    "class WholeDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, structFile, input_transform=None, onlyDB=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "        self.dbStruct = parse_dbStruct(structFile)\n",
    "        self.images = [join(root_dir, dbIm) for dbIm in self.dbStruct.dbImage]\n",
    "        if not onlyDB:\n",
    "            self.images += [join(queries_dir, qIm) for qIm in self.dbStruct.qImage]\n",
    "\n",
    "        self.whichSet = self.dbStruct.whichSet\n",
    "        self.dataset = self.dbStruct.dataset\n",
    "\n",
    "        self.positives = None\n",
    "        self.distances = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index])\n",
    "\n",
    "        if self.input_transform:\n",
    "            img = self.input_transform(img)\n",
    "\n",
    "        return img, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def getPositives(self):\n",
    "        # positives for evaluation are those within trivial threshold range\n",
    "        #fit NN to find them, search by radius\n",
    "        if  self.positives is None:\n",
    "            knn = NearestNeighbors(n_jobs=-1)\n",
    "            knn.fit(self.dbStruct.utmDb)\n",
    "\n",
    "            self.distances, self.positives = knn.radius_neighbors(self.dbStruct.utmQ,\n",
    "                    radius=self.dbStruct.posDistThr)\n",
    "\n",
    "        return self.positives\n",
    "\n",
    "class QueryDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, structFile, nNegSample=1000, nNeg=10, margin=0.1, input_transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.margin = margin\n",
    "\n",
    "        self.dbStruct = parse_dbStruct(structFile)\n",
    "        self.whichSet = self.dbStruct.whichSet\n",
    "        self.dataset = self.dbStruct.dataset\n",
    "        self.nNegSample = nNegSample # number of negatives to randomly sample\n",
    "        self.nNeg = nNeg # number of negatives used for training\n",
    "\n",
    "        # potential positives are those within nontrivial threshold range\n",
    "        #fit NN to find them, search by radius\n",
    "        knn = NearestNeighbors(n_jobs=-1)\n",
    "        knn.fit(self.dbStruct.utmDb)\n",
    "\n",
    "        # TODO use sqeuclidean as metric?\n",
    "        self.nontrivial_positives = list(knn.radius_neighbors(self.dbStruct.utmQ,\n",
    "                radius=self.dbStruct.nonTrivPosDistSqThr**0.5, \n",
    "                return_distance=False))\n",
    "        # radius returns unsorted, sort once now so we dont have to later\n",
    "        for i,posi in enumerate(self.nontrivial_positives):\n",
    "            self.nontrivial_positives[i] = np.sort(posi)\n",
    "        # its possible some queries don't have any non trivial potential positives\n",
    "        # lets filter those out\n",
    "        self.queries = np.where(np.array([len(x) for x in self.nontrivial_positives])>0)[0]\n",
    "\n",
    "        # potential negatives are those outside of posDistThr range\n",
    "        potential_positives = knn.radius_neighbors(self.dbStruct.utmQ,\n",
    "                radius=self.dbStruct.posDistThr, \n",
    "                return_distance=False)\n",
    "\n",
    "        self.potential_negatives = []\n",
    "        for pos in potential_positives:\n",
    "            self.potential_negatives.append(np.setdiff1d(np.arange(self.dbStruct.numDb),\n",
    "                pos, assume_unique=True))\n",
    "\n",
    "        self.cache = None # filepath of HDF5 containing feature vectors for images\n",
    "\n",
    "        self.negCache = [np.empty((0,)) for _ in range(self.dbStruct.numQ)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.queries[index] # re-map index to match dataset\n",
    "        with h5py.File(self.cache, mode='r') as h5: \n",
    "            h5feat = h5.get(\"features\")\n",
    "\n",
    "            qOffset = self.dbStruct.numDb \n",
    "            print(index, qOffset)\n",
    "            qFeat = h5feat[index+qOffset]\n",
    "\n",
    "            posFeat = h5feat[self.nontrivial_positives[index].tolist()]\n",
    "            knn = NearestNeighbors(n_jobs=-1) # TODO replace with faiss?\n",
    "            knn.fit(posFeat)\n",
    "            dPos, posNN = knn.kneighbors(qFeat.reshape(1,-1), 1)\n",
    "            dPos = dPos.item()\n",
    "            posIndex = self.nontrivial_positives[index][posNN[0]].item()\n",
    "\n",
    "            negSample = np.random.choice(self.potential_negatives[index], self.nNegSample)\n",
    "            negSample = np.unique(np.concatenate([self.negCache[index], negSample]))\n",
    "\n",
    "            negFeat = h5feat[negSample.tolist()]\n",
    "            knn.fit(negFeat)\n",
    "\n",
    "            dNeg, negNN = knn.kneighbors(qFeat.reshape(1,-1), \n",
    "                    self.nNeg*10) # to quote netvlad paper code: 10x is hacky but fine\n",
    "            dNeg = dNeg.reshape(-1)\n",
    "            negNN = negNN.reshape(-1)\n",
    "\n",
    "            # try to find negatives that are within margin, if there aren't any return none\n",
    "            violatingNeg = dNeg < dPos + self.margin**0.5\n",
    "     \n",
    "            if np.sum(violatingNeg) < 1:\n",
    "                #if none are violating then skip this query\n",
    "                return None\n",
    "\n",
    "            negNN = negNN[violatingNeg][:self.nNeg]\n",
    "            negIndices = negSample[negNN].astype(np.int32)\n",
    "            self.negCache[index] = negIndices\n",
    "\n",
    "        query = Image.open(join(queries_dir, self.dbStruct.qImage[index]))\n",
    "        positive = Image.open(join(root_dir, self.dbStruct.dbImage[posIndex]))\n",
    "\n",
    "        if self.input_transform:\n",
    "            query = self.input_transform(query)\n",
    "            positive = self.input_transform(positive)\n",
    "\n",
    "        negatives = []\n",
    "        for negIndex in negIndices:\n",
    "            negative = Image.open(join(root_dir, self.dbStruct.dbImage[negIndex]))\n",
    "            if self.input_transform:\n",
    "                negative = self.input_transform(negative)\n",
    "            negatives.append(negative)\n",
    "\n",
    "        negatives = torch.stack(negatives, 0)\n",
    "\n",
    "        return query, positive, negatives, [index, posIndex]+negIndices.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dbStruct(path):\n",
    "    mat = loadmat(path)\n",
    "    matStruct = mat['dbStruct'].item()\n",
    "\n",
    "    if '250k' in path.split('/')[-1]:\n",
    "        dataset = 'pitts250k'\n",
    "    else:\n",
    "        dataset = 'pitts30k'\n",
    "\n",
    "    whichSet = matStruct[0].item()\n",
    "\n",
    "    dbImage = [f[0].item() for f in matStruct[1]]\n",
    "    utmDb = matStruct[2].T\n",
    "\n",
    "    qImage = [f[0].item() for f in matStruct[3]]\n",
    "    utmQ = matStruct[4].T\n",
    "\n",
    "    numDb = matStruct[5].item()\n",
    "    numQ = matStruct[6].item()\n",
    "\n",
    "    posDistThr = matStruct[7].item()\n",
    "    posDistSqThr = matStruct[8].item()\n",
    "    nonTrivPosDistSqThr = matStruct[9].item()\n",
    "\n",
    "    return dbStruct(whichSet, dataset, dbImage, utmDb, qImage, \n",
    "            utmQ, numDb, numQ, posDistThr, \n",
    "            posDistSqThr, nonTrivPosDistSqThr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (query, positive, negatives).\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuple (query, positive, negatives). \n",
    "            - query: torch tensor of shape (3, h, w).\n",
    "            - positive: torch tensor of shape (3, h, w).\n",
    "            - negative: torch tensor of shape (n, 3, h, w).\n",
    "    Returns:\n",
    "        query: torch tensor of shape (batch_size, 3, h, w).\n",
    "        positive: torch tensor of shape (batch_size, 3, h, w).\n",
    "        negatives: torch tensor of shape (batch_size, n, 3, h, w).\n",
    "    \"\"\"\n",
    "\n",
    "    batch = list(filter (lambda x:x is not None, batch))\n",
    "    if len(batch) == 0: return None, None, None, None, None\n",
    "\n",
    "    query, positive, negatives, indices = zip(*batch)\n",
    "\n",
    "    query = data.dataloader.default_collate(query)\n",
    "    positive = data.dataloader.default_collate(positive)\n",
    "    negCounts = data.dataloader.default_collate([x.shape[0] for x in negatives])\n",
    "    negatives = torch.cat(negatives, 0)\n",
    "    import itertools\n",
    "    indices = list(itertools.chain(*indices))\n",
    "\n",
    "    return query, positive, negatives, negCounts, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    epoch_loss = 0\n",
    "    startIter = 1 # keep track of batch iter across subsets for logging\n",
    "\n",
    "    if opt.cacheRefreshRate > 0:\n",
    "        subsetN = ceil(len(train_set) / opt.cacheRefreshRate)\n",
    "        #TODO randomise the arange before splitting?\n",
    "        subsetIdx = np.array_split(np.arange(len(train_set)), subsetN)\n",
    "    else:\n",
    "        subsetN = 1\n",
    "        subsetIdx = [np.arange(len(train_set))]\n",
    "\n",
    "    nBatches = (len(train_set) + opt.batchSize - 1) // opt.batchSize\n",
    "\n",
    "    for subIter in range(subsetN):\n",
    "        print('====> Building Cache')\n",
    "        model.eval()\n",
    "        train_set.cache = join(opt.cachePath, train_set.whichSet + '_feat_cache.hdf5')\n",
    "        with h5py.File(train_set.cache, mode='w') as h5: \n",
    "            pool_size = encoder_dim\n",
    "            if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters\n",
    "            h5feat = h5.create_dataset(\"features\", \n",
    "                    [len(whole_train_set), pool_size], \n",
    "                    dtype=np.float32)\n",
    "            with torch.no_grad():\n",
    "                for iteration, (input, indices) in enumerate(whole_training_data_loader, 1):\n",
    "                    input = input.to(device)\n",
    "                    image_encoding = model.encoder(input)\n",
    "                    vlad_encoding = model.pool(image_encoding) \n",
    "                    h5feat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()\n",
    "                    del input, image_encoding, vlad_encoding\n",
    "\n",
    "        sub_train_set = Subset(dataset=train_set, indices=subsetIdx[subIter])\n",
    "\n",
    "        training_data_loader = DataLoader(dataset=sub_train_set, num_workers=opt.threads, \n",
    "                    batch_size=opt.batchSize, shuffle=True, \n",
    "                    collate_fn=collate_fn, pin_memory=cuda)\n",
    "\n",
    "        print('Allocated:', torch.cuda.memory_allocated())\n",
    "        print('Cached:', torch.cuda.memory_cached())\n",
    "\n",
    "        model.train()\n",
    "        for iteration, (query, positives, negatives, negCounts, indices) in enumerate(training_data_loader, startIter):\n",
    "            # some reshaping to put query, pos, negs in a single (N, 3, H, W) tensor\n",
    "            # where N = batchSize * (nQuery + nPos + nNeg)\n",
    "            if query is None: continue # in case we get an empty batch\n",
    "\n",
    "            B, C, H, W = query.shape\n",
    "            nNeg = torch.sum(negCounts)\n",
    "            input = torch.cat([query, positives, negatives])\n",
    "\n",
    "            input = input.to(device)\n",
    "            image_encoding = model.encoder(input)\n",
    "            vlad_encoding = model.pool(image_encoding) \n",
    "\n",
    "            vladQ, vladP, vladN = torch.split(vlad_encoding, [B, B, nNeg])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # calculate loss for each Query, Positive, Negative triplet\n",
    "            # due to potential difference in number of negatives have to \n",
    "            # do it per query, per negative\n",
    "            loss = 0\n",
    "            for i, negCount in enumerate(negCounts):\n",
    "                for n in range(negCount):\n",
    "                    negIx = (torch.sum(negCounts[:i]) + n).item()\n",
    "                    loss += criterion(vladQ[i:i+1], vladP[i:i+1], vladN[negIx:negIx+1])\n",
    "\n",
    "            loss /= nNeg.float().to(device) # normalise by actual number of negatives\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del input, image_encoding, vlad_encoding, vladQ, vladP, vladN\n",
    "            del query, positives, negatives\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            if iteration % 50 == 0 or nBatches <= 10:\n",
    "                print(\"==> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, \n",
    "                    nBatches, batch_loss), flush=True)\n",
    "                writer.add_scalar('Train/Loss', batch_loss, \n",
    "                        ((epoch-1) * nBatches) + iteration)\n",
    "                writer.add_scalar('Train/nNeg', nNeg, \n",
    "                        ((epoch-1) * nBatches) + iteration)\n",
    "                print('Allocated:', torch.cuda.memory_allocated())\n",
    "                print('Cached:', torch.cuda.memory_cached())\n",
    "\n",
    "        startIter += len(training_data_loader)\n",
    "        del training_data_loader, loss\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        remove(train_set.cache) # delete HDF5 cache\n",
    "\n",
    "    avg_loss = epoch_loss / nBatches\n",
    "\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, avg_loss), \n",
    "            flush=True)\n",
    "    writer.add_scalar('Train/AvgLoss', avg_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(eval_set, epoch=0, write_tboard=False):\n",
    "    # TODO what if features dont fit in memory? \n",
    "    test_data_loader = DataLoader(dataset=eval_set, \n",
    "                num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False, \n",
    "                pin_memory=cuda)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('====> Extracting Features')\n",
    "        pool_size = encoder_dim\n",
    "        if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters\n",
    "        dbFeat = np.empty((len(eval_set), pool_size))\n",
    "\n",
    "        for iteration, (input, indices) in enumerate(test_data_loader, 1):\n",
    "            input = input.to(device)\n",
    "            image_encoding = model.encoder(input)\n",
    "            vlad_encoding = model.pool(image_encoding) \n",
    "\n",
    "            dbFeat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()\n",
    "            if iteration % 50 == 0 or len(test_data_loader) <= 10:\n",
    "                print(\"==> Batch ({}/{})\".format(iteration, \n",
    "                    len(test_data_loader)), flush=True)\n",
    "\n",
    "            del input, image_encoding, vlad_encoding\n",
    "    del test_data_loader\n",
    "\n",
    "    # extracted for both db and query, now split in own sets\n",
    "    qFeat = dbFeat[eval_set.dbStruct.numDb:].astype('float32')\n",
    "    dbFeat = dbFeat[:eval_set.dbStruct.numDb].astype('float32')\n",
    "    \n",
    "    print('====> Building faiss index')\n",
    "    faiss_index = faiss.IndexFlatL2(pool_size)\n",
    "    faiss_index.add(dbFeat)\n",
    "\n",
    "    print('====> Calculating recall @ N')\n",
    "    n_values = [1,5,10,20]\n",
    "\n",
    "    _, predictions = faiss_index.search(qFeat, max(n_values)) \n",
    "\n",
    "    # for each query get those within threshold distance\n",
    "    gt = eval_set.getPositives() \n",
    "\n",
    "    correct_at_n = np.zeros(len(n_values))\n",
    "    #TODO can we do this on the matrix in one go?\n",
    "    for qIx, pred in enumerate(predictions):\n",
    "        for i,n in enumerate(n_values):\n",
    "            # if in top N then also in top NN, where NN > N\n",
    "            if np.any(np.in1d(pred[:n], gt[qIx])):\n",
    "                correct_at_n[i:] += 1\n",
    "                break\n",
    "    recall_at_n = correct_at_n / eval_set.dbStruct.numQ\n",
    "\n",
    "    recalls = {} #make dict for output\n",
    "    for i,n in enumerate(n_values):\n",
    "        recalls[n] = recall_at_n[i]\n",
    "        print(\"====> Recall@{}: {:.4f}\".format(n, recall_at_n[i]))\n",
    "        if write_tboard: writer.add_scalar('Val/Recall@' + str(n), recall_at_n[i], epoch)\n",
    "\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    model_out_path = join(opt.savePath, filename)\n",
    "    torch.save(state, model_out_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(model_out_path, join(opt.savePath, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='vgg16', batchSize=4, cacheBatchSize=24, cachePath='/tmp', cacheRefreshRate=1000, ckpt='latest', dataPath='/home/ubuntu/Desktop/pytorch-NetVlad/data/', dataset='pittsburgh', evalEvery=1, fromscratch=False, lr=0.0001, lrGamma=0.5, lrStep=5, margin=0.1, mode='train', momentum=0.9, nEpochs=30, nGPU=1, nocuda=False, num_clusters=64, optim='SGD', patience=10, pooling='netvlad', resume='', runsPath='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', savePath='checkpoints', seed=123, split='val', start_epoch=0, threads=8, vladv2=False, weightDecay=0.001)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args('')\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = not opt.nocuda\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Training query set: 7320\n",
      "===> Evaluating on val set, query count: 7608\n"
     ]
    }
   ],
   "source": [
    "whole_train_set = get_whole_training_set()\n",
    "whole_training_data_loader = DataLoader(dataset=whole_train_set, \n",
    "        num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False, \n",
    "        pin_memory=cuda)\n",
    "\n",
    "train_set = get_training_query_set(opt.margin)\n",
    "\n",
    "print('====> Training query set:', len(train_set))\n",
    "whole_test_set = get_whole_val_set()\n",
    "print('===> Evaluating on val set, query count:', whole_test_set.dbStruct.numQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = not opt.fromscratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = 512\n",
    "encoder = models.vgg16(pretrained=pretrained)\n",
    "# capture only feature part and remove last relu and maxpool\n",
    "layers = list(encoder.features.children())[:-2]\n",
    "\n",
    "if pretrained:\n",
    "    # if using pretrained then only train conv5_1, conv5_2, and conv5_3\n",
    "    for l in layers[:-5]: \n",
    "        for p in l.parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.mode.lower() == 'cluster' and not opt.vladv2:\n",
    "    layers.append(L2Norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(*layers)\n",
    "model = nn.Module() \n",
    "model.add_module('encoder', encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_vlad = netvlad.NetVLAD(num_clusters=opt.num_clusters, dim=encoder_dim, vladv2=opt.vladv2)\n",
    "if not opt.resume: \n",
    "    if opt.mode.lower() == 'train':\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + train_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "    else:\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + whole_test_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "\n",
    "    if not exists(initcache):\n",
    "        raise FileNotFoundError('Could not find clusters, please run with --mode=cluster before proceeding')\n",
    "\n",
    "    with h5py.File(initcache, mode='r') as h5: \n",
    "        clsts = h5.get(\"centroids\")[...]\n",
    "        traindescs = h5.get(\"descriptors\")[...]\n",
    "        net_vlad.init_params(clsts, traindescs) \n",
    "        del clsts, traindescs\n",
    "\n",
    "model.add_module('pool', net_vlad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "isParallel = False\n",
    "if opt.nGPU > 1 and torch.cuda.device_count() > 1:\n",
    "    model.encoder = nn.DataParallel(model.encoder)\n",
    "    if opt.mode.lower() != 'cluster':\n",
    "        model.pool = nn.DataParallel(model.pool)\n",
    "    isParallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.optim.upper() == 'ADAM':\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, \n",
    "        model.parameters()), lr=opt.lr)#, betas=(0,0.9))\n",
    "elif opt.optim.upper() == 'SGD':\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, \n",
    "        model.parameters()), lr=opt.lr,\n",
    "        momentum=opt.momentum,\n",
    "        weight_decay=opt.weightDecay)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lrStep, gamma=opt.lrGamma)\n",
    "else:\n",
    "    raise ValueError('Unknown optimizer: ' + opt.optim)\n",
    "\n",
    "# original paper/code doesn't sqrt() the distances, we do, so sqrt() the margin, I think :D\n",
    "criterion = nn.TripletMarginLoss(margin=opt.margin**0.5, p=2, reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Training model\n",
      "===> Saving state to: /home/ubuntu/Desktop/pytorch-NetVlad/runs/May17_16-41-17_vgg16_netvlad\n",
      "====> Building Cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 60039168\n",
      "Cached: 13845397504\n",
      "269 10000\n",
      "143 10000\n",
      "287 10000\n",
      "3 10000\n",
      "228 10000\n",
      "571 10000\n",
      "754 10000\n",
      "64 10000\n",
      "50 10000\n",
      "500 10000\n",
      "323 10000\n",
      "907 10000\n",
      "340 10000\n",
      "141 10000\n",
      "5 10000\n",
      "554 10000\n",
      "591 10000\n",
      "695 10000\n",
      "329 10000\n",
      "284 10000\n",
      "777 10000\n",
      "317 10000\n",
      "51 10000\n",
      "377 10000\n",
      "255 10000\n",
      "342 10000\n",
      "446 10000\n",
      "909 10000\n",
      "859 10000\n",
      "653 10000\n",
      "826 10000\n",
      "353 10000\n",
      "389 10000\n",
      "725 10000\n",
      "581 10000\n",
      "728 10000\n",
      "495 10000\n",
      "96 10000\n",
      "903 10000\n",
      "881 10000\n",
      "516 10000\n",
      "636 10000\n",
      "834 10000\n",
      "110 10000\n",
      "637 10000\n",
      "650 10000\n",
      "69 10000\n",
      "97 10000\n",
      "397 10000\n",
      "133 10000\n",
      "503 10000\n",
      "819 10000\n",
      "517 10000\n",
      "730 10000\n",
      "804 10000\n",
      "660 10000\n",
      "794 10000\n",
      "142 10000\n",
      "496 10000\n",
      "770 10000\n",
      "580 10000\n",
      "225 10000\n",
      "184 10000\n",
      "621 10000\n",
      "136 10000\n",
      "799 10000\n",
      "884 10000\n",
      "252 10000\n",
      "474 10000\n",
      "217 10000\n",
      "8 10000\n",
      "816 10000\n",
      "682 10000\n",
      "659 10000\n",
      "31 10000\n",
      "86 10000\n",
      "714 10000\n",
      "640 10000\n",
      "756 10000\n",
      "538 10000\n",
      "391 10000\n",
      "874 10000\n",
      "237 10000\n",
      "605 10000\n",
      "395 10000\n",
      "673 10000\n",
      "366 10000\n",
      "723 10000\n",
      "631 10000\n",
      "104 10000\n",
      "100 10000\n",
      "524 10000\n",
      "189 10000\n",
      "620 10000\n",
      "414 10000\n",
      "573 10000\n",
      "839 10000\n",
      "430 10000\n",
      "630 10000\n",
      "864 10000\n",
      "148 10000\n",
      "16 10000\n",
      "809 10000\n",
      "120 10000\n",
      "476 10000\n",
      "861 10000\n",
      "378 10000\n",
      "814 10000\n",
      "144 10000\n",
      "250 10000\n",
      "39 10000\n",
      "468 10000\n",
      "825 10000\n",
      "835 10000\n",
      "471 10000\n",
      "150 10000\n",
      "170 10000\n",
      "616 10000\n",
      "568 10000\n",
      "850 10000\n",
      "93 10000\n",
      "209 10000\n",
      "622 10000\n",
      "328 10000\n",
      "602 10000\n",
      "680 10000\n",
      "624 10000\n",
      "604 10000\n",
      "245 10000\n",
      "845 10000\n",
      "42 10000\n",
      "685 10000\n",
      "677 10000\n",
      "676 10000\n",
      "867 10000\n",
      "908 10000\n",
      "838 10000\n",
      "447 10000\n",
      "755 10000\n",
      "59 10000\n",
      "752 10000\n",
      "29 10000\n",
      "52 10000\n",
      "187 10000\n",
      "300 10000\n",
      "291 10000\n",
      "873 10000\n",
      "639 10000\n",
      "129 10000\n",
      "594 10000\n",
      "613 10000\n",
      "126 10000\n",
      "420 10000\n",
      "520 10000\n",
      "656 10000\n",
      "12 10000\n",
      "285 10000\n",
      "352 10000\n",
      "868 10000\n",
      "385 10000\n",
      "406 10000\n",
      "790 10000\n",
      "560 10000\n",
      "251 10000\n",
      "306 10000\n",
      "190 10000\n",
      "61 10000\n",
      "276 10000\n",
      "159 10000\n",
      "530 10000\n",
      "326 10000\n",
      "220 10000\n",
      "27 10000\n",
      "319 10000\n",
      "335 10000\n",
      "704 10000\n",
      "156 10000\n",
      "113 10000\n",
      "382 10000\n",
      "687 10000\n",
      "635 10000\n",
      "151 10000\n",
      "501 10000\n",
      "334 10000\n",
      "748 10000\n",
      "648 10000\n",
      "283 10000\n",
      "246 10000\n",
      "173 10000\n",
      "762 10000\n",
      "398 10000\n",
      "740 10000\n",
      "331 10000\n",
      "103 10000\n",
      "618 10000\n",
      "213 10000\n",
      "311 10000\n",
      "270 10000\n",
      "619 10000\n",
      "18 10000\n",
      "175 10000\n",
      "147 10000\n",
      "258 10000\n",
      "87 10000\n",
      "597 10000\n",
      "612 10000\n",
      "404 10000\n",
      "325 10000\n",
      "307 10000\n",
      "895 10000\n",
      "767 10000\n",
      "25 10000\n",
      "416 10000\n",
      "872 10000\n",
      "417 10000\n",
      "537 10000\n",
      "19 10000\n",
      "535 10000\n",
      "701 10000\n",
      "715 10000\n",
      "865 10000\n",
      "841 10000\n",
      "699 10000\n",
      "98 10000\n",
      "154 10000\n",
      "753 10000\n",
      "805 10000\n",
      "215 10000\n",
      "362 10000\n",
      "185 10000\n",
      "9 10000\n",
      "515 10000\n",
      "==> Epoch[1](50/1830): Loss: 0.3160\n",
      "526 10000\n",
      "652 10000\n",
      "Allocated: 117199360\n",
      "Cached: 16601055232\n",
      "244 10000\n",
      "88 10000\n",
      "111 10000\n",
      "271 10000\n",
      "415 10000\n",
      "212 10000\n",
      "529 10000\n",
      "81 10000\n",
      "134 10000\n",
      "577 10000\n",
      "758 10000\n",
      "65 10000\n",
      "558 10000\n",
      "548 10000\n",
      "242 10000\n",
      "77 10000\n",
      "507 10000\n",
      "465 10000\n",
      "890 10000\n",
      "410 10000\n",
      "843 10000\n",
      "375 10000\n",
      "429 10000\n",
      "897 10000\n",
      "906 10000\n",
      "663 10000\n",
      "486 10000\n",
      "203 10000\n",
      "492 10000\n",
      "396 10000\n",
      "425 10000\n",
      "188 10000\n",
      "343 10000\n",
      "779 10000\n",
      "169 10000\n",
      "880 10000\n",
      "564 10000\n",
      "49 10000\n",
      "131 10000\n",
      "807 10000\n",
      "219 10000\n",
      "824 10000\n",
      "811 10000\n",
      "480 10000\n",
      "449 10000\n",
      "693 10000\n",
      "485 10000\n",
      "772 10000\n",
      "196 10000\n",
      "316 10000\n",
      "112 10000\n",
      "629 10000\n",
      "43 10000\n",
      "649 10000\n",
      "183 10000\n",
      "191 10000\n",
      "532 10000\n",
      "789 10000\n",
      "122 10000\n",
      "28 10000\n",
      "384 10000\n",
      "354 10000\n",
      "277 10000\n",
      "388 10000\n",
      "798 10000\n",
      "694 10000\n",
      "58 10000\n",
      "588 10000\n",
      "735 10000\n",
      "211 10000\n",
      "745 10000\n",
      "68 10000\n",
      "294 10000\n",
      "114 10000\n",
      "351 10000\n",
      "233 10000\n",
      "552 10000\n",
      "521 10000\n",
      "539 10000\n",
      "609 10000\n",
      "674 10000\n",
      "467 10000\n",
      "902 10000\n",
      "337 10000\n",
      "793 10000\n",
      "226 10000\n",
      "713 10000\n",
      "611 10000\n",
      "262 10000\n",
      "643 10000\n",
      "487 10000\n",
      "74 10000\n",
      "858 10000\n",
      "426 10000\n",
      "288 10000\n",
      "239 10000\n",
      "763 10000\n",
      "775 10000\n",
      "914 10000\n",
      "297 10000\n",
      "229 10000\n",
      "172 10000\n",
      "791 10000\n",
      "585 10000\n",
      "828 10000\n",
      "705 10000\n",
      "0 10000\n",
      "893 10000\n",
      "261 10000\n",
      "223 10000\n",
      "706 10000\n",
      "617 10000\n",
      "40 10000\n",
      "160 10000\n",
      "731 10000\n",
      "124 10000\n",
      "303 10000\n",
      "875 10000\n",
      "349 10000\n",
      "829 10000\n",
      "726 10000\n",
      "499 10000\n",
      "664 10000\n",
      "553 10000\n",
      "491 10000\n",
      "106 10000\n",
      "26 10000\n",
      "281 10000\n",
      "44 10000\n",
      "140 10000\n",
      "913 10000\n",
      "666 10000\n",
      "690 10000\n",
      "452 10000\n",
      "679 10000\n",
      "70 10000\n",
      "2 10000\n",
      "272 10000\n",
      "80 10000\n",
      "438 10000\n",
      "165 10000\n",
      "119 10000\n",
      "346 10000\n",
      "235 10000\n",
      "708 10000\n",
      "448 10000\n",
      "675 10000\n",
      "370 10000\n",
      "356 10000\n",
      "435 10000\n",
      "116 10000\n",
      "822 10000\n",
      "778 10000\n",
      "831 10000\n",
      "615 10000\n",
      "598 10000\n",
      "662 10000\n",
      "107 10000\n",
      "563 10000\n",
      "626 10000\n",
      "592 10000\n",
      "511 10000\n",
      "889 10000\n",
      "94 10000\n",
      "312 10000\n",
      "118 10000\n",
      "434 10000\n",
      "210 10000\n",
      "764 10000\n",
      "206 10000\n",
      "504 10000\n",
      "533 10000\n",
      "823 10000\n",
      "1 10000\n",
      "181 10000\n",
      "760 10000\n",
      "527 10000\n",
      "407 10000\n",
      "808 10000\n",
      "66 10000\n",
      "672 10000\n",
      "878 10000\n",
      "279 10000\n",
      "358 10000\n",
      "293 10000\n",
      "248 10000\n",
      "218 10000\n",
      "458 10000\n",
      "628 10000\n",
      "333 10000\n",
      "23 10000\n",
      "109 10000\n",
      "601 10000\n",
      "502 10000\n",
      "152 10000\n",
      "610 10000\n",
      "266 10000\n",
      "361 10000\n",
      "145 10000\n",
      "==> Epoch[1](100/1830): Loss: 0.3161\n",
      "Allocated: 117199360\n",
      "Cached: 16601055232\n",
      "899 10000\n",
      "201 10000\n",
      "882 10000\n",
      "157 10000\n",
      "409 10000\n",
      "528 10000\n",
      "702 10000\n",
      "431 10000\n",
      "508 10000\n",
      "463 10000\n",
      "91 10000\n",
      "651 10000\n",
      "512 10000\n",
      "339 10000\n",
      "153 10000\n",
      "108 10000\n",
      "393 10000\n",
      "376 10000\n",
      "439 10000\n",
      "466 10000\n",
      "523 10000\n",
      "565 10000\n",
      "174 10000\n",
      "719 10000\n",
      "720 10000\n",
      "830 10000\n",
      "721 10000\n",
      "525 10000\n",
      "842 10000\n",
      "37 10000\n",
      "302 10000\n",
      "304 10000\n",
      "536 10000\n",
      "785 10000\n",
      "295 10000\n",
      "57 10000\n",
      "750 10000\n",
      "138 10000\n",
      "90 10000\n",
      "401 10000\n",
      "596 10000\n",
      "162 10000\n",
      "164 10000\n",
      "547 10000\n",
      "759 10000\n",
      "578 10000\n",
      "347 10000\n",
      "658 10000\n",
      "886 10000\n",
      "885 10000\n",
      "41 10000\n",
      "194 10000\n",
      "177 10000\n",
      "857 10000\n",
      "551 10000\n",
      "22 10000\n",
      "514 10000\n",
      "734 10000\n",
      "130 10000\n",
      "30 10000\n",
      "440 10000\n",
      "35 10000\n",
      "424 10000\n",
      "84 10000\n",
      "253 10000\n",
      "313 10000\n",
      "444 10000\n",
      "78 10000\n",
      "427 10000\n",
      "540 10000\n",
      "99 10000\n",
      "4 10000\n",
      "771 10000\n",
      "505 10000\n",
      "442 10000\n",
      "787 10000\n",
      "827 10000\n",
      "445 10000\n",
      "46 10000\n",
      "166 10000\n",
      "310 10000\n",
      "322 10000\n",
      "117 10000\n",
      "534 10000\n",
      "593 10000\n",
      "795 10000\n",
      "579 10000\n",
      "531 10000\n",
      "800 10000\n",
      "264 10000\n",
      "327 10000\n",
      "149 10000\n",
      "275 10000\n",
      "198 10000\n",
      "812 10000\n",
      "792 10000\n",
      "820 10000\n",
      "569 10000\n",
      "661 10000\n",
      "871 10000\n",
      "257 10000\n",
      "844 10000\n",
      "566 10000\n",
      "359 10000\n",
      "513 10000\n",
      "757 10000\n",
      "67 10000\n",
      "766 10000\n",
      "421 10000\n",
      "115 10000\n",
      "751 10000\n",
      "299 10000\n",
      "392 10000\n",
      "856 10000\n",
      "125 10000\n",
      "654 10000\n",
      "614 10000\n",
      "365 10000\n",
      "769 10000\n",
      "411 10000\n",
      "905 10000\n",
      "600 10000\n",
      "509 10000\n",
      "54 10000\n",
      "782 10000\n",
      "105 10000\n",
      "813 10000\n",
      "803 10000\n",
      "10 10000\n",
      "336 10000\n",
      "488 10000\n",
      "854 10000\n",
      "412 10000\n",
      "475 10000\n",
      "724 10000\n",
      "374 10000\n",
      "688 10000\n",
      "441 10000\n",
      "910 10000\n",
      "683 10000\n",
      "892 10000\n",
      "428 10000\n",
      "214 10000\n",
      "781 10000\n",
      "542 10000\n",
      "256 10000\n",
      "479 10000\n",
      "543 10000\n",
      "589 10000\n",
      "599 10000\n",
      "894 10000\n",
      "729 10000\n",
      "186 10000\n",
      "498 10000\n",
      "765 10000\n",
      "692 10000\n",
      "608 10000\n",
      "204 10000\n",
      "360 10000\n",
      "461 10000\n",
      "716 10000\n",
      "72 10000\n",
      "278 10000\n",
      "82 10000\n",
      "733 10000\n",
      "883 10000\n",
      "63 10000\n",
      "6 10000\n",
      "73 10000\n",
      "296 10000\n",
      "691 10000\n",
      "638 10000\n",
      "562 10000\n",
      "739 10000\n",
      "92 10000\n",
      "437 10000\n",
      "887 10000\n",
      "863 10000\n",
      "17 10000\n",
      "314 10000\n",
      "642 10000\n",
      "669 10000\n",
      "216 10000\n",
      "289 10000\n",
      "667 10000\n",
      "711 10000\n",
      "21 10000\n",
      "243 10000\n",
      "418 10000\n",
      "102 10000\n",
      "773 10000\n",
      "625 10000\n",
      "522 10000\n",
      "736 10000\n",
      "423 10000\n",
      "627 10000\n",
      "549 10000\n",
      "911 10000\n",
      "477 10000\n",
      "==> Epoch[1](150/1830): Loss: 0.3161\n",
      "Allocated: 117199360\n",
      "Cached: 16601055232\n",
      "776 10000\n",
      "405 10000\n",
      "484 10000\n",
      "371 10000\n",
      "557 10000\n",
      "230 10000\n",
      "837 10000\n",
      "559 10000\n",
      "419 10000\n",
      "780 10000\n",
      "238 10000\n",
      "45 10000\n",
      "717 10000\n",
      "305 10000\n",
      "123 10000\n",
      "456 10000\n",
      "862 10000\n",
      "891 10000\n",
      "127 10000\n",
      "451 10000\n",
      "56 10000\n",
      "146 10000\n",
      "607 10000\n",
      "386 10000\n",
      "718 10000\n",
      "224 10000\n",
      "670 10000\n",
      "743 10000\n",
      "896 10000\n",
      "33 10000\n",
      "457 10000\n",
      "904 10000\n",
      "38 10000\n",
      "139 10000\n",
      "171 10000\n",
      "128 10000\n",
      "263 10000\n",
      "606 10000\n",
      "301 10000\n",
      "320 10000\n",
      "678 10000\n",
      "121 10000\n",
      "710 10000\n",
      "379 10000\n",
      "180 10000\n",
      "818 10000\n",
      "464 10000\n",
      "53 10000\n",
      "741 10000\n",
      "236 10000\n",
      "355 10000\n",
      "345 10000\n",
      "182 10000\n",
      "36 10000\n",
      "89 10000\n",
      "684 10000\n",
      "860 10000\n",
      "460 10000\n",
      "851 10000\n",
      "546 10000\n",
      "60 10000\n",
      "806 10000\n",
      "231 10000\n",
      "315 10000\n",
      "341 10000\n",
      "574 10000\n",
      "454 10000\n",
      "470 10000\n",
      "494 10000\n",
      "802 10000\n",
      "473 10000\n",
      "786 10000\n",
      "575 10000\n",
      "202 10000\n",
      "318 10000\n",
      "399 10000\n",
      "85 10000\n",
      "402 10000\n",
      "309 10000\n",
      "506 10000\n",
      "645 10000\n",
      "403 10000\n",
      "373 10000\n",
      "550 10000\n",
      "158 10000\n",
      "737 10000\n",
      "668 10000\n",
      "833 10000\n",
      "76 10000\n",
      "912 10000\n",
      "877 10000\n",
      "590 10000\n",
      "267 10000\n",
      "168 10000\n",
      "176 10000\n",
      "280 10000\n",
      "137 10000\n",
      "869 10000\n",
      "545 10000\n",
      "541 10000\n",
      "481 10000\n",
      "761 10000\n",
      "436 10000\n",
      "497 10000\n",
      "644 10000\n",
      "11 10000\n",
      "697 10000\n",
      "321 10000\n",
      "24 10000\n",
      "408 10000\n",
      "7 10000\n",
      "259 10000\n",
      "817 10000\n",
      "707 10000\n",
      "205 10000\n",
      "163 10000\n",
      "603 10000\n",
      "696 10000\n",
      "493 10000\n",
      "836 10000\n",
      "783 10000\n",
      "478 10000\n",
      "746 10000\n",
      "192 10000\n",
      "846 10000\n",
      "810 10000\n",
      "646 10000\n",
      "849 10000\n",
      "207 10000\n",
      "132 10000\n",
      "400 10000\n",
      "247 10000\n",
      "208 10000\n",
      "227 10000\n",
      "469 10000\n",
      "200 10000\n",
      "587 10000\n",
      "722 10000\n",
      "20 10000\n",
      "13 10000\n",
      "241 10000\n",
      "840 10000\n",
      "686 10000\n",
      "866 10000\n",
      "774 10000\n",
      "510 10000\n",
      "199 10000\n",
      "193 10000\n",
      "490 10000\n",
      "292 10000\n",
      "368 10000\n",
      "348 10000\n",
      "797 10000\n",
      "876 10000\n",
      "700 10000\n",
      "197 10000\n",
      "556 10000\n",
      "155 10000\n",
      "387 10000\n",
      "544 10000\n",
      "657 10000\n",
      "179 10000\n",
      "698 10000\n",
      "15 10000\n",
      "413 10000\n",
      "357 10000\n",
      "572 10000\n",
      "784 10000\n",
      "273 10000\n",
      "821 10000\n",
      "847 10000\n",
      "260 10000\n",
      "195 10000\n",
      "703 10000\n",
      "647 10000\n",
      "369 10000\n",
      "83 10000\n",
      "472 10000\n",
      "567 10000\n",
      "727 10000\n",
      "586 10000\n",
      "432 10000\n",
      "14 10000\n",
      "161 10000\n",
      "372 10000\n",
      "47 10000\n",
      "422 10000\n",
      "555 10000\n",
      "222 10000\n",
      "742 10000\n",
      "75 10000\n",
      "268 10000\n",
      "95 10000\n",
      "738 10000\n",
      "381 10000\n",
      "234 10000\n",
      "747 10000\n",
      "455 10000\n",
      "744 10000\n",
      "==> Epoch[1](200/1830): Loss: 0.3162\n",
      "584 10000\n",
      "Allocated: 117199360\n",
      "Cached: 16601055232\n",
      "32 10000\n",
      "290 10000\n",
      "363 10000\n",
      "344 10000\n",
      "308 10000\n",
      "633 10000\n",
      "265 10000\n",
      "380 10000\n",
      "390 10000\n",
      "655 10000\n",
      "898 10000\n",
      "274 10000\n",
      "576 10000\n",
      "240 10000\n",
      "62 10000\n",
      "167 10000\n",
      "853 10000\n",
      "681 10000\n",
      "671 10000\n",
      "71 10000\n",
      "815 10000\n",
      "338 10000\n",
      "330 10000\n",
      "34 10000\n",
      "79 10000\n",
      "583 10000\n",
      "768 10000\n",
      "888 10000\n",
      "879 10000\n",
      "518 10000\n",
      "298 10000\n",
      "55 10000\n",
      "848 10000\n",
      "364 10000\n",
      "450 10000\n",
      "519 10000\n",
      "367 10000\n",
      "178 10000\n",
      "462 10000\n",
      "788 10000\n",
      "855 10000\n",
      "796 10000\n",
      "443 10000\n",
      "232 10000\n",
      "665 10000\n",
      "870 10000\n",
      "634 10000\n",
      "383 10000\n",
      "453 10000\n",
      "482 10000\n",
      "641 10000\n",
      "433 10000\n",
      "324 10000\n",
      "286 10000\n",
      "570 10000\n",
      "459 10000\n",
      "582 10000\n",
      "623 10000\n",
      "595 10000\n",
      "709 10000\n",
      "483 10000\n",
      "852 10000\n",
      "282 10000\n",
      "901 10000\n",
      "332 10000\n",
      "101 10000\n",
      "394 10000\n",
      "350 10000\n",
      "749 10000\n",
      "801 10000\n",
      "689 10000\n",
      "632 10000\n",
      "48 10000\n",
      "900 10000\n",
      "732 10000\n",
      "135 10000\n",
      "489 10000\n",
      "254 10000\n",
      "249 10000\n",
      "712 10000\n",
      "221 10000\n",
      "561 10000\n",
      "832 10000\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 17532190720\n",
      "1793 10000\n",
      "1592 10000\n",
      "1132 10000\n",
      "1796 10000\n",
      "1202 10000\n",
      "1721 10000\n",
      "1660 10000\n",
      "1356 10000\n",
      "1379 10000\n",
      "1437 10000\n",
      "934 10000\n",
      "976 10000\n",
      "1008 10000\n",
      "1513 10000\n",
      "1169 10000\n",
      "1389 10000\n",
      "1771 10000\n",
      "1633 10000\n",
      "1285 10000\n",
      "1467 10000\n",
      "987 10000\n",
      "1812 10000\n",
      "1250 10000\n",
      "1424 10000\n",
      "1326 10000\n",
      "1724 10000\n",
      "1823 10000\n",
      "1234 10000\n",
      "1817 10000\n",
      "1683 10000\n",
      "1668 10000\n",
      "1228 10000\n",
      "920 10000\n",
      "919 10000\n",
      "1363 10000\n",
      "1732 10000\n",
      "1133 10000\n",
      "1360 10000\n",
      "963 10000\n",
      "1297 10000\n",
      "1381 10000\n",
      "1329 10000\n",
      "924 10000\n",
      "1256 10000\n",
      "1734 10000\n",
      "1111 10000\n",
      "1113 10000\n",
      "1359 10000\n",
      "1727 10000\n",
      "1579 10000\n",
      "1532 10000\n",
      "1770 10000\n",
      "1781 10000\n",
      "1682 10000\n",
      "1554 10000\n",
      "1469 10000\n",
      "988 10000\n",
      "1764 10000\n",
      "1728 10000\n",
      "1324 10000\n",
      "1052 10000\n",
      "1313 10000\n",
      "989 10000\n",
      "1059 10000\n",
      "1128 10000\n",
      "1002 10000\n",
      "1149 10000\n",
      "1688 10000\n",
      "1587 10000\n",
      "1220 10000\n",
      "1095 10000\n",
      "1051 10000\n",
      "1367 10000\n",
      "1400 10000\n",
      "1481 10000\n",
      "935 10000\n",
      "1499 10000\n",
      "1271 10000\n",
      "1431 10000\n",
      "1517 10000\n",
      "1598 10000\n",
      "1508 10000\n",
      "1807 10000\n",
      "1766 10000\n",
      "1524 10000\n",
      "999 10000\n",
      "1600 10000\n",
      "1648 10000\n",
      "1421 10000\n",
      "1492 10000\n",
      "1568 10000\n",
      "1069 10000\n",
      "1262 10000\n",
      "961 10000\n",
      "1644 10000\n",
      "1328 10000\n",
      "1269 10000\n",
      "1268 10000\n",
      "1674 10000\n",
      "1362 10000\n",
      "1669 10000\n",
      "1057 10000\n",
      "1033 10000\n",
      "1440 10000\n",
      "1810 10000\n",
      "1061 10000\n",
      "1173 10000\n",
      "1047 10000\n",
      "1468 10000\n",
      "1396 10000\n",
      "1347 10000\n",
      "1491 10000\n",
      "1385 10000\n",
      "1425 10000\n",
      "1000 10000\n",
      "1658 10000\n",
      "1790 10000\n",
      "1023 10000\n",
      "1292 10000\n",
      "==> Epoch[1](250/1830): Loss: 0.3162\n",
      "Allocated: 117199360\n",
      "Cached: 16127098880\n",
      "1258 10000\n",
      "1100 10000\n",
      "1017 10000\n",
      "1209 10000\n",
      "1371 10000\n",
      "927 10000\n",
      "1673 10000\n",
      "1696 10000\n",
      "1445 10000\n",
      "1015 10000\n",
      "1759 10000\n",
      "1466 10000\n",
      "1659 10000\n",
      "1244 10000\n",
      "968 10000\n",
      "942 10000\n",
      "1233 10000\n",
      "1616 10000\n",
      "1411 10000\n",
      "1265 10000\n",
      "1068 10000\n",
      "1743 10000\n",
      "1601 10000\n",
      "1544 10000\n",
      "1005 10000\n",
      "1500 10000\n",
      "1619 10000\n",
      "1701 10000\n",
      "1676 10000\n",
      "1717 10000\n",
      "1085 10000\n",
      "945 10000\n",
      "1596 10000\n",
      "1065 10000\n",
      "1203 10000\n",
      "1373 10000\n",
      "1553 10000\n",
      "1084 10000\n",
      "1054 10000\n",
      "1156 10000\n",
      "1037 10000\n",
      "1403 10000\n",
      "1064 10000\n",
      "1312 10000\n",
      "1614 10000\n",
      "1194 10000\n",
      "1087 10000\n",
      "1174 10000\n",
      "1122 10000\n",
      "1422 10000\n",
      "1542 10000\n",
      "1415 10000\n",
      "1749 10000\n",
      "1442 10000\n",
      "1254 10000\n",
      "1496 10000\n",
      "1695 10000\n",
      "1767 10000\n",
      "1370 10000\n",
      "1595 10000\n",
      "1551 10000\n",
      "972 10000\n",
      "1298 10000\n",
      "1765 10000\n",
      "1689 10000\n",
      "1758 10000\n",
      "1638 10000\n",
      "1249 10000\n",
      "1155 10000\n",
      "926 10000\n",
      "1448 10000\n",
      "973 10000\n",
      "1779 10000\n",
      "1402 10000\n",
      "1081 10000\n",
      "1215 10000\n",
      "1339 10000\n",
      "1439 10000\n",
      "1671 10000\n",
      "1345 10000\n",
      "1318 10000\n",
      "1196 10000\n",
      "1393 10000\n",
      "1712 10000\n",
      "1091 10000\n",
      "1353 10000\n",
      "1116 10000\n",
      "1535 10000\n",
      "1530 10000\n",
      "1311 10000\n",
      "947 10000\n",
      "1615 10000\n",
      "1165 10000\n",
      "1716 10000\n",
      "977 10000\n",
      "1211 10000\n",
      "1341 10000\n",
      "1305 10000\n",
      "1296 10000\n",
      "1007 10000\n",
      "1564 10000\n",
      "1261 10000\n",
      "1131 10000\n",
      "1158 10000\n",
      "1189 10000\n",
      "953 10000\n",
      "1566 10000\n",
      "1735 10000\n",
      "1503 10000\n",
      "992 10000\n",
      "1522 10000\n",
      "1698 10000\n",
      "1237 10000\n",
      "1287 10000\n",
      "1130 10000\n",
      "1745 10000\n",
      "1575 10000\n",
      "969 10000\n",
      "1561 10000\n",
      "1040 10000\n",
      "962 10000\n",
      "1772 10000\n",
      "1192 10000\n",
      "1121 10000\n",
      "1438 10000\n",
      "1651 10000\n",
      "1733 10000\n",
      "1219 10000\n",
      "1570 10000\n",
      "1474 10000\n",
      "1580 10000\n",
      "1710 10000\n",
      "1126 10000\n",
      "1813 10000\n",
      "1700 10000\n",
      "1455 10000\n",
      "1545 10000\n",
      "1475 10000\n",
      "1009 10000\n",
      "1327 10000\n",
      "1025 10000\n",
      "950 10000\n",
      "1141 10000\n",
      "1773 10000\n",
      "1207 10000\n",
      "1804 10000\n",
      "1454 10000\n",
      "1166 10000\n",
      "1751 10000\n",
      "1594 10000\n",
      "1814 10000\n",
      "1361 10000\n",
      "1083 10000\n",
      "1795 10000\n",
      "1515 10000\n",
      "1334 10000\n",
      "1573 10000\n",
      "1098 10000\n",
      "1284 10000\n",
      "1206 10000\n",
      "1299 10000\n",
      "1686 10000\n",
      "956 10000\n",
      "1376 10000\n",
      "1289 10000\n",
      "1243 10000\n",
      "1769 10000\n",
      "1150 10000\n",
      "1152 10000\n",
      "1239 10000\n",
      "1447 10000\n",
      "1665 10000\n",
      "1026 10000\n",
      "1653 10000\n",
      "1407 10000\n",
      "1092 10000\n",
      "1387 10000\n",
      "1323 10000\n",
      "978 10000\n",
      "1815 10000\n",
      "1739 10000\n",
      "1622 10000\n",
      "1449 10000\n",
      "983 10000\n",
      "1806 10000\n",
      "1694 10000\n",
      "1099 10000\n",
      "1351 10000\n",
      "991 10000\n",
      "1811 10000\n",
      "1006 10000\n",
      "1030 10000\n",
      "1170 10000\n",
      "943 10000\n",
      "1191 10000\n",
      "1136 10000\n",
      "1074 10000\n",
      "1238 10000\n",
      "1450 10000\n",
      "1461 10000\n",
      "==> Epoch[1](300/1830): Loss: 0.3160\n",
      "Allocated: 117199360\n",
      "Cached: 16127098880\n",
      "1736 10000\n",
      "1719 10000\n",
      "1690 10000\n",
      "1406 10000\n",
      "955 10000\n",
      "1634 10000\n",
      "1349 10000\n",
      "1520 10000\n",
      "1290 10000\n",
      "1581 10000\n",
      "1391 10000\n",
      "1151 10000\n",
      "1822 10000\n",
      "1210 10000\n",
      "1282 10000\n",
      "1159 10000\n",
      "1045 10000\n",
      "1654 10000\n",
      "1102 10000\n",
      "1380 10000\n",
      "1526 10000\n",
      "1704 10000\n",
      "974 10000\n",
      "1308 10000\n",
      "1322 10000\n",
      "1677 10000\n",
      "1485 10000\n",
      "1011 10000\n",
      "995 10000\n",
      "1123 10000\n",
      "1799 10000\n",
      "1459 10000\n",
      "1279 10000\n",
      "1632 10000\n",
      "981 10000\n",
      "1048 10000\n",
      "957 10000\n",
      "1768 10000\n",
      "1611 10000\n",
      "1392 10000\n",
      "1129 10000\n",
      "1293 10000\n",
      "952 10000\n",
      "1828 10000\n",
      "1763 10000\n",
      "1603 10000\n",
      "1134 10000\n",
      "954 10000\n",
      "1019 10000\n",
      "1108 10000\n",
      "1618 10000\n",
      "990 10000\n",
      "1267 10000\n",
      "1038 10000\n",
      "1058 10000\n",
      "1018 10000\n",
      "1452 10000\n",
      "1399 10000\n",
      "1147 10000\n",
      "1185 10000\n",
      "1604 10000\n",
      "1826 10000\n",
      "993 10000\n",
      "1664 10000\n",
      "1182 10000\n",
      "1072 10000\n",
      "1368 10000\n",
      "1762 10000\n",
      "1394 10000\n",
      "1537 10000\n",
      "1332 10000\n",
      "1388 10000\n",
      "997 10000\n",
      "1528 10000\n",
      "1016 10000\n",
      "1384 10000\n",
      "1756 10000\n",
      "944 10000\n",
      "1436 10000\n",
      "1073 10000\n",
      "1744 10000\n",
      "1217 10000\n",
      "1563 10000\n",
      "1599 10000\n",
      "1181 10000\n",
      "1820 10000\n",
      "1086 10000\n",
      "1275 10000\n",
      "1691 10000\n",
      "1090 10000\n",
      "1778 10000\n",
      "1613 10000\n",
      "1205 10000\n",
      "1678 10000\n",
      "1042 10000\n",
      "1315 10000\n",
      "982 10000\n",
      "1259 10000\n",
      "1639 10000\n",
      "1755 10000\n",
      "932 10000\n",
      "1479 10000\n",
      "1774 10000\n",
      "949 10000\n",
      "1432 10000\n",
      "1096 10000\n",
      "929 10000\n",
      "1555 10000\n",
      "1490 10000\n",
      "1444 10000\n",
      "1246 10000\n",
      "1063 10000\n",
      "1034 10000\n",
      "1574 10000\n",
      "1036 10000\n",
      "1538 10000\n",
      "915 10000\n",
      "1637 10000\n",
      "1338 10000\n",
      "1819 10000\n",
      "1657 10000\n",
      "1049 10000\n",
      "985 10000\n",
      "1805 10000\n",
      "1548 10000\n",
      "1014 10000\n",
      "1472 10000\n",
      "1093 10000\n",
      "1738 10000\n",
      "1718 10000\n",
      "1120 10000\n",
      "1245 10000\n",
      "1509 10000\n",
      "1585 10000\n",
      "1193 10000\n",
      "1283 10000\n",
      "1414 10000\n",
      "1761 10000\n",
      "967 10000\n",
      "1489 10000\n",
      "1187 10000\n",
      "1142 10000\n",
      "1161 10000\n",
      "1412 10000\n",
      "1270 10000\n",
      "1410 10000\n",
      "1730 10000\n",
      "1306 10000\n",
      "1746 10000\n",
      "1584 10000\n",
      "1792 10000\n",
      "1184 10000\n",
      "1667 10000\n",
      "1221 10000\n",
      "958 10000\n",
      "1114 10000\n",
      "1320 10000\n",
      "1457 10000\n",
      "1343 10000\n",
      "1443 10000\n",
      "1164 10000\n",
      "1628 10000\n",
      "1413 10000\n",
      "1075 10000\n",
      "1178 10000\n",
      "1286 10000\n",
      "1163 10000\n",
      "966 10000\n",
      "1617 10000\n",
      "1409 10000\n",
      "1251 10000\n",
      "1307 10000\n",
      "1789 10000\n",
      "1512 10000\n",
      "1725 10000\n",
      "1200 10000\n",
      "1383 10000\n",
      "1103 10000\n",
      "1747 10000\n",
      "1423 10000\n",
      "1675 10000\n",
      "1107 10000\n",
      "1146 10000\n",
      "1565 10000\n",
      "1337 10000\n",
      "1557 10000\n",
      "1567 10000\n",
      "1434 10000\n",
      "1372 10000\n",
      "1277 10000\n",
      "931 10000\n",
      "1502 10000\n",
      "1697 10000\n",
      "1786 10000\n",
      "1420 10000\n",
      "1216 10000\n",
      "917 10000\n",
      "1471 10000\n",
      "1680 10000\n",
      "1816 10000\n",
      "1395 10000\n",
      "==> Epoch[1](350/1830): Loss: 0.3161\n",
      "Allocated: 117199360\n",
      "Cached: 16127098880\n",
      "1609 10000\n",
      "1652 10000\n",
      "1456 10000\n",
      "1242 10000\n",
      "1430 10000\n",
      "1401 10000\n",
      "1523 10000\n",
      "1031 10000\n",
      "1572 10000\n",
      "1681 10000\n",
      "1534 10000\n",
      "1336 10000\n",
      "1801 10000\n",
      "984 10000\n",
      "1162 10000\n",
      "1213 10000\n",
      "1127 10000\n",
      "1316 10000\n",
      "925 10000\n",
      "998 10000\n",
      "1582 10000\n",
      "1476 10000\n",
      "1645 10000\n",
      "1070 10000\n",
      "1501 10000\n",
      "1229 10000\n",
      "1138 10000\n",
      "1397 10000\n",
      "1685 10000\n",
      "1344 10000\n",
      "1722 10000\n",
      "1248 10000\n",
      "1550 10000\n",
      "1498 10000\n",
      "1240 10000\n",
      "1714 10000\n",
      "1626 10000\n",
      "1053 10000\n",
      "1514 10000\n",
      "1317 10000\n",
      "1706 10000\n",
      "1488 10000\n",
      "1546 10000\n",
      "1446 10000\n",
      "1043 10000\n",
      "1024 10000\n",
      "1346 10000\n",
      "1656 10000\n",
      "1232 10000\n",
      "1539 10000\n",
      "1794 10000\n",
      "1377 10000\n",
      "1050 10000\n",
      "1382 10000\n",
      "1713 10000\n",
      "1398 10000\n",
      "1607 10000\n",
      "1197 10000\n",
      "1274 10000\n",
      "1357 10000\n",
      "1235 10000\n",
      "1094 10000\n",
      "1144 10000\n",
      "1230 10000\n",
      "1044 10000\n",
      "1115 10000\n",
      "1464 10000\n",
      "1076 10000\n",
      "936 10000\n",
      "1602 10000\n",
      "1148 10000\n",
      "1225 10000\n",
      "1650 10000\n",
      "1536 10000\n",
      "1740 10000\n",
      "1495 10000\n",
      "941 10000\n",
      "916 10000\n",
      "1039 10000\n",
      "1800 10000\n",
      "1378 10000\n",
      "1663 10000\n",
      "1404 10000\n",
      "1731 10000\n",
      "1478 10000\n",
      "1253 10000\n",
      "994 10000\n",
      "1559 10000\n",
      "1540 10000\n",
      "1708 10000\n",
      "1610 10000\n",
      "1662 10000\n",
      "1829 10000\n",
      "1518 10000\n",
      "1266 10000\n",
      "1001 10000\n",
      "923 10000\n",
      "1078 10000\n",
      "1631 10000\n",
      "1465 10000\n",
      "1646 10000\n",
      "1630 10000\n",
      "1672 10000\n",
      "1593 10000\n",
      "1506 10000\n",
      "1487 10000\n",
      "1214 10000\n",
      "1670 10000\n",
      "1711 10000\n",
      "1177 10000\n",
      "1451 10000\n",
      "1441 10000\n",
      "1303 10000\n",
      "1335 10000\n",
      "1157 10000\n",
      "1760 10000\n",
      "1366 10000\n",
      "1310 10000\n",
      "1227 10000\n",
      "1304 10000\n",
      "1309 10000\n",
      "1117 10000\n",
      "1623 10000\n",
      "1791 10000\n",
      "1278 10000\n",
      "1620 10000\n",
      "1473 10000\n",
      "1333 10000\n",
      "1301 10000\n",
      "1516 10000\n",
      "1477 10000\n",
      "1088 10000\n",
      "1418 10000\n",
      "1494 10000\n",
      "1709 10000\n",
      "1552 10000\n",
      "1056 10000\n",
      "1527 10000\n",
      "1046 10000\n",
      "1788 10000\n",
      "1776 10000\n",
      "1687 10000\n",
      "1294 10000\n",
      "1358 10000\n",
      "1470 10000\n",
      "1012 10000\n",
      "1702 10000\n",
      "1483 10000\n",
      "1110 10000\n",
      "1260 10000\n",
      "1723 10000\n",
      "1486 10000\n",
      "1750 10000\n",
      "1560 10000\n",
      "1588 10000\n",
      "1519 10000\n",
      "1571 10000\n",
      "1417 10000\n",
      "948 10000\n",
      "1319 10000\n",
      "1699 10000\n",
      "1291 10000\n",
      "1583 10000\n",
      "1369 10000\n",
      "1642 10000\n",
      "928 10000\n",
      "1754 10000\n",
      "1435 10000\n",
      "1365 10000\n",
      "1109 10000\n",
      "1777 10000\n",
      "1655 10000\n",
      "1541 10000\n",
      "938 10000\n",
      "1077 10000\n",
      "1022 10000\n",
      "939 10000\n",
      "1160 10000\n",
      "1199 10000\n",
      "1825 10000\n",
      "1803 10000\n",
      "1350 10000\n",
      "1010 10000\n",
      "1137 10000\n",
      "1569 10000\n",
      "1180 10000\n",
      "980 10000\n",
      "1505 10000\n",
      "1263 10000\n",
      "1222 10000\n",
      "1342 10000\n",
      "975 10000\n",
      "1066 10000\n",
      "1741 10000\n",
      "1641 10000\n",
      "1082 10000\n",
      "1606 10000\n",
      "1280 10000\n",
      "1352 10000\n",
      "==> Epoch[1](400/1830): Loss: 0.3162\n",
      "1521 10000\n",
      "Allocated: 117199360\n",
      "Cached: 16127098880\n",
      "1101 10000\n",
      "1591 10000\n",
      "1692 10000\n",
      "1504 10000\n",
      "951 10000\n",
      "1428 10000\n",
      "1625 10000\n",
      "1533 10000\n",
      "1720 10000\n",
      "1119 10000\n",
      "1547 10000\n",
      "1247 10000\n",
      "1374 10000\n",
      "1529 10000\n",
      "1183 10000\n",
      "1752 10000\n",
      "1405 10000\n",
      "1787 10000\n",
      "1062 10000\n",
      "1497 10000\n",
      "1302 10000\n",
      "1649 10000\n",
      "1188 10000\n",
      "1224 10000\n",
      "1003 10000\n",
      "1104 10000\n",
      "1510 10000\n",
      "1818 10000\n",
      "1331 10000\n",
      "1419 10000\n",
      "1429 10000\n",
      "1252 10000\n",
      "1693 10000\n",
      "1802 10000\n",
      "1426 10000\n",
      "1597 10000\n",
      "1543 10000\n",
      "1027 10000\n",
      "1798 10000\n",
      "1145 10000\n",
      "1273 10000\n",
      "1729 10000\n",
      "1375 10000\n",
      "986 10000\n",
      "1325 10000\n",
      "1208 10000\n",
      "1179 10000\n",
      "1236 10000\n",
      "965 10000\n",
      "1013 10000\n",
      "1684 10000\n",
      "971 10000\n",
      "1276 10000\n",
      "940 10000\n",
      "1354 10000\n",
      "1355 10000\n",
      "1186 10000\n",
      "1590 10000\n",
      "1785 10000\n",
      "1035 10000\n",
      "1135 10000\n",
      "1458 10000\n",
      "1106 10000\n",
      "1703 10000\n",
      "1433 10000\n",
      "1726 10000\n",
      "1195 10000\n",
      "1153 10000\n",
      "1775 10000\n",
      "1605 10000\n",
      "1143 10000\n",
      "1021 10000\n",
      "1427 10000\n",
      "1408 10000\n",
      "1640 10000\n",
      "1032 10000\n",
      "1089 10000\n",
      "1020 10000\n",
      "1549 10000\n",
      "1493 10000\n",
      "1525 10000\n",
      "1071 10000\n",
      "1105 10000\n",
      "1780 10000\n",
      "1784 10000\n",
      "1321 10000\n",
      "1589 10000\n",
      "933 10000\n",
      "1154 10000\n",
      "1314 10000\n",
      "1462 10000\n",
      "1824 10000\n",
      "1627 10000\n",
      "1067 10000\n",
      "1757 10000\n",
      "1241 10000\n",
      "937 10000\n",
      "1172 10000\n",
      "1647 10000\n",
      "1635 10000\n",
      "1167 10000\n",
      "964 10000\n",
      "996 10000\n",
      "1608 10000\n",
      "1140 10000\n",
      "1190 10000\n",
      "1231 10000\n",
      "1295 10000\n",
      "1272 10000\n",
      "1797 10000\n",
      "1264 10000\n",
      "1288 10000\n",
      "1281 10000\n",
      "1577 10000\n",
      "1624 10000\n",
      "1531 10000\n",
      "1742 10000\n",
      "1055 10000\n",
      "970 10000\n",
      "1340 10000\n",
      "959 10000\n",
      "1586 10000\n",
      "1125 10000\n",
      "1139 10000\n",
      "1661 10000\n",
      "1562 10000\n",
      "1460 10000\n",
      "1097 10000\n",
      "1679 10000\n",
      "1204 10000\n",
      "1257 10000\n",
      "1556 10000\n",
      "1705 10000\n",
      "1629 10000\n",
      "1198 10000\n",
      "1809 10000\n",
      "1168 10000\n",
      "1753 10000\n",
      "1827 10000\n",
      "1124 10000\n",
      "1821 10000\n",
      "1112 10000\n",
      "1212 10000\n",
      "1255 10000\n",
      "1578 10000\n",
      "1201 10000\n",
      "1737 10000\n",
      "946 10000\n",
      "1480 10000\n",
      "1226 10000\n",
      "1612 10000\n",
      "1783 10000\n",
      "1171 10000\n",
      "1386 10000\n",
      "1028 10000\n",
      "1748 10000\n",
      "930 10000\n",
      "1484 10000\n",
      "921 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b3082ca1672c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SGD'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalEvery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrecalls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_tboard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-9dcb4cf8fdb0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('===> Training model')\n",
    "writer = SummaryWriter(log_dir=join(opt.runsPath, datetime.now().strftime('%b%d_%H-%M-%S')+'_'+opt.arch+'_'+opt.pooling))\n",
    "\n",
    "# write checkpoints in logdir\n",
    "logdir = writer.file_writer.get_logdir()\n",
    "opt.savePath = join(logdir, opt.savePath)\n",
    "if not opt.resume:\n",
    "    makedirs(opt.savePath)\n",
    "\n",
    "with open(join(opt.savePath, 'flags.json'), 'w') as f:\n",
    "    f.write(json.dumps(\n",
    "        {k:v for k,v in vars(opt).items()}\n",
    "        ))\n",
    "print('===> Saving state to:', logdir)\n",
    "\n",
    "not_improved = 0\n",
    "best_score = 0\n",
    "for epoch in range(opt.start_epoch+1, opt.nEpochs + 1):\n",
    "    if opt.optim.upper() == 'SGD':\n",
    "        scheduler.step(epoch)\n",
    "    train(epoch)\n",
    "    if (epoch % opt.evalEvery) == 0:\n",
    "        recalls = test(whole_test_set, epoch, write_tboard=True)\n",
    "        is_best = recalls[5] > best_score \n",
    "        if is_best:\n",
    "            not_improved = 0\n",
    "            best_score = recalls[5]\n",
    "        else: \n",
    "            not_improved += 1\n",
    "\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'recalls': recalls,\n",
    "                'best_score': best_score,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'parallel' : isParallel,\n",
    "        }, is_best)\n",
    "\n",
    "        if opt.patience > 0 and not_improved > (opt.patience / opt.evalEvery):\n",
    "            print('Performance did not improve for', opt.patience, 'epochs. Stopping.')\n",
    "            break\n",
    "\n",
    "print(\"=> Best Recall@5: {:.4f}\".format(best_score), flush=True)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
