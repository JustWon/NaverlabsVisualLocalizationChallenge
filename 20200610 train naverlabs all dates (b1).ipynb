{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_posDistThr = 5\n",
    "G_posDistSqThr = 25\n",
    "G_nonTrivPosDistSqThr = 20\n",
    "G_cache_name = '_feat_cache_naverlabs.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "import random, shutil, json\n",
    "from math import log10, ceil\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "import faiss\n",
    "\n",
    "import netvlad\n",
    "\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "from os import makedirs, remove, chdir, environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--fromscratch'], dest='fromscratch', nargs=0, const=True, default=False, type=None, choices=None, help='Train from scratch rather than using pretrained models', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='pytorch-NetVlad')\n",
    "parser.add_argument('--mode', type=str, default='train', help='Mode', choices=['train', 'test', 'cluster'])\n",
    "parser.add_argument('--batchSize', type=int, default=4, help='Number of triplets (query, pos, negs). Each triplet consists of 12 images.')\n",
    "parser.add_argument('--cacheBatchSize', type=int, default=24, help='Batch size for caching and testing')\n",
    "parser.add_argument('--cacheRefreshRate', type=int, default=1000, help='How often to refresh cache, in number of queries. 0 for off')\n",
    "parser.add_argument('--nEpochs', type=int, default=30, help='number of epochs to train for')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--nGPU', type=int, default=1, help='number of GPU to use.')\n",
    "parser.add_argument('--optim', type=str, default='SGD', help='optimizer to use', choices=['SGD', 'ADAM'])\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate.')\n",
    "parser.add_argument('--lrStep', type=float, default=5, help='Decay LR ever N steps.')\n",
    "parser.add_argument('--lrGamma', type=float, default=0.5, help='Multiply LR by Gamma for decaying.')\n",
    "parser.add_argument('--weightDecay', type=float, default=0.001, help='Weight decay for SGD.')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGD.')\n",
    "parser.add_argument('--nocuda', action='store_true', help='Dont use cuda')\n",
    "parser.add_argument('--threads', type=int, default=8, help='Number of threads for each data loader to use')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed to use.')\n",
    "parser.add_argument('--dataPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/data/', help='Path for centroid data.')\n",
    "parser.add_argument('--runsPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', help='Path to save runs to.')\n",
    "parser.add_argument('--savePath', type=str, default='checkpoints', help='Path to save checkpoints to in logdir. Default=checkpoints/')\n",
    "parser.add_argument('--cachePath', type=str, default='/tmp', help='Path to save cache to.')\n",
    "parser.add_argument('--resume', type=str, default='', help='Path to load checkpoint from, for resuming training or testing.')\n",
    "parser.add_argument('--ckpt', type=str, default='latest', help='Resume from latest or best checkpoint.', choices=['latest', 'best'])\n",
    "parser.add_argument('--evalEvery', type=int, default=1, help='Do a validation set run, and save, every N epochs.')\n",
    "parser.add_argument('--patience', type=int, default=10, help='Patience for early stopping. 0 is off.')\n",
    "parser.add_argument('--dataset', type=str, default='pittsburgh', help='Dataset to use', choices=['pittsburgh','naverlabs','naverlabs_b1'])\n",
    "parser.add_argument('--arch', type=str, default='vgg16', help='basenetwork to use', choices=['vgg16', 'alexnet'])\n",
    "parser.add_argument('--vladv2', action='store_true', help='Use VLAD v2')\n",
    "parser.add_argument('--pooling', type=str, default='netvlad', help='type of pooling to use', choices=['netvlad', 'max', 'avg'])\n",
    "parser.add_argument('--num_clusters', type=int, default=64, help='Number of NetVlad clusters. Default=64')\n",
    "parser.add_argument('--margin', type=float, default=0.1, help='Margin for triplet loss. Default=0.1')\n",
    "parser.add_argument('--split', type=str, default='val', help='Data split to use for testing. Default is val', choices=['test', 'test250k', 'train', 'val'])\n",
    "parser.add_argument('--fromscratch', action='store_true', help='Train from scratch rather than using pretrained models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "from os.path import join, exists\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import h5py\n",
    "\n",
    "def input_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def get_whole_training_set(onlyDB=False):\n",
    "    return WholeDatasetFromStruct(input_transform=input_transform(), mode='train')\n",
    "\n",
    "def get_training_query_set(margin=0.1):\n",
    "    return QueryDatasetFromStruct(input_transform=input_transform(), margin=margin, mode='train')\n",
    "\n",
    "def get_whole_val_set():\n",
    "    return WholeDatasetFromStruct(input_transform=input_transform(), mode='val')\n",
    "\n",
    "dbStruct = namedtuple('dbStruct', ['whichSet', 'dataset', \n",
    "                                   'db_image', 'db_utms', 'db_num', 'db_full_pose',\n",
    "                                   'q_image', 'q_utms', 'q_num', 'q_full_pose',\n",
    "                                   'posDistThr', 'posDistSqThr', 'nonTrivPosDistSqThr'])\n",
    "\n",
    "class WholeDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, input_transform=None, mode='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "        self.dbStruct = my_parse_dbStruct(mode)\n",
    "        self.images = np.hstack([self.dbStruct.db_image, self.dbStruct.q_image])\n",
    "\n",
    "        self.whichSet = self.dbStruct.whichSet\n",
    "        self.dataset = self.dbStruct.dataset\n",
    "\n",
    "        self.positives = None\n",
    "        self.distances = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index])\n",
    "        img = img.resize((640, 480))\n",
    "\n",
    "        if self.input_transform:\n",
    "            img = self.input_transform(img)\n",
    "\n",
    "        return img, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def getPositives(self):\n",
    "        # positives for evaluation are those within trivial threshold range\n",
    "        #fit NN to find them, search by radius\n",
    "        if  self.positives is None:\n",
    "            knn = NearestNeighbors(n_jobs=-1)\n",
    "            knn.fit(self.dbStruct.db_utms)\n",
    "\n",
    "            self.distances, self.positives = knn.radius_neighbors(self.dbStruct.q_utms, radius=self.dbStruct.posDistThr)\n",
    "\n",
    "        return self.positives\n",
    "\n",
    "class QueryDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, nNegSample=1000, nNeg=10, margin=0.1, input_transform=None, mode='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.margin = margin\n",
    "\n",
    "        self.dbStruct = my_parse_dbStruct(mode)\n",
    "        self.whichSet = self.dbStruct.whichSet\n",
    "        self.dataset = self.dbStruct.dataset\n",
    "        self.nNegSample = nNegSample # number of negatives to randomly sample\n",
    "        self.nNeg = nNeg # number of negatives used for training\n",
    "\n",
    "        # potential positives are those within nontrivial threshold range\n",
    "        #fit NN to find them, search by radius\n",
    "        knn = NearestNeighbors(n_jobs=-1)\n",
    "        knn.fit(self.dbStruct.db_utms)\n",
    "\n",
    "        # TODO use sqeuclidean as metric?\n",
    "        self.nontrivial_positives = list(knn.radius_neighbors(self.dbStruct.q_utms,\n",
    "                radius=self.dbStruct.nonTrivPosDistSqThr**0.5, \n",
    "                return_distance=False))\n",
    "        # radius returns unsorted, sort once now so we dont have to later\n",
    "        for i,posi in enumerate(self.nontrivial_positives):\n",
    "            self.nontrivial_positives[i] = np.sort(posi)\n",
    "        # its possible some queries don't have any non trivial potential positives\n",
    "        # lets filter those out\n",
    "        self.queries = np.where(np.array([len(x) for x in self.nontrivial_positives])>0)[0]\n",
    "\n",
    "        # potential negatives are those outside of posDistThr range\n",
    "        potential_positives = knn.radius_neighbors(self.dbStruct.q_utms,\n",
    "                radius=self.dbStruct.posDistThr, \n",
    "                return_distance=False)\n",
    "\n",
    "        self.potential_negatives = []\n",
    "        for pos in potential_positives:\n",
    "            self.potential_negatives.append(np.setdiff1d(np.arange(self.dbStruct.db_num),pos, assume_unique=True))\n",
    "\n",
    "        self.cache = None # filepath of HDF5 containing feature vectors for images\n",
    "\n",
    "        self.negCache = [np.empty((0,)) for _ in range(self.dbStruct.q_num)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.queries[index] # re-map index to match dataset\n",
    "        with h5py.File(self.cache, mode='r') as h5: \n",
    "            h5feat = h5.get(\"features\")\n",
    "\n",
    "            qOffset = self.dbStruct.db_num \n",
    "            qFeat = h5feat[index+qOffset]\n",
    "\n",
    "            posFeat = h5feat[self.nontrivial_positives[index].tolist()]\n",
    "            knn = NearestNeighbors(n_jobs=-1) # TODO replace with faiss?\n",
    "            knn.fit(posFeat)\n",
    "            dPos, posNN = knn.kneighbors(qFeat.reshape(1,-1), 1)\n",
    "            dPos = dPos.item()\n",
    "            posIndex = self.nontrivial_positives[index][posNN[0]].item()\n",
    "\n",
    "            negSample = np.random.choice(self.potential_negatives[index], self.nNegSample)\n",
    "            negSample = np.unique(np.concatenate([self.negCache[index], negSample]))\n",
    "\n",
    "            negFeat = h5feat[negSample.tolist()]\n",
    "            knn.fit(negFeat)\n",
    "\n",
    "            dNeg, negNN = knn.kneighbors(qFeat.reshape(1,-1), self.nNeg*10) # to quote netvlad paper code: 10x is hacky but fine\n",
    "            dNeg = dNeg.reshape(-1)\n",
    "            negNN = negNN.reshape(-1)\n",
    "\n",
    "            # try to find negatives that are within margin, if there aren't any return none\n",
    "            violatingNeg = dNeg < dPos + self.margin**0.5\n",
    "     \n",
    "            if np.sum(violatingNeg) < 1:\n",
    "                #if none are violating then skip this query\n",
    "                return None\n",
    "\n",
    "            negNN = negNN[violatingNeg][:self.nNeg]\n",
    "            negIndices = negSample[negNN].astype(np.int32)\n",
    "            self.negCache[index] = negIndices\n",
    "\n",
    "        query = Image.open(self.dbStruct.q_image[index])\n",
    "        query = query.resize((640, 480))\n",
    "        positive = Image.open(self.dbStruct.db_image[posIndex])\n",
    "        positive = positive.resize((640, 480))\n",
    "\n",
    "        if self.input_transform:\n",
    "            query = self.input_transform(query)\n",
    "            positive = self.input_transform(positive)\n",
    "\n",
    "        negatives = []\n",
    "        for negIndex in negIndices:\n",
    "            negative = Image.open(self.dbStruct.db_image[negIndex])\n",
    "            negative = negative.resize((640, 480))\n",
    "            if self.input_transform:\n",
    "                negative = self.input_transform(negative)\n",
    "            negatives.append(negative)\n",
    "\n",
    "        negatives = torch.stack(negatives, 0)\n",
    "\n",
    "        return query, positive, negatives, [index, posIndex]+negIndices.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_parse_dbStruct(_whichSet='train'):\n",
    "\n",
    "    whichSet = _whichSet\n",
    "    dataset = 'naverlabs_b1'\n",
    "    \n",
    "    # for (2019-04-16_15-35-46)\n",
    "    image_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/b1/train/2019-04-16_15-35-46/images'\n",
    "    image_files_list = []\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970285*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970286*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970288*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970289*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970290*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970291*.jpg'))))\n",
    "    \n",
    "    gt_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/b1/train/2019-04-16_15-35-46/groundtruth.hdf5'\n",
    "    full_pose_list = []\n",
    "    with h5py.File(gt_path, \"r\") as f:\n",
    "        full_pose_list.append(np.array(f['22970285_pose']))\n",
    "        full_pose_list.append(np.array(f['22970286_pose']))\n",
    "        full_pose_list.append(np.array(f['22970288_pose']))\n",
    "        full_pose_list.append(np.array(f['22970289_pose']))\n",
    "        full_pose_list.append(np.array(f['22970290_pose']))\n",
    "        full_pose_list.append(np.array(f['22970291_pose']))\n",
    "    \n",
    "    # for (2019-04-16_16-14-48)\n",
    "    image_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/b1/train/2019-04-16_16-14-48/images'\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970285*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970286*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970288*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970289*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970290*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970291*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324954*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324955*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324969*.jpg'))))\n",
    "    \n",
    "    gt_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/b1/train/2019-04-16_16-14-48/groundtruth.hdf5'\n",
    "    with h5py.File(gt_path, \"r\") as f:\n",
    "        full_pose_list.append(np.array(f['22970285_pose']))\n",
    "        full_pose_list.append(np.array(f['22970286_pose']))\n",
    "        full_pose_list.append(np.array(f['22970288_pose']))\n",
    "        full_pose_list.append(np.array(f['22970289_pose']))\n",
    "        full_pose_list.append(np.array(f['22970290_pose']))\n",
    "        full_pose_list.append(np.array(f['22970291_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324954_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324955_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324969_pose']))\n",
    "    \n",
    "    # for (2019-08-20_10-41-18)\n",
    "    image_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/b1/train/2019-08-20_10-41-18/images'\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324954*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324955*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324968*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324969*.jpg'))))\n",
    "    \n",
    "    gt_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/b1/train/2019-08-20_10-41-18/groundtruth.hdf5'\n",
    "    with h5py.File(gt_path, \"r\") as f:\n",
    "        full_pose_list.append(np.array(f['AC01324954_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324955_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324968_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324969_pose']))\n",
    "    \n",
    "    image_files = np.hstack(image_files_list)    \n",
    "    full_images = image_files\n",
    "    full_pose = np.vstack(full_pose_list)\n",
    "    full_utms = full_pose[:,:2]\n",
    "    \n",
    "    partition_idx = int(len(full_images)*0.8)\n",
    "    \n",
    "    db_image = full_images[:partition_idx]\n",
    "    db_utms = full_utms[:partition_idx]\n",
    "    db_num = len(db_image)\n",
    "    db_full_pose = full_pose[:partition_idx]\n",
    "    \n",
    "    q_image = full_images[partition_idx:]\n",
    "    q_utms = full_utms[partition_idx:]\n",
    "    q_num = len(q_image)\n",
    "    q_full_pose = full_pose[partition_idx:]\n",
    "    \n",
    "\n",
    "    return dbStruct(whichSet, dataset, \n",
    "                    db_image, db_utms, db_num, db_full_pose,\n",
    "                    q_image, q_utms, q_num, q_full_pose, \n",
    "                    5, 25, 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (query, positive, negatives).\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuple (query, positive, negatives). \n",
    "            - query: torch tensor of shape (3, h, w).\n",
    "            - positive: torch tensor of shape (3, h, w).\n",
    "            - negative: torch tensor of shape (n, 3, h, w).\n",
    "    Returns:\n",
    "        query: torch tensor of shape (batch_size, 3, h, w).\n",
    "        positive: torch tensor of shape (batch_size, 3, h, w).\n",
    "        negatives: torch tensor of shape (batch_size, n, 3, h, w).\n",
    "    \"\"\"\n",
    "\n",
    "    batch = list(filter (lambda x:x is not None, batch))\n",
    "    if len(batch) == 0: return None, None, None, None, None\n",
    "\n",
    "    query, positive, negatives, indices = zip(*batch)\n",
    "\n",
    "    query = data.dataloader.default_collate(query)\n",
    "    positive = data.dataloader.default_collate(positive)\n",
    "    negCounts = data.dataloader.default_collate([x.shape[0] for x in negatives])\n",
    "    negatives = torch.cat(negatives, 0)\n",
    "    import itertools\n",
    "    indices = list(itertools.chain(*indices))\n",
    "\n",
    "    return query, positive, negatives, negCounts, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    epoch_loss = 0\n",
    "    startIter = 1 # keep track of batch iter across subsets for logging\n",
    "\n",
    "    if opt.cacheRefreshRate > 0:\n",
    "        subsetN = ceil(len(train_set) / opt.cacheRefreshRate)\n",
    "        #TODO randomise the arange before splitting?\n",
    "        subsetIdx = np.array_split(np.arange(len(train_set)), subsetN)\n",
    "    else:\n",
    "        subsetN = 1\n",
    "        subsetIdx = [np.arange(len(train_set))]\n",
    "\n",
    "    nBatches = (len(train_set) + opt.batchSize - 1) // opt.batchSize\n",
    "\n",
    "    for subIter in range(subsetN):\n",
    "        print('====> Building Cache')\n",
    "        model.eval()\n",
    "        train_set.cache = join(opt.cachePath, train_set.whichSet + G_cache_name)\n",
    "        with h5py.File(train_set.cache, mode='w') as h5: \n",
    "            pool_size = encoder_dim\n",
    "            if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters\n",
    "            h5feat = h5.create_dataset(\"features\", \n",
    "                    [len(whole_train_set), pool_size], \n",
    "                    dtype=np.float32)\n",
    "            with torch.no_grad():\n",
    "                for iteration, (input, indices) in enumerate(whole_training_data_loader, 1):\n",
    "                    input = input.to(device)\n",
    "                    image_encoding = model.encoder(input)\n",
    "                    vlad_encoding = model.pool(image_encoding) \n",
    "                    h5feat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()\n",
    "                    del input, image_encoding, vlad_encoding\n",
    "\n",
    "        sub_train_set = Subset(dataset=train_set, indices=subsetIdx[subIter])\n",
    "\n",
    "        training_data_loader = DataLoader(dataset=sub_train_set, num_workers=opt.threads, \n",
    "                    batch_size=opt.batchSize, shuffle=True, \n",
    "                    collate_fn=collate_fn, pin_memory=cuda)\n",
    "\n",
    "        print('Allocated:', torch.cuda.memory_allocated())\n",
    "        print('Cached:', torch.cuda.memory_cached())\n",
    "\n",
    "        model.train()\n",
    "        for iteration, (query, positives, negatives, negCounts, indices) in enumerate(training_data_loader, startIter):\n",
    "            # some reshaping to put query, pos, negs in a single (N, 3, H, W) tensor\n",
    "            # where N = batchSize * (nQuery + nPos + nNeg)\n",
    "            if query is None: continue # in case we get an empty batch\n",
    "\n",
    "            B, C, H, W = query.shape\n",
    "            nNeg = torch.sum(negCounts)\n",
    "            input = torch.cat([query, positives, negatives])\n",
    "\n",
    "            input = input.to(device)\n",
    "            image_encoding = model.encoder(input)\n",
    "            vlad_encoding = model.pool(image_encoding) \n",
    "\n",
    "            vladQ, vladP, vladN = torch.split(vlad_encoding, [B, B, nNeg])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # calculate loss for each Query, Positive, Negative triplet\n",
    "            # due to potential difference in number of negatives have to \n",
    "            # do it per query, per negative\n",
    "            loss = 0\n",
    "            for i, negCount in enumerate(negCounts):\n",
    "                for n in range(negCount):\n",
    "                    negIx = (torch.sum(negCounts[:i]) + n).item()\n",
    "                    loss += criterion(vladQ[i:i+1], vladP[i:i+1], vladN[negIx:negIx+1])\n",
    "\n",
    "            loss /= nNeg.float().to(device) # normalise by actual number of negatives\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del input, image_encoding, vlad_encoding, vladQ, vladP, vladN\n",
    "            del query, positives, negatives\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            if iteration % 50 == 0 or nBatches <= 10:\n",
    "                print(\"==> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, \n",
    "                    nBatches, batch_loss), flush=True)\n",
    "                writer.add_scalar('Train/Loss', batch_loss, \n",
    "                        ((epoch-1) * nBatches) + iteration)\n",
    "                writer.add_scalar('Train/nNeg', nNeg, \n",
    "                        ((epoch-1) * nBatches) + iteration)\n",
    "                print('Allocated:', torch.cuda.memory_allocated())\n",
    "                print('Cached:', torch.cuda.memory_cached())\n",
    "\n",
    "        startIter += len(training_data_loader)\n",
    "        del training_data_loader, loss\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        remove(train_set.cache) # delete HDF5 cache\n",
    "\n",
    "    avg_loss = epoch_loss / nBatches\n",
    "\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, avg_loss), flush=True)\n",
    "    writer.add_scalar('Train/AvgLoss', avg_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(eval_set, epoch=0, write_tboard=False):\n",
    "    # TODO what if features dont fit in memory? \n",
    "    test_data_loader = DataLoader(dataset=eval_set, \n",
    "                num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False, \n",
    "                pin_memory=cuda)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('====> Extracting Features')\n",
    "        pool_size = encoder_dim\n",
    "        if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters\n",
    "        dbFeat = np.empty((len(eval_set), pool_size))\n",
    "\n",
    "        for iteration, (input, indices) in enumerate(test_data_loader, 1):\n",
    "            input = input.to(device)\n",
    "            image_encoding = model.encoder(input)\n",
    "            vlad_encoding = model.pool(image_encoding) \n",
    "\n",
    "            dbFeat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()\n",
    "            if iteration % 50 == 0 or len(test_data_loader) <= 10:\n",
    "                print(\"==> Batch ({}/{})\".format(iteration, \n",
    "                    len(test_data_loader)), flush=True)\n",
    "\n",
    "            del input, image_encoding, vlad_encoding\n",
    "    del test_data_loader\n",
    "\n",
    "    # extracted for both db and query, now split in own sets\n",
    "    qFeat = dbFeat[eval_set.dbStruct.db_num:].astype('float32')\n",
    "    dbFeat = dbFeat[:eval_set.dbStruct.db_num].astype('float32')\n",
    "    \n",
    "    print('====> Building faiss index')\n",
    "    faiss_index = faiss.IndexFlatL2(pool_size)\n",
    "    faiss_index.add(dbFeat)\n",
    "\n",
    "    print('====> Calculating recall @ N')\n",
    "    n_values = [1,5,10,20]\n",
    "\n",
    "    _, predictions = faiss_index.search(qFeat, max(n_values)) \n",
    "\n",
    "    # for each query get those within threshold distance\n",
    "    gt = eval_set.getPositives() \n",
    "\n",
    "    correct_at_n = np.zeros(len(n_values))\n",
    "    #TODO can we do this on the matrix in one go?\n",
    "    for qIx, pred in enumerate(predictions):\n",
    "        for i,n in enumerate(n_values):\n",
    "            # if in top N then also in top NN, where NN > N\n",
    "            if np.any(np.in1d(pred[:n], gt[qIx])):\n",
    "                correct_at_n[i:] += 1\n",
    "                break\n",
    "    recall_at_n = correct_at_n / eval_set.dbStruct.q_num\n",
    "\n",
    "    recalls = {} #make dict for output\n",
    "    for i,n in enumerate(n_values):\n",
    "        recalls[n] = recall_at_n[i]\n",
    "        print(\"====> Recall@{}: {:.4f}\".format(n, recall_at_n[i]))\n",
    "        if write_tboard: writer.add_scalar('Val/Recall@' + str(n), recall_at_n[i], epoch)\n",
    "\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    model_out_path = join(opt.savePath, filename)\n",
    "    torch.save(state, model_out_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(model_out_path, join(opt.savePath, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='vgg16', batchSize=4, cacheBatchSize=24, cachePath='/tmp', cacheRefreshRate=1000, ckpt='latest', dataPath='/home/ubuntu/Desktop/pytorch-NetVlad/data/', dataset='naverlabs_b1', evalEvery=1, fromscratch=False, lr=0.0001, lrGamma=0.5, lrStep=5, margin=0.1, mode='train', momentum=0.9, nEpochs=30, nGPU=1, nocuda=False, num_clusters=64, optim='SGD', patience=10, pooling='netvlad', resume='', runsPath='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', savePath='checkpoints', seed=123, split='val', start_epoch=0, threads=8, vladv2=False, weightDecay=0.001)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args(args='--dataset=naverlabs_b1 --mode=train --arch=vgg16 --pooling=netvlad --num_clusters=64'.split(' '))\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = not opt.nocuda\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Training query set: 5292\n",
      "===> Evaluating on val set, query count: 5358\n"
     ]
    }
   ],
   "source": [
    "whole_train_set = get_whole_training_set()\n",
    "whole_training_data_loader = DataLoader(dataset=whole_train_set, \n",
    "        num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False, \n",
    "        pin_memory=cuda)\n",
    "\n",
    "train_set = get_training_query_set(opt.margin)\n",
    "\n",
    "print('====> Training query set:', len(train_set))\n",
    "whole_test_set = get_whole_val_set()\n",
    "print('===> Evaluating on val set, query count:', whole_test_set.dbStruct.q_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = not opt.fromscratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = 512\n",
    "encoder = models.vgg16(pretrained=pretrained)\n",
    "# capture only feature part and remove last relu and maxpool\n",
    "layers = list(encoder.features.children())[:-2]\n",
    "\n",
    "if pretrained:\n",
    "    # if using pretrained then only train conv5_1, conv5_2, and conv5_3\n",
    "    for l in layers[:-5]: \n",
    "        for p in l.parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(*layers)\n",
    "model = nn.Module() \n",
    "model.add_module('encoder', encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_vlad = netvlad.NetVLAD(num_clusters=opt.num_clusters, dim=encoder_dim, vladv2=opt.vladv2)\n",
    "if not opt.resume: \n",
    "    if opt.mode.lower() == 'train':\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + train_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "    else:\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + whole_test_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "\n",
    "    if not exists(initcache):\n",
    "        raise FileNotFoundError('Could not find clusters, please run with --mode=cluster before proceeding')\n",
    "\n",
    "    with h5py.File(initcache, mode='r') as h5: \n",
    "        clsts = h5.get(\"centroids\")[...]\n",
    "        traindescs = h5.get(\"descriptors\")[...]\n",
    "        net_vlad.init_params(clsts, traindescs) \n",
    "        del clsts, traindescs\n",
    "\n",
    "model.add_module('pool', net_vlad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Desktop/pytorch-NetVlad/data/centroids/vgg16_naverlabs_b1_64_desc_cen.hdf5'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "isParallel = False\n",
    "if opt.nGPU > 1 and torch.cuda.device_count() > 1:\n",
    "    model.encoder = nn.DataParallel(model.encoder)\n",
    "    if opt.mode.lower() != 'cluster':\n",
    "        model.pool = nn.DataParallel(model.pool)\n",
    "    isParallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.optim.upper() == 'ADAM':\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, \n",
    "        model.parameters()), lr=opt.lr)#, betas=(0,0.9))\n",
    "elif opt.optim.upper() == 'SGD':\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, \n",
    "        model.parameters()), lr=opt.lr,\n",
    "        momentum=opt.momentum,\n",
    "        weight_decay=opt.weightDecay)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lrStep, gamma=opt.lrGamma)\n",
    "else:\n",
    "    raise ValueError('Unknown optimizer: ' + opt.optim)\n",
    "\n",
    "# original paper/code doesn't sqrt() the distances, we do, so sqrt() the margin, I think :D\n",
    "criterion = nn.TripletMarginLoss(margin=opt.margin**0.5, p=2, reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Training model\n",
      "===> Saving state to: /home/ubuntu/Desktop/pytorch-NetVlad/runs/Jun15_22-35-11_vgg16_netvlad\n",
      "====> Building Cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 60039168\n",
      "Cached: 13847494656\n",
      "==> Epoch[1](50/1323): Loss: 0.2182\n",
      "Allocated: 117199360\n",
      "Cached: 28730982400\n",
      "==> Epoch[1](100/1323): Loss: 0.1509\n",
      "Allocated: 117199360\n",
      "Cached: 28730982400\n",
      "==> Epoch[1](150/1323): Loss: 0.2500\n",
      "Allocated: 117199360\n",
      "Cached: 28730982400\n",
      "==> Epoch[1](200/1323): Loss: 0.2196\n",
      "Allocated: 117199360\n",
      "Cached: 28730982400\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](250/1323): Loss: 0.2369\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](300/1323): Loss: 0.2332\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](350/1323): Loss: 0.2186\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](400/1323): Loss: 0.2004\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](450/1323): Loss: 0.2573\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[1](500/1323): Loss: 0.1857\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](550/1323): Loss: 0.1662\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](600/1323): Loss: 0.2646\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[1](650/1323): Loss: 0.2224\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](700/1323): Loss: 0.2102\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](750/1323): Loss: 0.1984\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](800/1323): Loss: 0.2298\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](850/1323): Loss: 0.2348\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[1](900/1323): Loss: 0.2062\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](950/1323): Loss: 0.2142\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1000/1323): Loss: 0.2118\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1050/1323): Loss: 0.2151\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[1](1100/1323): Loss: 0.2095\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[1](1150/1323): Loss: 0.2459\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[1](1200/1323): Loss: 0.2833\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[1](1250/1323): Loss: 0.2566\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[1](1300/1323): Loss: 0.2202\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 1 Complete: Avg. Loss: 0.2258\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.5862\n",
      "====> Recall@5: 0.7835\n",
      "====> Recall@10: 0.8445\n",
      "====> Recall@20: 0.8929\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](50/1323): Loss: 0.2044\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](100/1323): Loss: 0.2334\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](150/1323): Loss: 0.1798\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](200/1323): Loss: 0.2406\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](250/1323): Loss: 0.1465\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](300/1323): Loss: 0.2255\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](350/1323): Loss: 0.2105\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](400/1323): Loss: 0.2217\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](450/1323): Loss: 0.1910\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[2](500/1323): Loss: 0.2210\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](550/1323): Loss: 0.2024\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](600/1323): Loss: 0.1518\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](650/1323): Loss: 0.2321\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](700/1323): Loss: 0.2563\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](750/1323): Loss: 0.2306\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](800/1323): Loss: 0.1271\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](850/1323): Loss: 0.2363\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[2](900/1323): Loss: 0.1041\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](950/1323): Loss: 0.1667\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1000/1323): Loss: 0.1803\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1050/1323): Loss: 0.2132\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[2](1100/1323): Loss: 0.2063\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[2](1150/1323): Loss: 0.2043\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[2](1200/1323): Loss: 0.2163\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[2](1250/1323): Loss: 0.2409\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[2](1300/1323): Loss: 0.2784\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 2 Complete: Avg. Loss: 0.2189\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6185\n",
      "====> Recall@5: 0.8091\n",
      "====> Recall@10: 0.8664\n",
      "====> Recall@20: 0.9076\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](50/1323): Loss: 0.2329\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](100/1323): Loss: 0.2232\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](150/1323): Loss: 0.2211\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](200/1323): Loss: 0.1572\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](250/1323): Loss: 0.1432\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](300/1323): Loss: 0.1976\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](350/1323): Loss: 0.2034\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](400/1323): Loss: 0.1486\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](450/1323): Loss: 0.1493\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](500/1323): Loss: 0.2135\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[3](550/1323): Loss: 0.1982\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](600/1323): Loss: 0.1078\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[3](650/1323): Loss: 0.2230\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](700/1323): Loss: 0.1705\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](750/1323): Loss: 0.1963\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](800/1323): Loss: 0.2114\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](850/1323): Loss: 0.2438\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[3](900/1323): Loss: 0.1379\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[3](950/1323): Loss: 0.2579\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1000/1323): Loss: 0.2984\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1050/1323): Loss: 0.1844\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1100/1323): Loss: 0.2505\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[3](1150/1323): Loss: 0.2424\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[3](1200/1323): Loss: 0.2510\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[3](1250/1323): Loss: 0.2365\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[3](1300/1323): Loss: 0.2264\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 3 Complete: Avg. Loss: 0.2152\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6342\n",
      "====> Recall@5: 0.8244\n",
      "====> Recall@10: 0.8753\n",
      "====> Recall@20: 0.9160\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](50/1323): Loss: 0.2150\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](100/1323): Loss: 0.2236\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](150/1323): Loss: 0.2194\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](200/1323): Loss: 0.2226\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](250/1323): Loss: 0.1499\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](300/1323): Loss: 0.1804\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](350/1323): Loss: 0.2405\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](400/1323): Loss: 0.2140\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](450/1323): Loss: 0.2047\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[4](500/1323): Loss: 0.1714\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[4](550/1323): Loss: 0.1520\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](600/1323): Loss: 0.1571\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](650/1323): Loss: 0.2194\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](700/1323): Loss: 0.2373\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](750/1323): Loss: 0.2279\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](800/1323): Loss: 0.2254\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](850/1323): Loss: 0.2019\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](900/1323): Loss: 0.2122\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](950/1323): Loss: 0.2013\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1000/1323): Loss: 0.2586\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[4](1050/1323): Loss: 0.1656\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1100/1323): Loss: 0.2681\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[4](1150/1323): Loss: 0.2021\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[4](1200/1323): Loss: 0.2761\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[4](1250/1323): Loss: 0.2492\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[4](1300/1323): Loss: 0.2243\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 4 Complete: Avg. Loss: 0.2120\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6459\n",
      "====> Recall@5: 0.8317\n",
      "====> Recall@10: 0.8820\n",
      "====> Recall@20: 0.9183\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](50/1323): Loss: 0.1901\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](100/1323): Loss: 0.2091\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](150/1323): Loss: 0.2100\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](200/1323): Loss: 0.1883\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](250/1323): Loss: 0.2403\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](300/1323): Loss: 0.1827\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](350/1323): Loss: 0.1936\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](400/1323): Loss: 0.2156\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](450/1323): Loss: 0.2379\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[5](500/1323): Loss: 0.2295\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](550/1323): Loss: 0.2099\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[5](600/1323): Loss: 0.1820\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](650/1323): Loss: 0.1737\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](700/1323): Loss: 0.2156\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](750/1323): Loss: 0.1913\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](800/1323): Loss: 0.2434\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](850/1323): Loss: 0.2061\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](900/1323): Loss: 0.1942\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[5](950/1323): Loss: 0.2570\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1000/1323): Loss: 0.2027\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1050/1323): Loss: 0.1979\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[5](1100/1323): Loss: 0.1676\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[5](1150/1323): Loss: 0.2460\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[5](1200/1323): Loss: 0.2118\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[5](1250/1323): Loss: 0.2308\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[5](1300/1323): Loss: 0.2188\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 5 Complete: Avg. Loss: 0.2096\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6527\n",
      "====> Recall@5: 0.8359\n",
      "====> Recall@10: 0.8850\n",
      "====> Recall@20: 0.9197\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](50/1323): Loss: 0.2023\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](100/1323): Loss: 0.1733\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](150/1323): Loss: 0.2067\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](200/1323): Loss: 0.2156\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](250/1323): Loss: 0.1922\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](300/1323): Loss: 0.1790\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](350/1323): Loss: 0.2048\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](400/1323): Loss: 0.2142\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[6](450/1323): Loss: 0.2669\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](500/1323): Loss: 0.2036\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](550/1323): Loss: 0.2527\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](600/1323): Loss: 0.1812\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](650/1323): Loss: 0.1847\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](700/1323): Loss: 0.2518\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](750/1323): Loss: 0.2036\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](800/1323): Loss: 0.1723\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](850/1323): Loss: 0.1724\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[6](900/1323): Loss: 0.2766\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](950/1323): Loss: 0.2341\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1000/1323): Loss: 0.2532\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[6](1050/1323): Loss: 0.1201\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1100/1323): Loss: 0.1955\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[6](1150/1323): Loss: 0.2322\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[6](1200/1323): Loss: 0.2630\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1250/1323): Loss: 0.2699\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[6](1300/1323): Loss: 0.2043\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 6 Complete: Avg. Loss: 0.2081\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6573\n",
      "====> Recall@5: 0.8393\n",
      "====> Recall@10: 0.8863\n",
      "====> Recall@20: 0.9214\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](50/1323): Loss: 0.2016\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](100/1323): Loss: 0.2455\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](150/1323): Loss: 0.1128\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](200/1323): Loss: 0.2137\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](250/1323): Loss: 0.1460\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](300/1323): Loss: 0.1620\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](350/1323): Loss: 0.1585\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](400/1323): Loss: 0.1904\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](450/1323): Loss: 0.1974\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](500/1323): Loss: 0.2045\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[7](550/1323): Loss: 0.1921\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[7](600/1323): Loss: 0.2167\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](650/1323): Loss: 0.2138\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](700/1323): Loss: 0.2235\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](750/1323): Loss: 0.2381\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](800/1323): Loss: 0.2116\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](850/1323): Loss: 0.2118\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[7](900/1323): Loss: 0.1967\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](950/1323): Loss: 0.2057\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1000/1323): Loss: 0.2214\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1050/1323): Loss: 0.1763\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[7](1100/1323): Loss: 0.1968\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[7](1150/1323): Loss: 0.2134\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[7](1200/1323): Loss: 0.2511\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[7](1250/1323): Loss: 0.2381\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[7](1300/1323): Loss: 0.2471\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 7 Complete: Avg. Loss: 0.2068\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6652\n",
      "====> Recall@5: 0.8425\n",
      "====> Recall@10: 0.8876\n",
      "====> Recall@20: 0.9231\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](50/1323): Loss: 0.2196\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](100/1323): Loss: 0.2161\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](150/1323): Loss: 0.1713\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](200/1323): Loss: 0.1706\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](250/1323): Loss: 0.1999\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](300/1323): Loss: 0.1770\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](350/1323): Loss: 0.2167\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](400/1323): Loss: 0.1992\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](450/1323): Loss: 0.1938\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[8](500/1323): Loss: 0.2054\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](550/1323): Loss: 0.1472\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](600/1323): Loss: 0.1518\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](650/1323): Loss: 0.2075\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](700/1323): Loss: 0.1789\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](750/1323): Loss: 0.2182\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](800/1323): Loss: 0.2241\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](850/1323): Loss: 0.2066\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[8](900/1323): Loss: 0.1852\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[8](950/1323): Loss: 0.2425\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1000/1323): Loss: 0.2153\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1050/1323): Loss: 0.1740\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[8](1100/1323): Loss: 0.2474\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[8](1150/1323): Loss: 0.1854\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[8](1200/1323): Loss: 0.2231\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[8](1250/1323): Loss: 0.2114\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[8](1300/1323): Loss: 0.2558\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 8 Complete: Avg. Loss: 0.2054\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6710\n",
      "====> Recall@5: 0.8455\n",
      "====> Recall@10: 0.8912\n",
      "====> Recall@20: 0.9239\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](50/1323): Loss: 0.1900\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](100/1323): Loss: 0.2194\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](150/1323): Loss: 0.2313\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](200/1323): Loss: 0.1932\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](250/1323): Loss: 0.2159\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](300/1323): Loss: 0.1912\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](350/1323): Loss: 0.2588\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](400/1323): Loss: 0.2140\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](450/1323): Loss: 0.2492\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](500/1323): Loss: 0.1960\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[9](550/1323): Loss: 0.1580\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[9](600/1323): Loss: 0.1542\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](650/1323): Loss: 0.1950\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](700/1323): Loss: 0.2355\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](750/1323): Loss: 0.2147\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](800/1323): Loss: 0.1867\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](850/1323): Loss: 0.2249\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[9](900/1323): Loss: 0.2184\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](950/1323): Loss: 0.1667\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[9](1000/1323): Loss: 0.2172\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1050/1323): Loss: 0.2097\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1100/1323): Loss: 0.2334\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[9](1150/1323): Loss: 0.2420\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[9](1200/1323): Loss: 0.2356\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1250/1323): Loss: 0.2732\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[9](1300/1323): Loss: 0.1957\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 9 Complete: Avg. Loss: 0.2043\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6775\n",
      "====> Recall@5: 0.8479\n",
      "====> Recall@10: 0.8936\n",
      "====> Recall@20: 0.9250\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](50/1323): Loss: 0.2471\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](100/1323): Loss: 0.1855\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](150/1323): Loss: 0.2377\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](200/1323): Loss: 0.2310\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](250/1323): Loss: 0.2176\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](300/1323): Loss: 0.1794\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](350/1323): Loss: 0.2167\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](400/1323): Loss: 0.1976\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](450/1323): Loss: 0.2188\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](500/1323): Loss: 0.2323\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[10](550/1323): Loss: 0.2193\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[10](600/1323): Loss: 0.1697\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](650/1323): Loss: 0.2089\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](700/1323): Loss: 0.1933\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](750/1323): Loss: 0.2568\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](800/1323): Loss: 0.2624\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](850/1323): Loss: 0.2056\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[10](900/1323): Loss: 0.1913\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[10](950/1323): Loss: 0.2252\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1000/1323): Loss: 0.2298\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1050/1323): Loss: 0.2563\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1100/1323): Loss: 0.1313\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[10](1150/1323): Loss: 0.1802\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1200/1323): Loss: 0.2031\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[10](1250/1323): Loss: 0.2207\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[10](1300/1323): Loss: 0.2179\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 10 Complete: Avg. Loss: 0.2033\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6794\n",
      "====> Recall@5: 0.8486\n",
      "====> Recall@10: 0.8940\n",
      "====> Recall@20: 0.9252\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](50/1323): Loss: 0.2170\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](100/1323): Loss: 0.2390\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](150/1323): Loss: 0.2035\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](200/1323): Loss: 0.1683\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](250/1323): Loss: 0.1751\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](300/1323): Loss: 0.1914\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](350/1323): Loss: 0.1844\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](400/1323): Loss: 0.1396\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](450/1323): Loss: 0.1715\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[11](500/1323): Loss: 0.2260\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](550/1323): Loss: 0.1675\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](600/1323): Loss: 0.2645\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[11](650/1323): Loss: 0.2741\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](700/1323): Loss: 0.2400\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](750/1323): Loss: 0.1740\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](800/1323): Loss: 0.1977\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](850/1323): Loss: 0.2321\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[11](900/1323): Loss: 0.2118\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](950/1323): Loss: 0.2488\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1000/1323): Loss: 0.2477\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[11](1050/1323): Loss: 0.1865\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1100/1323): Loss: 0.2285\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[11](1150/1323): Loss: 0.2443\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[11](1200/1323): Loss: 0.2273\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[11](1250/1323): Loss: 0.2033\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[11](1300/1323): Loss: 0.2546\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 11 Complete: Avg. Loss: 0.2029\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6816\n",
      "====> Recall@5: 0.8498\n",
      "====> Recall@10: 0.8940\n",
      "====> Recall@20: 0.9263\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[12](50/1323): Loss: 0.1179\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](100/1323): Loss: 0.2137\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](150/1323): Loss: 0.1718\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](200/1323): Loss: 0.2125\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[12](250/1323): Loss: 0.1985\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](300/1323): Loss: 0.1935\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](350/1323): Loss: 0.2012\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](400/1323): Loss: 0.2350\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[12](450/1323): Loss: 0.1294\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[12](500/1323): Loss: 0.1467\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](550/1323): Loss: 0.1378\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[12](600/1323): Loss: 0.2036\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](650/1323): Loss: 0.2245\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[12](700/1323): Loss: 0.1973\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](750/1323): Loss: 0.1284\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](800/1323): Loss: 0.2254\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](850/1323): Loss: 0.1950\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[12](900/1323): Loss: 0.1915\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[12](950/1323): Loss: 0.2295\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](1000/1323): Loss: 0.2179\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](1050/1323): Loss: 0.1660\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](1100/1323): Loss: 0.2124\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[12](1150/1323): Loss: 0.1928\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[12](1200/1323): Loss: 0.2334\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[12](1250/1323): Loss: 0.2573\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[12](1300/1323): Loss: 0.2192\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 12 Complete: Avg. Loss: 0.2025\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6837\n",
      "====> Recall@5: 0.8505\n",
      "====> Recall@10: 0.8942\n",
      "====> Recall@20: 0.9268\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[13](50/1323): Loss: 0.1889\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](100/1323): Loss: 0.2089\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](150/1323): Loss: 0.2106\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](200/1323): Loss: 0.2006\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[13](250/1323): Loss: 0.1592\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](300/1323): Loss: 0.2259\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](350/1323): Loss: 0.2080\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](400/1323): Loss: 0.2006\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[13](450/1323): Loss: 0.1423\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](500/1323): Loss: 0.1332\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](550/1323): Loss: 0.2086\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[13](600/1323): Loss: 0.1538\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](650/1323): Loss: 0.1799\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[13](700/1323): Loss: 0.2308\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](750/1323): Loss: 0.2469\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](800/1323): Loss: 0.2228\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](850/1323): Loss: 0.1653\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[13](900/1323): Loss: 0.1429\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](950/1323): Loss: 0.2260\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](1000/1323): Loss: 0.1660\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](1050/1323): Loss: 0.1747\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[13](1100/1323): Loss: 0.2288\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[13](1150/1323): Loss: 0.2713\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[13](1200/1323): Loss: 0.2214\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](1250/1323): Loss: 0.2347\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[13](1300/1323): Loss: 0.2350\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 13 Complete: Avg. Loss: 0.2021\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6863\n",
      "====> Recall@5: 0.8516\n",
      "====> Recall@10: 0.8946\n",
      "====> Recall@20: 0.9276\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[14](50/1323): Loss: 0.1908\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](100/1323): Loss: 0.1942\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](150/1323): Loss: 0.1476\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](200/1323): Loss: 0.1761\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[14](250/1323): Loss: 0.1730\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](300/1323): Loss: 0.2406\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](350/1323): Loss: 0.2443\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](400/1323): Loss: 0.1546\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[14](450/1323): Loss: 0.1411\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[14](500/1323): Loss: 0.1571\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](550/1323): Loss: 0.2005\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](600/1323): Loss: 0.1611\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](650/1323): Loss: 0.2086\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[14](700/1323): Loss: 0.2074\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](750/1323): Loss: 0.1803\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](800/1323): Loss: 0.2080\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](850/1323): Loss: 0.2036\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[14](900/1323): Loss: 0.2373\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](950/1323): Loss: 0.1837\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](1000/1323): Loss: 0.2259\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[14](1050/1323): Loss: 0.2013\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](1100/1323): Loss: 0.2604\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[14](1150/1323): Loss: 0.2426\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](1200/1323): Loss: 0.2187\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[14](1250/1323): Loss: 0.2258\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[14](1300/1323): Loss: 0.2690\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 14 Complete: Avg. Loss: 0.2012\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6874\n",
      "====> Recall@5: 0.8518\n",
      "====> Recall@10: 0.8953\n",
      "====> Recall@20: 0.9287\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[15](50/1323): Loss: 0.2103\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](100/1323): Loss: 0.1956\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](150/1323): Loss: 0.1816\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](200/1323): Loss: 0.1804\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[15](250/1323): Loss: 0.1507\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](300/1323): Loss: 0.2094\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](350/1323): Loss: 0.1611\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](400/1323): Loss: 0.2196\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[15](450/1323): Loss: 0.1771\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[15](500/1323): Loss: 0.1871\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](550/1323): Loss: 0.2082\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[15](600/1323): Loss: 0.2306\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](650/1323): Loss: 0.2483\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[15](700/1323): Loss: 0.1368\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](750/1323): Loss: 0.1818\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](800/1323): Loss: 0.2044\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](850/1323): Loss: 0.1905\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[15](900/1323): Loss: 0.2188\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](950/1323): Loss: 0.2168\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](1000/1323): Loss: 0.1944\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](1050/1323): Loss: 0.2233\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[15](1100/1323): Loss: 0.1887\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[15](1150/1323): Loss: 0.2230\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[15](1200/1323): Loss: 0.2215\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[15](1250/1323): Loss: 0.2009\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[15](1300/1323): Loss: 0.2340\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 15 Complete: Avg. Loss: 0.2006\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6887\n",
      "====> Recall@5: 0.8518\n",
      "====> Recall@10: 0.8953\n",
      "====> Recall@20: 0.9289\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[16](50/1323): Loss: 0.1720\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](100/1323): Loss: 0.1980\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](150/1323): Loss: 0.2384\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](200/1323): Loss: 0.1729\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[16](250/1323): Loss: 0.2078\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](300/1323): Loss: 0.2143\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](350/1323): Loss: 0.2300\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](400/1323): Loss: 0.1856\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[16](450/1323): Loss: 0.1516\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](500/1323): Loss: 0.1478\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[16](550/1323): Loss: 0.1935\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[16](600/1323): Loss: 0.1771\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](650/1323): Loss: 0.1897\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[16](700/1323): Loss: 0.1813\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](750/1323): Loss: 0.2220\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](800/1323): Loss: 0.2292\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](850/1323): Loss: 0.1653\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[16](900/1323): Loss: 0.1791\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](950/1323): Loss: 0.2526\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[16](1000/1323): Loss: 0.2318\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](1050/1323): Loss: 0.2139\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](1100/1323): Loss: 0.2326\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[16](1150/1323): Loss: 0.2097\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[16](1200/1323): Loss: 0.1442\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[16](1250/1323): Loss: 0.1959\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[16](1300/1323): Loss: 0.1890\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 16 Complete: Avg. Loss: 0.2007\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6891\n",
      "====> Recall@5: 0.8537\n",
      "====> Recall@10: 0.8953\n",
      "====> Recall@20: 0.9295\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[17](50/1323): Loss: 0.1432\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](100/1323): Loss: 0.2040\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](150/1323): Loss: 0.2019\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](200/1323): Loss: 0.2109\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[17](250/1323): Loss: 0.1756\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](300/1323): Loss: 0.2200\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](350/1323): Loss: 0.2426\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](400/1323): Loss: 0.2051\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[17](450/1323): Loss: 0.2189\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](500/1323): Loss: 0.2045\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](550/1323): Loss: 0.1541\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](600/1323): Loss: 0.2390\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[17](650/1323): Loss: 0.2082\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[17](700/1323): Loss: 0.1868\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](750/1323): Loss: 0.2221\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](800/1323): Loss: 0.2830\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](850/1323): Loss: 0.2427\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[17](900/1323): Loss: 0.2171\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[17](950/1323): Loss: 0.2009\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](1000/1323): Loss: 0.2310\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](1050/1323): Loss: 0.1806\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](1100/1323): Loss: 0.1662\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[17](1150/1323): Loss: 0.2433\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[17](1200/1323): Loss: 0.1842\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[17](1250/1323): Loss: 0.2154\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[17](1300/1323): Loss: 0.2118\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 17 Complete: Avg. Loss: 0.2002\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6900\n",
      "====> Recall@5: 0.8541\n",
      "====> Recall@10: 0.8957\n",
      "====> Recall@20: 0.9298\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[18](50/1323): Loss: 0.1632\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](100/1323): Loss: 0.1745\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](150/1323): Loss: 0.1981\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](200/1323): Loss: 0.2219\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[18](250/1323): Loss: 0.2017\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](300/1323): Loss: 0.1426\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](350/1323): Loss: 0.2367\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](400/1323): Loss: 0.2118\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[18](450/1323): Loss: 0.1824\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](500/1323): Loss: 0.1929\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](550/1323): Loss: 0.1952\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](600/1323): Loss: 0.1855\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[18](650/1323): Loss: 0.1804\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[18](700/1323): Loss: 0.2091\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](750/1323): Loss: 0.1993\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](800/1323): Loss: 0.1796\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](850/1323): Loss: 0.1937\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[18](900/1323): Loss: 0.2390\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](950/1323): Loss: 0.2575\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](1000/1323): Loss: 0.1411\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[18](1050/1323): Loss: 0.2417\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[18](1100/1323): Loss: 0.1977\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[18](1150/1323): Loss: 0.2372\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[18](1200/1323): Loss: 0.2287\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[18](1250/1323): Loss: 0.2364\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[18](1300/1323): Loss: 0.1701\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 18 Complete: Avg. Loss: 0.1999\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6902\n",
      "====> Recall@5: 0.8542\n",
      "====> Recall@10: 0.8953\n",
      "====> Recall@20: 0.9304\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[19](50/1323): Loss: 0.2513\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](100/1323): Loss: 0.1739\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](150/1323): Loss: 0.0960\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](200/1323): Loss: 0.1785\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[19](250/1323): Loss: 0.2083\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](300/1323): Loss: 0.1846\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](350/1323): Loss: 0.1715\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](400/1323): Loss: 0.2314\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[19](450/1323): Loss: 0.1725\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[19](500/1323): Loss: 0.2255\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](550/1323): Loss: 0.1640\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](600/1323): Loss: 0.1606\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](650/1323): Loss: 0.2050\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[19](700/1323): Loss: 0.2122\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](750/1323): Loss: 0.1787\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](800/1323): Loss: 0.1992\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](850/1323): Loss: 0.1954\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[19](900/1323): Loss: 0.2234\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](950/1323): Loss: 0.1428\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[19](1000/1323): Loss: 0.1925\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](1050/1323): Loss: 0.2180\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[19](1100/1323): Loss: 0.2166\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[19](1150/1323): Loss: 0.2050\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[19](1200/1323): Loss: 0.2238\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[19](1250/1323): Loss: 0.1774\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[19](1300/1323): Loss: 0.2011\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 19 Complete: Avg. Loss: 0.1995\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6902\n",
      "====> Recall@5: 0.8546\n",
      "====> Recall@10: 0.8953\n",
      "====> Recall@20: 0.9308\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[20](50/1323): Loss: 0.2103\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](100/1323): Loss: 0.1854\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](150/1323): Loss: 0.2134\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](200/1323): Loss: 0.1979\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[20](250/1323): Loss: 0.1687\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](300/1323): Loss: 0.2158\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](350/1323): Loss: 0.2383\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](400/1323): Loss: 0.1671\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[20](450/1323): Loss: 0.2097\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](500/1323): Loss: 0.1569\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](550/1323): Loss: 0.1349\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[20](600/1323): Loss: 0.2159\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](650/1323): Loss: 0.2267\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[20](700/1323): Loss: 0.2481\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](750/1323): Loss: 0.2171\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](800/1323): Loss: 0.1670\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](850/1323): Loss: 0.2072\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[20](900/1323): Loss: 0.1942\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](950/1323): Loss: 0.1806\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](1000/1323): Loss: 0.1901\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](1050/1323): Loss: 0.1938\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[20](1100/1323): Loss: 0.1758\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[20](1150/1323): Loss: 0.2228\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[20](1200/1323): Loss: 0.2241\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[20](1250/1323): Loss: 0.2359\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[20](1300/1323): Loss: 0.2193\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 20 Complete: Avg. Loss: 0.1996\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6902\n",
      "====> Recall@5: 0.8550\n",
      "====> Recall@10: 0.8955\n",
      "====> Recall@20: 0.9308\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[21](50/1323): Loss: 0.2006\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](100/1323): Loss: 0.2180\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](150/1323): Loss: 0.1946\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](200/1323): Loss: 0.1574\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[21](250/1323): Loss: 0.2301\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](300/1323): Loss: 0.1852\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](350/1323): Loss: 0.2528\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](400/1323): Loss: 0.1995\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[21](450/1323): Loss: 0.2199\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[21](500/1323): Loss: 0.1802\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](550/1323): Loss: 0.2145\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](600/1323): Loss: 0.1754\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](650/1323): Loss: 0.2490\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[21](700/1323): Loss: 0.1660\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](750/1323): Loss: 0.2399\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](800/1323): Loss: 0.1567\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](850/1323): Loss: 0.2677\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[21](900/1323): Loss: 0.2372\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[21](950/1323): Loss: 0.2656\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](1000/1323): Loss: 0.2151\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](1050/1323): Loss: 0.2520\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](1100/1323): Loss: 0.2471\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[21](1150/1323): Loss: 0.2071\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[21](1200/1323): Loss: 0.1765\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[21](1250/1323): Loss: 0.2282\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[21](1300/1323): Loss: 0.2297\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 21 Complete: Avg. Loss: 0.1993\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6906\n",
      "====> Recall@5: 0.8555\n",
      "====> Recall@10: 0.8957\n",
      "====> Recall@20: 0.9308\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[22](50/1323): Loss: 0.1696\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](100/1323): Loss: 0.2133\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](150/1323): Loss: 0.2281\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](200/1323): Loss: 0.1819\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[22](250/1323): Loss: 0.1843\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](300/1323): Loss: 0.1810\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](350/1323): Loss: 0.1882\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](400/1323): Loss: 0.1924\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[22](450/1323): Loss: 0.1692\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](500/1323): Loss: 0.1494\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[22](550/1323): Loss: 0.2591\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](600/1323): Loss: 0.1700\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](650/1323): Loss: 0.1835\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[22](700/1323): Loss: 0.1106\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](750/1323): Loss: 0.2078\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](800/1323): Loss: 0.2617\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](850/1323): Loss: 0.1675\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[22](900/1323): Loss: 0.2131\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[22](950/1323): Loss: 0.2265\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](1000/1323): Loss: 0.2045\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](1050/1323): Loss: 0.1624\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](1100/1323): Loss: 0.1497\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[22](1150/1323): Loss: 0.2221\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[22](1200/1323): Loss: 0.2606\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[22](1250/1323): Loss: 0.2230\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[22](1300/1323): Loss: 0.2378\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 22 Complete: Avg. Loss: 0.1992\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6911\n",
      "====> Recall@5: 0.8559\n",
      "====> Recall@10: 0.8957\n",
      "====> Recall@20: 0.9309\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[23](50/1323): Loss: 0.1975\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](100/1323): Loss: 0.1888\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](150/1323): Loss: 0.1792\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](200/1323): Loss: 0.2106\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[23](250/1323): Loss: 0.1926\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](300/1323): Loss: 0.1885\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](350/1323): Loss: 0.1969\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](400/1323): Loss: 0.1975\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[23](450/1323): Loss: 0.1500\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](500/1323): Loss: 0.2061\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[23](550/1323): Loss: 0.1709\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[23](600/1323): Loss: 0.1971\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](650/1323): Loss: 0.2316\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[23](700/1323): Loss: 0.2267\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](750/1323): Loss: 0.2258\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](800/1323): Loss: 0.2193\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](850/1323): Loss: 0.1838\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[23](900/1323): Loss: 0.2030\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](950/1323): Loss: 0.2275\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](1000/1323): Loss: 0.2121\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[23](1050/1323): Loss: 0.2329\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[23](1100/1323): Loss: 0.2427\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[23](1150/1323): Loss: 0.2422\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[23](1200/1323): Loss: 0.1978\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[23](1250/1323): Loss: 0.2779\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[23](1300/1323): Loss: 0.2538\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 23 Complete: Avg. Loss: 0.1991\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6915\n",
      "====> Recall@5: 0.8557\n",
      "====> Recall@10: 0.8959\n",
      "====> Recall@20: 0.9309\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[24](50/1323): Loss: 0.2026\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](100/1323): Loss: 0.1326\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](150/1323): Loss: 0.1615\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](200/1323): Loss: 0.1747\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[24](250/1323): Loss: 0.2357\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](300/1323): Loss: 0.1682\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](350/1323): Loss: 0.1903\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](400/1323): Loss: 0.2019\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[24](450/1323): Loss: 0.1773\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](500/1323): Loss: 0.1687\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[24](550/1323): Loss: 0.2139\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](600/1323): Loss: 0.1615\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[24](650/1323): Loss: 0.1813\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[24](700/1323): Loss: 0.2323\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](750/1323): Loss: 0.2109\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](800/1323): Loss: 0.1796\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](850/1323): Loss: 0.1709\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[24](900/1323): Loss: 0.1682\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](950/1323): Loss: 0.2681\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](1000/1323): Loss: 0.1917\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[24](1050/1323): Loss: 0.2035\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[24](1100/1323): Loss: 0.2034\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[24](1150/1323): Loss: 0.1411\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[24](1200/1323): Loss: 0.2348\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[24](1250/1323): Loss: 0.2006\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[24](1300/1323): Loss: 0.2174\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 24 Complete: Avg. Loss: 0.1992\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6920\n",
      "====> Recall@5: 0.8561\n",
      "====> Recall@10: 0.8962\n",
      "====> Recall@20: 0.9311\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[25](50/1323): Loss: 0.1942\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](100/1323): Loss: 0.1487\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](150/1323): Loss: 0.2328\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](200/1323): Loss: 0.1901\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[25](250/1323): Loss: 0.2249\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](300/1323): Loss: 0.1898\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](350/1323): Loss: 0.1976\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](400/1323): Loss: 0.2124\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[25](450/1323): Loss: 0.1961\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](500/1323): Loss: 0.1738\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](550/1323): Loss: 0.1932\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[25](600/1323): Loss: 0.2244\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](650/1323): Loss: 0.1508\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[25](700/1323): Loss: 0.1365\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](750/1323): Loss: 0.2437\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](800/1323): Loss: 0.2270\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](850/1323): Loss: 0.1761\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n",
      "==> Epoch[25](900/1323): Loss: 0.1560\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](950/1323): Loss: 0.2339\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](1000/1323): Loss: 0.2111\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[25](1050/1323): Loss: 0.1986\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "==> Epoch[25](1100/1323): Loss: 0.2019\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "====> Building Cache\n",
      "Allocated: 117198848\n",
      "Cached: 13937672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[25](1150/1323): Loss: 0.2728\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[25](1200/1323): Loss: 0.1872\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[25](1250/1323): Loss: 0.2560\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch[25](1300/1323): Loss: 0.2240\n",
      "Allocated: 117199360\n",
      "Cached: 28821159936\n",
      "===> Epoch 25 Complete: Avg. Loss: 0.1988\n",
      "====> Extracting Features\n",
      "==> Batch (50/1117)\n",
      "==> Batch (100/1117)\n",
      "==> Batch (150/1117)\n",
      "==> Batch (200/1117)\n",
      "==> Batch (250/1117)\n",
      "==> Batch (300/1117)\n",
      "==> Batch (350/1117)\n",
      "==> Batch (400/1117)\n",
      "==> Batch (450/1117)\n",
      "==> Batch (500/1117)\n",
      "==> Batch (550/1117)\n",
      "==> Batch (600/1117)\n",
      "==> Batch (650/1117)\n",
      "==> Batch (700/1117)\n",
      "==> Batch (750/1117)\n",
      "==> Batch (800/1117)\n",
      "==> Batch (850/1117)\n",
      "==> Batch (900/1117)\n",
      "==> Batch (950/1117)\n",
      "==> Batch (1000/1117)\n",
      "==> Batch (1050/1117)\n",
      "==> Batch (1100/1117)\n",
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "====> Recall@1: 0.6924\n",
      "====> Recall@5: 0.8563\n",
      "====> Recall@10: 0.8962\n",
      "====> Recall@20: 0.9313\n",
      "====> Building Cache\n"
     ]
    }
   ],
   "source": [
    "print('===> Training model')\n",
    "writer = SummaryWriter(log_dir=join(opt.runsPath, datetime.now().strftime('%b%d_%H-%M-%S')+'_'+opt.arch+'_'+opt.pooling))\n",
    "\n",
    "# write checkpoints in logdir\n",
    "logdir = writer.file_writer.get_logdir()\n",
    "opt.savePath = join(logdir, opt.savePath)\n",
    "if not opt.resume:\n",
    "    makedirs(opt.savePath)\n",
    "\n",
    "with open(join(opt.savePath, 'flags.json'), 'w') as f:\n",
    "    f.write(json.dumps(\n",
    "        {k:v for k,v in vars(opt).items()}\n",
    "        ))\n",
    "print('===> Saving state to:', logdir)\n",
    "\n",
    "not_improved = 0\n",
    "best_score = 0\n",
    "for epoch in range(opt.start_epoch+1, opt.nEpochs + 1):\n",
    "    if opt.optim.upper() == 'SGD':\n",
    "        scheduler.step(epoch)\n",
    "    train(epoch)\n",
    "    if (epoch % opt.evalEvery) == 0:\n",
    "        recalls = test(whole_test_set, epoch, write_tboard=True)\n",
    "        is_best = recalls[5] > best_score \n",
    "        if is_best:\n",
    "            not_improved = 0\n",
    "            best_score = recalls[5]\n",
    "        else: \n",
    "            not_improved += 1\n",
    "\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'recalls': recalls,\n",
    "                'best_score': best_score,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'parallel' : isParallel,\n",
    "        }, is_best)\n",
    "\n",
    "        if opt.patience > 0 and not_improved > (opt.patience / opt.evalEvery):\n",
    "            print('Performance did not improve for', opt.patience, 'epochs. Stopping.')\n",
    "            break\n",
    "\n",
    "print(\"=> Best Recall@5: {:.4f}\".format(best_score), flush=True)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
