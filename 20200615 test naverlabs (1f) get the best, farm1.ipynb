{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Farm 1\n",
    "0~1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_start_idx = 0\n",
    "farm_end_idx = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, exists, isfile, realpath, dirname\n",
    "import argparse\n",
    "from glob import glob\n",
    "import random, shutil, json\n",
    "from math import log10, ceil\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "import faiss\n",
    "\n",
    "import netvlad\n",
    "\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "from os import makedirs, remove, chdir, environ\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import pyquaternion as pyq\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/Desktop/SuperPoint-VO/\")\n",
    "\n",
    "import time\n",
    "\n",
    "import utility\n",
    "\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "weight_dir = 'Jun05_17-49-03_vgg16_netvlad'\n",
    "lidar_map_pcd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--fromscratch'], dest='fromscratch', nargs=0, const=True, default=False, type=None, choices=None, help='Train from scratch rather than using pretrained models', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='pytorch-NetVlad')\n",
    "parser.add_argument('--mode', type=str, default='train', help='Mode', choices=['train', 'test', 'cluster'])\n",
    "parser.add_argument('--batchSize', type=int, default=4, help='Number of triplets (query, pos, negs). Each triplet consists of 12 images.')\n",
    "parser.add_argument('--cacheBatchSize', type=int, default=24, help='Batch size for caching and testing')\n",
    "parser.add_argument('--cacheRefreshRate', type=int, default=1000, help='How often to refresh cache, in number of queries. 0 for off')\n",
    "parser.add_argument('--nEpochs', type=int, default=30, help='number of epochs to train for')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--nGPU', type=int, default=1, help='number of GPU to use.')\n",
    "parser.add_argument('--optim', type=str, default='SGD', help='optimizer to use', choices=['SGD', 'ADAM'])\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate.')\n",
    "parser.add_argument('--lrStep', type=float, default=5, help='Decay LR ever N steps.')\n",
    "parser.add_argument('--lrGamma', type=float, default=0.5, help='Multiply LR by Gamma for decaying.')\n",
    "parser.add_argument('--weightDecay', type=float, default=0.001, help='Weight decay for SGD.')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGD.')\n",
    "parser.add_argument('--nocuda', action='store_true', help='Dont use cuda')\n",
    "parser.add_argument('--threads', type=int, default=8, help='Number of threads for each data loader to use')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed to use.')\n",
    "parser.add_argument('--dataPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/data/', help='Path for centroid data.')\n",
    "parser.add_argument('--runsPath', type=str, default='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', help='Path to save runs to.')\n",
    "parser.add_argument('--savePath', type=str, default='checkpoints', help='Path to save checkpoints to in logdir. Default=checkpoints/')\n",
    "parser.add_argument('--cachePath', type=str, default='/tmp', help='Path to save cache to.')\n",
    "parser.add_argument('--resume', type=str, default='', help='Path to load checkpoint from, for resuming training or testing.')\n",
    "parser.add_argument('--ckpt', type=str, default='latest', help='Resume from latest or best checkpoint.', choices=['latest', 'best'])\n",
    "parser.add_argument('--evalEvery', type=int, default=1, help='Do a validation set run, and save, every N epochs.')\n",
    "parser.add_argument('--patience', type=int, default=10, help='Patience for early stopping. 0 is off.')\n",
    "parser.add_argument('--dataset', type=str, default='pittsburgh', help='Dataset to use', choices=['pittsburgh','naverlabs'])\n",
    "parser.add_argument('--arch', type=str, default='vgg16', help='basenetwork to use', choices=['vgg16', 'alexnet'])\n",
    "parser.add_argument('--vladv2', action='store_true', help='Use VLAD v2')\n",
    "parser.add_argument('--pooling', type=str, default='netvlad', help='type of pooling to use', choices=['netvlad', 'max', 'avg'])\n",
    "parser.add_argument('--num_clusters', type=int, default=64, help='Number of NetVlad clusters. Default=64')\n",
    "parser.add_argument('--margin', type=float, default=0.1, help='Margin for triplet loss. Default=0.1')\n",
    "parser.add_argument('--split', type=str, default='val', help='Data split to use for testing. Default is val', choices=['test', 'test250k', 'train', 'val'])\n",
    "parser.add_argument('--fromscratch', action='store_true', help='Train from scratch rather than using pretrained models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "from os.path import join, exists\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import h5py\n",
    "\n",
    "def input_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def get_whole_test_set():\n",
    "    return WholeDatasetFromStruct(input_transform=input_transform(), mode='test')\n",
    "\n",
    "dbStruct = namedtuple('dbStruct', ['whichSet', 'dataset', \n",
    "                                   'db_image', 'db_utms', 'db_num', 'db_full_pose',\n",
    "                                   'q_image', 'q_utms', 'q_num', 'q_full_pose',\n",
    "                                   'posDistThr', 'posDistSqThr', 'nonTrivPosDistSqThr'])\n",
    "\n",
    "class WholeDatasetFromStruct(data.Dataset):\n",
    "    def __init__(self, input_transform=None, mode='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "        self.dbStruct = my_parse_dbStruct(mode)\n",
    "        self.images = np.hstack([self.dbStruct.db_image, self.dbStruct.q_image])\n",
    "\n",
    "        self.whichSet = self.dbStruct.whichSet\n",
    "        self.dataset = self.dbStruct.dataset\n",
    "\n",
    "        self.positives = None\n",
    "        self.distances = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index])\n",
    "        img = img.resize((640, 480))\n",
    "\n",
    "        if self.input_transform:\n",
    "            img = self.input_transform(img)\n",
    "\n",
    "        return img, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def getPositives(self):\n",
    "        # positives for evaluation are those within trivial threshold range\n",
    "        #fit NN to find them, search by radius\n",
    "        if  self.positives is None:\n",
    "            knn = NearestNeighbors(n_jobs=-1)\n",
    "            knn.fit(self.dbStruct.db_utms)\n",
    "\n",
    "            self.distances, self.positives = knn.radius_neighbors(self.dbStruct.q_utms, radius=self.dbStruct.posDistThr)\n",
    "\n",
    "        return self.positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_parse_dbStruct(_whichSet='train'):\n",
    "\n",
    "    whichSet = _whichSet\n",
    "    dataset = 'naverlabs'\n",
    "    \n",
    "     # for (2019-04-16_14-35-00)\n",
    "    image_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/images'\n",
    "    image_files_list = []\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970285*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970286*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970288*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970289*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970290*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '22970291*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324954*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324955*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324968*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324969*.jpg'))))\n",
    "    \n",
    "    gt_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/groundtruth.hdf5'\n",
    "    full_pose_list = []\n",
    "    with h5py.File(gt_path, \"r\") as f:\n",
    "        full_pose_list.append(np.array(f['22970285_pose']))\n",
    "        full_pose_list.append(np.array(f['22970286_pose']))\n",
    "        full_pose_list.append(np.array(f['22970288_pose']))\n",
    "        full_pose_list.append(np.array(f['22970289_pose']))\n",
    "        full_pose_list.append(np.array(f['22970290_pose']))\n",
    "        full_pose_list.append(np.array(f['22970291_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324954_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324955_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324968_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324969_pose']))\n",
    "    \n",
    "    \n",
    "    # for (2019-08-20_11-32-05)\n",
    "    image_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-08-20_11-32-05/images'\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324954*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324955*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324968*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324969*.jpg'))))\n",
    "    \n",
    "    gt_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-08-20_11-32-05/groundtruth.hdf5'\n",
    "    with h5py.File(gt_path, \"r\") as f:\n",
    "        full_pose_list.append(np.array(f['AC01324954_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324955_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324968_pose']))\n",
    "        full_pose_list.append(np.array(f['AC01324969_pose']))\n",
    "    \n",
    "    db_image = np.hstack(image_files_list)\n",
    "    db_full_pose = np.vstack(full_pose_list)\n",
    "    db_num = len(db_image)\n",
    "    db_utms = db_full_pose[:,:2]\n",
    "    \n",
    "    image_path = '/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/test/2019-08-21_12-10-13/images'\n",
    "    image_files_list = []\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '40027089*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '40029628*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '40030065*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '40031951*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '40033113*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, '40033116*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324954*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324955*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324968*.jpg'))))\n",
    "    image_files_list.append(sorted(glob(os.path.join(image_path, 'AC01324969*.jpg'))))\n",
    "    image_files = np.hstack(image_files_list)\n",
    "    \n",
    "    ####### for processing farm\n",
    "    image_files = image_files[farm_start_idx:farm_end_idx]\n",
    "    \n",
    "    q_image = image_files\n",
    "    q_utms = None\n",
    "    q_num = len(q_image)\n",
    "    q_full_pose = None\n",
    "    \n",
    "\n",
    "    return dbStruct(whichSet, dataset, \n",
    "                    db_image, db_utms, db_num, db_full_pose,\n",
    "                    q_image, q_utms, q_num, q_full_pose, \n",
    "                    5, 25, 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored flags: ['--optim', 'SGD', '--lr', '0.0001', '--lrStep', '5', '--lrGamma', '0.5', '--weightDecay', '0.001', '--momentum', '0.9', '--seed', '123', '--runsPath', '/home/ubuntu/Desktop/pytorch-NetVlad/runs/', '--savePath', '/home/ubuntu/Desktop/pytorch-NetVlad/runs/Jun05_17-49-03_vgg16_netvlad/checkpoints', '--patience', '10', '--arch', 'vgg16', '--pooling', 'netvlad', '--num_clusters', '64', '--margin', '0.1']\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args(args=('--dataset=naverlabs --mode=test --resume=runs/%s --split=test' % weight_dir).split(' '))\n",
    "restore_var = ['lr', 'lrStep', 'lrGamma', 'weightDecay', 'momentum', \n",
    "            'runsPath', 'savePath', 'arch', 'num_clusters', 'pooling', 'optim',\n",
    "            'margin', 'seed', 'patience']\n",
    "\n",
    "flag_file = join(opt.resume, 'checkpoints', 'flags.json')\n",
    "if exists(flag_file):\n",
    "    with open(flag_file, 'r') as f:\n",
    "        stored_flags = {'--'+k : str(v) for k,v in json.load(f).items() if k in restore_var}\n",
    "        to_del = []\n",
    "        for flag, val in stored_flags.items():\n",
    "            for act in parser._actions:\n",
    "                if act.dest == flag[2:]:\n",
    "                    # store_true / store_false args don't accept arguments, filter these \n",
    "                    if type(act.const) == type(True):\n",
    "                        if val == str(act.default):\n",
    "                            to_del.append(flag)\n",
    "                        else:\n",
    "                            stored_flags[flag] = ''\n",
    "        for flag in to_del: del stored_flags[flag]\n",
    "\n",
    "        train_flags = [x for x in list(sum(stored_flags.items(), tuple())) if len(x) > 0]\n",
    "        print('Restored flags:', train_flags)\n",
    "        opt = parser.parse_args(train_flags, namespace=opt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='vgg16', batchSize=4, cacheBatchSize=24, cachePath='/tmp', cacheRefreshRate=1000, ckpt='latest', dataPath='/home/ubuntu/Desktop/pytorch-NetVlad/data/', dataset='naverlabs', evalEvery=1, fromscratch=False, lr=0.0001, lrGamma=0.5, lrStep=5.0, margin=0.1, mode='test', momentum=0.9, nEpochs=30, nGPU=1, nocuda=False, num_clusters=64, optim='SGD', patience=10, pooling='netvlad', resume='runs/Jun05_17-49-03_vgg16_netvlad', runsPath='/home/ubuntu/Desktop/pytorch-NetVlad/runs/', savePath='/home/ubuntu/Desktop/pytorch-NetVlad/runs/Jun05_17-49-03_vgg16_netvlad/checkpoints', seed=123, split='test', start_epoch=0, threads=8, vladv2=False, weightDecay=0.001)\n"
     ]
    }
   ],
   "source": [
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "device = torch.device(\"cuda\")\n",
    "random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "torch.cuda.manual_seed(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_test_set = get_whole_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim = 512\n",
    "encoder = models.vgg16(pretrained=True)\n",
    "# capture only feature part and remove last relu and maxpool\n",
    "layers = list(encoder.features.children())[:-2]\n",
    "\n",
    "# if using pretrained then only train conv5_1, conv5_2, and conv5_3\n",
    "for l in layers[:-5]: \n",
    "    for p in l.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(*layers)\n",
    "model = nn.Module() \n",
    "model.add_module('encoder', encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_vlad = netvlad.NetVLAD(num_clusters=opt.num_clusters, dim=encoder_dim, vladv2=opt.vladv2)\n",
    "if not opt.resume: \n",
    "    if opt.mode.lower() == 'train':\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + train_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "    else:\n",
    "        initcache = join(opt.dataPath, 'centroids', opt.arch + '_' + whole_test_set.dataset + '_' + str(opt.num_clusters) +'_desc_cen.hdf5')\n",
    "\n",
    "    if not exists(initcache):\n",
    "        raise FileNotFoundError('Could not find clusters, please run with --mode=cluster before proceeding')\n",
    "\n",
    "    with h5py.File(initcache, mode='r') as h5: \n",
    "        clsts = h5.get(\"centroids\")[...]\n",
    "        traindescs = h5.get(\"descriptors\")[...]\n",
    "        net_vlad.init_params(clsts, traindescs) \n",
    "        del clsts, traindescs\n",
    "\n",
    "model.add_module('pool', net_vlad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'runs/Jun05_17-49-03_vgg16_netvlad/checkpoints/checkpoint.pth.tar'\n",
      "=> loaded checkpoint 'runs/Jun05_17-49-03_vgg16_netvlad/checkpoints/checkpoint.pth.tar' (epoch 30)\n"
     ]
    }
   ],
   "source": [
    "if opt.ckpt.lower() == 'latest':\n",
    "    resume_ckpt = join(opt.resume, 'checkpoints', 'checkpoint.pth.tar')\n",
    "elif opt.ckpt.lower() == 'best':\n",
    "    resume_ckpt = join(opt.resume, 'checkpoints', 'model_best.pth.tar')\n",
    "\n",
    "if isfile(resume_ckpt):\n",
    "    print(\"=> loading checkpoint '{}'\".format(resume_ckpt))\n",
    "    checkpoint = torch.load(resume_ckpt, map_location=lambda storage, loc: storage)\n",
    "    opt.start_epoch = checkpoint['epoch']\n",
    "    best_metric = checkpoint['best_score']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model = model.to(device)\n",
    "    if opt.mode == 'train':\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(resume_ckpt, checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(resume_ckpt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = whole_test_set\n",
    "test_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=True, pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Extracting Features\n",
      "==> Batch (50/1177)\n",
      "==> Batch (100/1177)\n",
      "==> Batch (150/1177)\n",
      "==> Batch (200/1177)\n",
      "==> Batch (250/1177)\n",
      "==> Batch (300/1177)\n",
      "==> Batch (350/1177)\n",
      "==> Batch (400/1177)\n",
      "==> Batch (450/1177)\n",
      "==> Batch (500/1177)\n",
      "==> Batch (550/1177)\n",
      "==> Batch (600/1177)\n",
      "==> Batch (650/1177)\n",
      "==> Batch (700/1177)\n",
      "==> Batch (750/1177)\n",
      "==> Batch (800/1177)\n",
      "==> Batch (850/1177)\n",
      "==> Batch (900/1177)\n",
      "==> Batch (950/1177)\n",
      "==> Batch (1000/1177)\n",
      "==> Batch (1050/1177)\n",
      "==> Batch (1100/1177)\n",
      "==> Batch (1150/1177)\n",
      "end time : 408.3172278404236\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('====> Extracting Features')\n",
    "    pool_size = encoder_dim\n",
    "    if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters\n",
    "    dbFeat = np.empty((len(test_set), pool_size))\n",
    "\n",
    "    for iteration, (input, indices) in enumerate(test_data_loader, 1):\n",
    "        input = input.to(device)\n",
    "        image_encoding = model.encoder(input)\n",
    "        vlad_encoding = model.pool(image_encoding) \n",
    "\n",
    "        dbFeat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()\n",
    "        if iteration % 50 == 0 or len(test_data_loader) <= 10:\n",
    "            print(\"==> Batch ({}/{})\".format(iteration, len(test_data_loader)), flush=True)\n",
    "\n",
    "        del input, image_encoding, vlad_encoding\n",
    "del test_data_loader\n",
    "\n",
    "print(\"end time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Building faiss index\n",
      "====> Calculating recall @ N\n",
      "end time : 38.02919960021973\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "# extracted for both db and query, now split in own sets\n",
    "qFeat = dbFeat[test_set.dbStruct.db_num:].astype('float32')\n",
    "dbFeat = dbFeat[:test_set.dbStruct.db_num].astype('float32')\n",
    "\n",
    "print('====> Building faiss index')\n",
    "faiss_index = faiss.IndexFlatL2(pool_size)\n",
    "faiss_index.add(dbFeat)\n",
    "\n",
    "print('====> Calculating recall @ N')\n",
    "n_values = [1,5,10,20]\n",
    "\n",
    "_, predictions = faiss_index.search(qFeat, max(n_values)) \n",
    "\n",
    "print(\"end time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HD Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time : 343.6502170562744\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "# VERY TIME CONSUMING!\n",
    "lidar_map_pcd1 = o3d.io.read_point_cloud(\"/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-04-16_14-35-00/map.pcd\")\n",
    "pcd_tree1 = o3d.geometry.KDTreeFlann(lidar_map_pcd1)\n",
    "\n",
    "lidar_map_pcd2 = o3d.io.read_point_cloud(\"/home/ubuntu/Desktop/visual-localization-challenge-2020/indoor_dataset/1f/train/2019-08-20_11-32-05/map.pcd\")\n",
    "pcd_tree2 = o3d.geometry.KDTreeFlann(lidar_map_pcd2)\n",
    "\n",
    "print(\"end time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sp_extractor import SuperPointFrontend, PointTracker\n",
    "\n",
    "class SP_VisualOdometry:\n",
    "    def __init__(self):\n",
    "        self.frame_stage = 0\n",
    "        self.cam = None\n",
    "        self.new_frame = None\n",
    "        self.last_frame = None\n",
    "        self.cur_R = None\n",
    "        self.cur_t = None\n",
    "        self.px_ref = None\n",
    "        self.px_cur = None\n",
    "        self.focal = None\n",
    "        self.pp = None\n",
    "        self.trueX, self.trueY, self.trueZ = 0, 0, 0\n",
    "        self.detector = SuperPointFrontend(weights_path=\"/home/ubuntu/Desktop/SuperPoint-VO/weights/superpoint_v1.pth\",\n",
    "                                           nms_dist=4,\n",
    "                                           conf_thresh=0.015,\n",
    "                                           nn_thresh=0.7,\n",
    "                                           cuda=True)\n",
    "        self.tracker = PointTracker(max_length=2, nn_thresh=self.detector.nn_thresh)\n",
    "\n",
    "    def featureTracking(self, new_frame):\n",
    "        pts, desc, heatmap = self.detector.run(new_frame)\n",
    "        # Add points and descriptors to the tracker.\n",
    "        self.tracker.update(pts, desc)\n",
    "        # Get tracks for points which were match successfully across all frames.\n",
    "        tracks = self.tracker.get_tracks(min_length=1)\n",
    "        # Normalize track scores to [0,1].\n",
    "        tracks[:, 1] /= float(self.detector.nn_thresh)\n",
    "        kp1, kp2 = self.tracker.draw_tracks(tracks)\n",
    "        return kp1, kp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All-in-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(utility)\n",
    "\n",
    "def VisualLocalization(dataset, query_idx):\n",
    "    query_item, pred_list = utility.placeRecognitionTopFive(dataset, predictions, query_idx)\n",
    "    [query_image_full_path, query_img, _] = query_item\n",
    "    query_camera_model = query_image_full_path.split('/')[-1].split('_')[0]\n",
    "    _, query_A, query_dist_coeff = utility.intrinsic_params(query_camera_model)\n",
    "\n",
    "    neighbor_radius = 15.0    \n",
    "\n",
    "    best_inlier = 0\n",
    "    best_pose = []\n",
    "    for rank, pred_val in enumerate(pred_list):\n",
    "        [pred_image_full_path, pred_img, pred_pose] = pred_val\n",
    "\n",
    "        acquisition_date = pred_image_full_path.split('/')[-3] \n",
    "        if acquisition_date == '2019-04-16_14-35-00':\n",
    "            pcd = lidar_map_pcd1\n",
    "            pcd_tree = pcd_tree1\n",
    "        elif acquisition_date == '2019-08-20_11-32-05':\n",
    "            pcd = lidar_map_pcd2\n",
    "            pcd_tree = pcd_tree2\n",
    "            \n",
    "        [k, idx, _] = pcd_tree.search_radius_vector_3d(pred_pose[:3], neighbor_radius) # time consuming\n",
    "        radius_points = np.asarray(pcd.points)[idx] \n",
    "\n",
    "        pred_Rt = np.eye(4)\n",
    "        pred_Rt[:3,3] = pred_pose[:3]\n",
    "        (pred_qw, pred_qx, pred_qy, pred_qz) = pred_pose[3:]\n",
    "        pred_Rt[:3,:3] = R.from_quat([pred_qx,pred_qy,pred_qz,pred_qw]).as_matrix()\n",
    "        pred_camera_model = pred_image_full_path.split('/')[-1].split('_')[0]\n",
    "        _, pred_A, pred_dist_coeff = utility.intrinsic_params(pred_camera_model)\n",
    "        pred_projected_img = utility.projection(pred_img, radius_points, pred_A, pred_Rt, thickness=3) # time consuming \n",
    "\n",
    "        # superpoint\n",
    "        sp_vo = SP_VisualOdometry()\n",
    "\n",
    "        query_img_sp = cv2.imread(query_image_full_path)\n",
    "        pred_img_sp = cv2.imread(pred_image_full_path)\n",
    "\n",
    "        query_img_sp = cv2.undistort(query_img_sp, query_A, query_dist_coeff)\n",
    "        pred_img_sp = cv2.undistort(pred_img_sp, pred_A, pred_dist_coeff)\n",
    "\n",
    "        sp_vo.featureTracking(query_img_sp)\n",
    "        px_query, px_pred = sp_vo.featureTracking(pred_img_sp)\n",
    "        curr_size = px_pred.shape[0]\n",
    "\n",
    "        # pnp\n",
    "        points_2d = []\n",
    "        points_3d = []\n",
    "        for corr_idx in range(curr_size):\n",
    "            if (not np.array_equal(pred_projected_img[int(px_pred[corr_idx][1]), int(px_pred[corr_idx][0])], np.ones(3)*np.inf)):\n",
    "                points_2d.append([px_query[corr_idx][0],px_query[corr_idx][1]])\n",
    "                points_3d.append(pred_projected_img[int(px_pred[corr_idx][1]), int(px_pred[corr_idx][0])])\n",
    "\n",
    "        points_2d = np.asarray(points_2d, dtype=np.float32)\n",
    "        points_3d = np.asarray(points_3d, dtype=np.float32)\n",
    "\n",
    "        try:\n",
    "            retval, rvec, tvec, inliers = cv2.solvePnPRansac(points_3d, points_2d, query_A, query_dist_coeff)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if (retval):\n",
    "            rotation_matrix = np.zeros(shape=(3,3))\n",
    "            cv2.Rodrigues(rvec, rotation_matrix)\n",
    "            query_rot = pred_Rt[:3,:3]@np.linalg.inv(np.asarray(rotation_matrix))\n",
    "            query_trans = np.transpose(np.asarray(tvec)) + pred_Rt[:3,3]\n",
    "            query_Rt = np.eye(4)\n",
    "            query_Rt[:3,:3] = query_rot\n",
    "            query_Rt[:3,3] = query_trans\n",
    "\n",
    "            query_quat = R.from_matrix(query_rot).as_quat()\n",
    "            result_pose = (query_quat,query_trans)\n",
    "\n",
    "            if (best_inlier < len(inliers)):\n",
    "                best_inlier = len(inliers)\n",
    "                best_pose = result_pose\n",
    "            \n",
    "    \n",
    "    if (best_inlier == 0):\n",
    "        return False, None, None, None\n",
    "    else:\n",
    "        return True, query_image_full_path, query_img, best_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 45.965402364730835\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "query_idx = np.random.randint(test_set.dbStruct.q_num)\n",
    "ret_val, _ , _, best_pose = VisualLocalization(test_set, query_idx)\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n",
      "100%|██████████| 1000/1000 [11:59:27<00:00, 43.17s/it]\n"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "for query_idx in tqdm(range(test_set.dbStruct.q_num)):\n",
    "    \n",
    "    try:\n",
    "        ret_val, query_image_full_path , _, best_pose = VisualLocalization(test_set, query_idx)\n",
    "    except KeyboardInterrupt: \n",
    "        break\n",
    "    except: \n",
    "        continue\n",
    "        \n",
    "    if ret_val == True:        \n",
    "        temp_dict = {\n",
    "                 'floor': \"1f\",\n",
    "                 'name': query_image_full_path.split('/')[-1],\n",
    "                 'qw': best_pose[0][3],\n",
    "                 'qx': best_pose[0][0],\n",
    "                 'qy': best_pose[0][1],\n",
    "                 'qz': best_pose[0][2],\n",
    "                 'x': best_pose[1][0][0],\n",
    "                 'y': best_pose[1][0][1],\n",
    "                 'z': best_pose[1][0][2]\n",
    "        }\n",
    "\n",
    "        result_list.append(temp_dict)\n",
    "\n",
    "    if (query_idx % 10 == 0):\n",
    "        with open('20200615_indoor_submit (farm1).json', 'w') as outfile:\n",
    "            json.dump(result_list, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('20200615_indoor_submit (farm1).json', 'w') as outfile:\n",
    "    json.dump(result_list, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
